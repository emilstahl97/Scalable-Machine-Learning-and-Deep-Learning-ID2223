{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilstahl97/Scalable-Machine-Learning-and-Deep-Learning-ID2223/blob/notebooks/SpeechRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72r5k4sECfZj"
      },
      "source": [
        "## Speech Emotion Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_MrfWn7CfZl"
      },
      "source": [
        "#### RAVDESS Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x0YmUqwCfZm"
      },
      "source": [
        "- The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
        "- The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). \n",
        "- The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. \n",
        "- Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. \n",
        "- Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. - All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  \n",
        "\n",
        "For this analysis, the below file types have been used:\n",
        "- Audio speech files. \n",
        "- Additionally, the speech from the video files have been extracted by converting the MP4 files to WAV format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCwwIY3oCfZn"
      },
      "source": [
        "File naming convention: Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: \n",
        "\n",
        "Filename identifiers \n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# README - Execute this cell to mount the notebook in your google drive. \n",
        "# Execute the cell and follow the link to sign and, paste the given key in the little text box. The credentials are only available for you. \n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/audio-dataset\"):\n",
        "  print(\"Pulling dataset\")\n",
        "  os.makedirs(\"/content/drive/MyDrive/audio-dataset\")\n",
        "  !git clone https://github.com/emilstahl97/Audio-dataset.git\n",
        "else:\n",
        "  print(\"Dataset already exists\")\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/audio-dataset\")\n",
        "#os.chdir(\"/content/drive/MyDrive/audio-dataset/Audio-dataset/Audio-dataset\")\n",
        "#os.chdir(\"/content/drive/MyDrive/audio-dataset/Audio-dataset/Rawdata\")\n",
        "\n",
        "!git pull\n",
        "!ls\n",
        "\n",
        "RAVDESS_PATH = \"./RAVDESS\"\n",
        "SAVEE_PATH = \"./SAVEE\"\n",
        "SAVED_MODELS_PATH = \"../saved_models\"\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/ID2223/project/mfcc\"):\n",
        "  os.makedirs(\"/content/drive/MyDrive/ID2223/project/mfcc/\")\n",
        "  \n",
        "if not os.path.exists(\"/content/drive/MyDrive/ID2223/project/emotion\"):\n",
        "  os.makedirs(\"/content/drive/MyDrive/ID2223/project/emotion/\")\n",
        "\n",
        "mfcc_file_path = \"/content/drive/MyDrive/ID2223/project/mfcc/mfcc.npy\"\n",
        "emotion_file_path = \"/content/drive/MyDrive/ID2223/project/emotion/emotion.npy\"\n",
        "path_AudioFiles = './RAVDESS'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t072N24D2lC",
        "outputId": "9b79b952-e158-422b-b666-76b1b8e92454"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset already exists\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "RAVDESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade keras\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade matplotlib\n",
        "!pip install --upgrade librosa\n",
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A498crVuC2oe",
        "outputId": "da47186d-41ff-4385-fd39-d5720bdcba80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.28.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.6)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qUuSd_z1CfZo"
      },
      "outputs": [],
      "source": [
        "# Import libraries \n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import pandas as pd\n",
        "import glob \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import IPython.display as ipd  \n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import subprocess\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZcPEqnzCfZu"
      },
      "source": [
        "### Explore the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SUCRoj90CfZ0"
      },
      "outputs": [],
      "source": [
        "# Define a function to generate MFCC from the audio files\n",
        "\n",
        "def fn_MFCC_Emotion(v_path_AudioFiles):\n",
        "    \"Feature Generation: MFCC and Emotion\"\n",
        "    print(\"in mfcc\")\n",
        "    i = 0\n",
        "    for path in [v_path_AudioFiles]:\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for name in files:\n",
        "                i = i + 1\n",
        "                print(name, i)\n",
        "\n",
        "                X, sample_rate = librosa.load(os.path.join(str(root),str(name)), res_type='kaiser_fast')  \n",
        "                v_mfcc = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "\n",
        "                v_emotion = int(name[7:8]) - 1 \n",
        "                \n",
        "                lst_mfcc.append(v_mfcc)\n",
        "                lst_emotion.append(v_emotion)\n",
        "    return lst_mfcc, lst_emotion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "lst_mfcc = []\n",
        "lst_emotion = []\n",
        "mfcc = []\n",
        "emotion = []\n",
        "\n",
        "if not os.path.exists(mfcc_file_path) or not os.path.exists(emotion_file_path):\n",
        "    print(\"Running fn_MFCC_Emotion...\")\n",
        "    %time mfcc, emotion =  fn_MFCC_Emotion(path_AudioFiles)\n",
        "\n",
        "    print(mfcc[0:3], emotion[0:3])\n",
        "\n",
        "    np.save(mfcc_file_path, mfcc)\n",
        "    np.save(emotion_file_path, emotion)\n",
        "  \n",
        "  \n",
        "elif os.path.exists(mfcc_file_path) and os.path.exists(emotion_file_path):\n",
        "  print(\"Loading mfcc and emotion from local filesystem...\")\n",
        "  mfcc = np.load(mfcc_file_path, allow_pickle=True)\n",
        "  emotion = np.load(emotion_file_path, allow_pickle=True)\n",
        "\n",
        "X = np.array(mfcc)\n",
        "y = np.array(emotion)\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qs5AEpuNkV6",
        "outputId": "56cfd4b3-6558-4208-f4c7-019d45987d11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading mfcc and emotion from local filesystem...\n",
            "(2884, 40) (2884,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DZUoyJDVCfZ2"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBtO3NxCfZ2"
      },
      "source": [
        "### Model 1: Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvc5stgQCfZ3",
        "outputId": "1b2de547-55bb-4d67-9ebe-b5329af6ca53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 217 ms, sys: 3.71 ms, total: 221 ms\n",
            "Wall time: 220 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=3,\n",
              "                       random_state=9)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dt = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf = 3, \n",
        "                                 random_state= 9)\n",
        "\n",
        "%time dt.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8S7ZSfoCfZ4",
        "outputId": "921fdc5e-c380-4b6b-8248-70a099bedf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.66      0.64        59\n",
            "           1       0.69      0.71      0.70       119\n",
            "           2       0.53      0.58      0.56       133\n",
            "           3       0.45      0.62      0.52        89\n",
            "           4       0.66      0.71      0.68       106\n",
            "           5       0.73      0.61      0.66       132\n",
            "           6       0.62      0.49      0.55       117\n",
            "           7       0.50      0.47      0.48       111\n",
            "\n",
            "    accuracy                           0.60       866\n",
            "   macro avg       0.60      0.60      0.60       866\n",
            "weighted avg       0.61      0.60      0.60       866\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dt.fit(X_train, y_train)\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzLFmZjuCfZ4"
      },
      "source": [
        "### Model 2: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aa2PYxeCfZ5",
        "outputId": "a040d26d-3a28-4339-bedb-4b33fde66738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 49s, sys: 1.04 s, total: 2min 50s\n",
            "Wall time: 2min 49s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=10, max_features='sqrt', max_leaf_nodes=100,\n",
              "                       min_samples_leaf=3, min_samples_split=20,\n",
              "                       n_estimators=20000, random_state=9)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "rf = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"sqrt\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 20000, random_state= 9)\n",
        "\n",
        "%time rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkDRkFQrCfZ5",
        "outputId": "0bc440d3-100d-4ae8-cbe2-e8bb9d5ceb3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.44      0.61        59\n",
            "           1       0.77      1.00      0.87       119\n",
            "           2       0.90      0.71      0.80       133\n",
            "           3       0.68      0.84      0.75        89\n",
            "           4       0.89      0.87      0.88       106\n",
            "           5       0.83      0.78      0.80       132\n",
            "           6       0.80      0.81      0.81       117\n",
            "           7       0.72      0.80      0.76       111\n",
            "\n",
            "    accuracy                           0.80       866\n",
            "   macro avg       0.82      0.78      0.78       866\n",
            "weighted avg       0.82      0.80      0.80       866\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aod5TPWFCfZ6"
      },
      "source": [
        "### Model 3: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4S6fvVtCfZ6",
        "outputId": "8d4e60d1-b12e-4d14-8e39-ee5baf20abe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 34s, sys: 727 ms, total: 3min 35s\n",
            "Wall time: 3min 34s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(gamma=0.5, max_depth=10, n_estimators=2000,\n",
              "              objective='multi:softprob')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "XGB = XGBClassifier(n_estimators=2000, gamma=0.5,learning_rate=0.1, max_depth = 10)\n",
        "\n",
        "%time XGB.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTG8Iwq1CfZ7",
        "outputId": "eb6c3fec-786b-4ba6-eec3-04083a7f91dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.73      0.81        59\n",
            "           1       0.91      0.97      0.93       119\n",
            "           2       0.91      0.92      0.92       133\n",
            "           3       0.81      0.98      0.89        89\n",
            "           4       0.91      0.92      0.92       106\n",
            "           5       0.89      0.88      0.89       132\n",
            "           6       0.94      0.88      0.91       117\n",
            "           7       0.86      0.80      0.83       111\n",
            "\n",
            "    accuracy                           0.89       866\n",
            "   macro avg       0.89      0.89      0.89       866\n",
            "weighted avg       0.90      0.89      0.89       866\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = XGB.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhmlLj2LCfZ7"
      },
      "source": [
        "### Model 4: CNN (Convolutional  Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcAEWiymCfZ8",
        "outputId": "424f81ba-2771-4809-8523-33b63365d08d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2018, 40, 1), (866, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r06kypHICfZ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(256, 8, padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=(4)))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=(4)))\n",
        "model.add(Conv1D(64, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(64, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8)) \n",
        "model.add(Activation('softmax'))\n",
        "opt = optimizers.RMSprop(learning_rate=0.00001, decay=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z4YdLAzCfZ9",
        "outputId": "fe3977cd-ac20-4dbd-bbb5-74aa2eb27bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 256)           2304      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 256)           0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 40, 256)           524544    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 40, 256)          1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 40, 256)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 256)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 10, 256)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 10, 128)           262272    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10, 128)           0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 10, 128)           131200    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 10, 128)           0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 10, 128)           131200    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 10, 128)           0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 10, 128)           131200    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 10, 128)          512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 10, 128)           0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 10, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 2, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 2, 64)             65600     \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 2, 64)             0         \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 2, 64)             32832     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 2, 64)             0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8)                 1032      \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,283,720\n",
            "Trainable params: 1,282,952\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model):\n",
        "    import os\n",
        "    model_name = 'cnn.h5'\n",
        "    save_dir = os.path.join(os.getcwd(), \"/content/drive/MyDrive/savedmodels/\")\n",
        "    # Save model and weights\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    model_path = os.path.join(save_dir, model_name)\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "    model_json = model.to_json()\n",
        "    with open(\"/content/drive/MyDrive/savedmodels/model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(\"/content/drive/MyDrive/savedmodels/model.h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "metadata": {
        "id": "ZjVAiKumQRDZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0w8zndPwCfZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b4fa59-7421-45c1-bab5-bac2ab1989f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained model\n",
            "Loaded model from disk\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from keras.models import model_from_json\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive/savedmodels/cnn.h5\"):\n",
        "  print(\"Loading pre-trained model\")\n",
        "  model = keras.models.load_model(\"/content/drive/MyDrive/savedmodels/cnn.h5\")\n",
        "  json_file = open('/content/drive/MyDrive/savedmodels/model.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  model.load_weights(\"/content/drive/MyDrive/savedmodels/model.h5\")\n",
        "  print(\"Loaded model from disk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  cnn = model.fit(X_train, y_train, batch_size=1, epochs=2, validation_data=(X_test, y_test)) #should be bs 16 and 1000 epochs\n",
        "  save_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc-V-h_AGLhX",
        "outputId": "6a0711ec-d4a2-41d8-9316-c19d4733364b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2018/2018 [==============================] - 50s 24ms/step - loss: 1.9228 - accuracy: 0.2418 - val_loss: 1.9495 - val_accuracy: 0.2309\n",
            "Epoch 2/2\n",
            "2018/2018 [==============================] - 45s 22ms/step - loss: 1.8944 - accuracy: 0.2671 - val_loss: 1.9231 - val_accuracy: 0.2517\n",
            "Saved trained model at /content/drive/MyDrive/savedmodels/cnn.h5 \n",
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "iu5F6t7xCfZ_",
        "outputId": "115a5d9e-5d75-4bb8-a4a1-8353600d5061"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the model loss and accuracy \n",
        "plt.plot(cnn.history['loss'])\n",
        "plt.plot(cnn.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(cnn.history['accuracy'])\n",
        "plt.plot(cnn.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build lstm model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(keras.layers.LSTM(64))\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "model.add(keras.layers.Dense(8, activation='softmax'))"
      ],
      "metadata": {
        "id": "-teayQUDz-Le"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "model.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o824Bqd0LV4",
        "outputId": "2def7846-6759-4b07-e5e2-175e0c972866"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 40, 64)            16896     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 520       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,600\n",
            "Trainable params: 54,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate model\n",
        "lstm = model.fit(X_train, y_train, batch_size=16, epochs=1000, validation_data=(X_test, y_test))\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print('LSTM model test accuracy: ', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH1haSpo0amp",
        "outputId": "4f2bf58c-05fd-4615-83e9-331c2fc71890"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "127/127 [==============================] - 10s 52ms/step - loss: 2.0791 - accuracy: 0.1189 - val_loss: 2.0734 - val_accuracy: 0.1432\n",
            "Epoch 2/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0785 - accuracy: 0.1224 - val_loss: 2.0717 - val_accuracy: 0.1432\n",
            "Epoch 3/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 2.0711 - accuracy: 0.1189 - val_loss: 2.0699 - val_accuracy: 0.1420\n",
            "Epoch 4/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0718 - accuracy: 0.1333 - val_loss: 2.0681 - val_accuracy: 0.1478\n",
            "Epoch 5/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0688 - accuracy: 0.1313 - val_loss: 2.0663 - val_accuracy: 0.1524\n",
            "Epoch 6/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 2.0703 - accuracy: 0.1343 - val_loss: 2.0649 - val_accuracy: 0.1524\n",
            "Epoch 7/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0664 - accuracy: 0.1442 - val_loss: 2.0636 - val_accuracy: 0.1582\n",
            "Epoch 8/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0653 - accuracy: 0.1358 - val_loss: 2.0624 - val_accuracy: 0.1570\n",
            "Epoch 9/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0623 - accuracy: 0.1591 - val_loss: 2.0613 - val_accuracy: 0.1594\n",
            "Epoch 10/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0623 - accuracy: 0.1442 - val_loss: 2.0597 - val_accuracy: 0.1570\n",
            "Epoch 11/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0621 - accuracy: 0.1591 - val_loss: 2.0584 - val_accuracy: 0.1605\n",
            "Epoch 12/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0578 - accuracy: 0.1665 - val_loss: 2.0570 - val_accuracy: 0.1663\n",
            "Epoch 13/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0571 - accuracy: 0.1625 - val_loss: 2.0556 - val_accuracy: 0.1721\n",
            "Epoch 14/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0531 - accuracy: 0.1734 - val_loss: 2.0542 - val_accuracy: 0.1744\n",
            "Epoch 15/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0548 - accuracy: 0.1680 - val_loss: 2.0528 - val_accuracy: 0.1721\n",
            "Epoch 16/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0534 - accuracy: 0.1734 - val_loss: 2.0514 - val_accuracy: 0.1813\n",
            "Epoch 17/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0518 - accuracy: 0.1759 - val_loss: 2.0501 - val_accuracy: 0.1836\n",
            "Epoch 18/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0482 - accuracy: 0.1848 - val_loss: 2.0487 - val_accuracy: 0.1882\n",
            "Epoch 19/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0480 - accuracy: 0.1734 - val_loss: 2.0471 - val_accuracy: 0.1952\n",
            "Epoch 20/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0466 - accuracy: 0.1838 - val_loss: 2.0454 - val_accuracy: 0.1998\n",
            "Epoch 21/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0440 - accuracy: 0.1858 - val_loss: 2.0438 - val_accuracy: 0.2009\n",
            "Epoch 22/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0420 - accuracy: 0.1913 - val_loss: 2.0423 - val_accuracy: 0.1986\n",
            "Epoch 23/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0393 - accuracy: 0.1938 - val_loss: 2.0408 - val_accuracy: 0.2125\n",
            "Epoch 24/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0411 - accuracy: 0.1962 - val_loss: 2.0392 - val_accuracy: 0.2206\n",
            "Epoch 25/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0376 - accuracy: 0.1893 - val_loss: 2.0376 - val_accuracy: 0.2217\n",
            "Epoch 26/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0302 - accuracy: 0.2081 - val_loss: 2.0357 - val_accuracy: 0.2263\n",
            "Epoch 27/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0331 - accuracy: 0.1883 - val_loss: 2.0337 - val_accuracy: 0.2333\n",
            "Epoch 28/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0306 - accuracy: 0.1992 - val_loss: 2.0319 - val_accuracy: 0.2367\n",
            "Epoch 29/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0281 - accuracy: 0.1933 - val_loss: 2.0305 - val_accuracy: 0.2356\n",
            "Epoch 30/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 2.0274 - accuracy: 0.2007 - val_loss: 2.0289 - val_accuracy: 0.2286\n",
            "Epoch 31/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0258 - accuracy: 0.1987 - val_loss: 2.0270 - val_accuracy: 0.2263\n",
            "Epoch 32/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0238 - accuracy: 0.2161 - val_loss: 2.0252 - val_accuracy: 0.2263\n",
            "Epoch 33/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0229 - accuracy: 0.1982 - val_loss: 2.0238 - val_accuracy: 0.2286\n",
            "Epoch 34/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0221 - accuracy: 0.2096 - val_loss: 2.0218 - val_accuracy: 0.2286\n",
            "Epoch 35/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0232 - accuracy: 0.2022 - val_loss: 2.0200 - val_accuracy: 0.2298\n",
            "Epoch 36/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0165 - accuracy: 0.1947 - val_loss: 2.0183 - val_accuracy: 0.2344\n",
            "Epoch 37/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 2.0185 - accuracy: 0.1967 - val_loss: 2.0167 - val_accuracy: 0.2321\n",
            "Epoch 38/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0159 - accuracy: 0.2116 - val_loss: 2.0147 - val_accuracy: 0.2390\n",
            "Epoch 39/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0142 - accuracy: 0.2106 - val_loss: 2.0127 - val_accuracy: 0.2413\n",
            "Epoch 40/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 2.0124 - accuracy: 0.2106 - val_loss: 2.0111 - val_accuracy: 0.2413\n",
            "Epoch 41/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0144 - accuracy: 0.2151 - val_loss: 2.0087 - val_accuracy: 0.2471\n",
            "Epoch 42/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 2.0055 - accuracy: 0.2175 - val_loss: 2.0069 - val_accuracy: 0.2494\n",
            "Epoch 43/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0078 - accuracy: 0.2151 - val_loss: 2.0055 - val_accuracy: 0.2506\n",
            "Epoch 44/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 2.0045 - accuracy: 0.2180 - val_loss: 2.0026 - val_accuracy: 0.2506\n",
            "Epoch 45/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9969 - accuracy: 0.2329 - val_loss: 2.0008 - val_accuracy: 0.2540\n",
            "Epoch 46/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9975 - accuracy: 0.2314 - val_loss: 1.9984 - val_accuracy: 0.2564\n",
            "Epoch 47/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9941 - accuracy: 0.2289 - val_loss: 1.9956 - val_accuracy: 0.2575\n",
            "Epoch 48/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9923 - accuracy: 0.2289 - val_loss: 1.9933 - val_accuracy: 0.2564\n",
            "Epoch 49/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9881 - accuracy: 0.2319 - val_loss: 1.9904 - val_accuracy: 0.2540\n",
            "Epoch 50/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 1.9909 - accuracy: 0.2279 - val_loss: 1.9875 - val_accuracy: 0.2529\n",
            "Epoch 51/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.9892 - accuracy: 0.2260 - val_loss: 1.9843 - val_accuracy: 0.2610\n",
            "Epoch 52/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9853 - accuracy: 0.2359 - val_loss: 1.9814 - val_accuracy: 0.2702\n",
            "Epoch 53/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9833 - accuracy: 0.2195 - val_loss: 1.9783 - val_accuracy: 0.2702\n",
            "Epoch 54/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9764 - accuracy: 0.2398 - val_loss: 1.9752 - val_accuracy: 0.2737\n",
            "Epoch 55/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9742 - accuracy: 0.2418 - val_loss: 1.9715 - val_accuracy: 0.2818\n",
            "Epoch 56/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9733 - accuracy: 0.2379 - val_loss: 1.9680 - val_accuracy: 0.2841\n",
            "Epoch 57/1000\n",
            "127/127 [==============================] - 6s 44ms/step - loss: 1.9705 - accuracy: 0.2493 - val_loss: 1.9641 - val_accuracy: 0.2887\n",
            "Epoch 58/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9620 - accuracy: 0.2379 - val_loss: 1.9612 - val_accuracy: 0.2898\n",
            "Epoch 59/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9672 - accuracy: 0.2453 - val_loss: 1.9585 - val_accuracy: 0.2864\n",
            "Epoch 60/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9632 - accuracy: 0.2453 - val_loss: 1.9548 - val_accuracy: 0.2887\n",
            "Epoch 61/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9588 - accuracy: 0.2651 - val_loss: 1.9503 - val_accuracy: 0.2852\n",
            "Epoch 62/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9520 - accuracy: 0.2547 - val_loss: 1.9466 - val_accuracy: 0.2875\n",
            "Epoch 63/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9448 - accuracy: 0.2656 - val_loss: 1.9430 - val_accuracy: 0.2887\n",
            "Epoch 64/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9474 - accuracy: 0.2527 - val_loss: 1.9387 - val_accuracy: 0.2887\n",
            "Epoch 65/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9328 - accuracy: 0.2577 - val_loss: 1.9343 - val_accuracy: 0.2864\n",
            "Epoch 66/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9328 - accuracy: 0.2626 - val_loss: 1.9304 - val_accuracy: 0.2945\n",
            "Epoch 67/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9305 - accuracy: 0.2626 - val_loss: 1.9264 - val_accuracy: 0.2968\n",
            "Epoch 68/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9235 - accuracy: 0.2552 - val_loss: 1.9228 - val_accuracy: 0.2968\n",
            "Epoch 69/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9314 - accuracy: 0.2721 - val_loss: 1.9191 - val_accuracy: 0.2921\n",
            "Epoch 70/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9210 - accuracy: 0.2621 - val_loss: 1.9148 - val_accuracy: 0.3025\n",
            "Epoch 71/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9158 - accuracy: 0.2785 - val_loss: 1.9126 - val_accuracy: 0.3002\n",
            "Epoch 72/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9127 - accuracy: 0.2716 - val_loss: 1.9087 - val_accuracy: 0.2979\n",
            "Epoch 73/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9122 - accuracy: 0.2755 - val_loss: 1.9047 - val_accuracy: 0.3002\n",
            "Epoch 74/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9089 - accuracy: 0.2696 - val_loss: 1.9011 - val_accuracy: 0.2979\n",
            "Epoch 75/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.9015 - accuracy: 0.2735 - val_loss: 1.8971 - val_accuracy: 0.3037\n",
            "Epoch 76/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.9055 - accuracy: 0.2790 - val_loss: 1.8938 - val_accuracy: 0.3048\n",
            "Epoch 77/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8937 - accuracy: 0.2859 - val_loss: 1.8897 - val_accuracy: 0.3095\n",
            "Epoch 78/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8994 - accuracy: 0.2839 - val_loss: 1.8875 - val_accuracy: 0.3095\n",
            "Epoch 79/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8961 - accuracy: 0.2701 - val_loss: 1.8842 - val_accuracy: 0.3072\n",
            "Epoch 80/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8898 - accuracy: 0.2894 - val_loss: 1.8814 - val_accuracy: 0.3048\n",
            "Epoch 81/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.8876 - accuracy: 0.2696 - val_loss: 1.8784 - val_accuracy: 0.3060\n",
            "Epoch 82/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.8908 - accuracy: 0.2775 - val_loss: 1.8749 - val_accuracy: 0.3199\n",
            "Epoch 83/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.8846 - accuracy: 0.2830 - val_loss: 1.8717 - val_accuracy: 0.3176\n",
            "Epoch 84/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8806 - accuracy: 0.2854 - val_loss: 1.8696 - val_accuracy: 0.3152\n",
            "Epoch 85/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8820 - accuracy: 0.2795 - val_loss: 1.8675 - val_accuracy: 0.3072\n",
            "Epoch 86/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8715 - accuracy: 0.2854 - val_loss: 1.8652 - val_accuracy: 0.3095\n",
            "Epoch 87/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8737 - accuracy: 0.2790 - val_loss: 1.8623 - val_accuracy: 0.3152\n",
            "Epoch 88/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8787 - accuracy: 0.2914 - val_loss: 1.8600 - val_accuracy: 0.3141\n",
            "Epoch 89/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8754 - accuracy: 0.2948 - val_loss: 1.8579 - val_accuracy: 0.3164\n",
            "Epoch 90/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8703 - accuracy: 0.2899 - val_loss: 1.8555 - val_accuracy: 0.3222\n",
            "Epoch 91/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8704 - accuracy: 0.2958 - val_loss: 1.8531 - val_accuracy: 0.3141\n",
            "Epoch 92/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8701 - accuracy: 0.2834 - val_loss: 1.8509 - val_accuracy: 0.3176\n",
            "Epoch 93/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8682 - accuracy: 0.2899 - val_loss: 1.8482 - val_accuracy: 0.3187\n",
            "Epoch 94/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8610 - accuracy: 0.2978 - val_loss: 1.8468 - val_accuracy: 0.3245\n",
            "Epoch 95/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8612 - accuracy: 0.2983 - val_loss: 1.8445 - val_accuracy: 0.3268\n",
            "Epoch 96/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8597 - accuracy: 0.2864 - val_loss: 1.8428 - val_accuracy: 0.3256\n",
            "Epoch 97/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8617 - accuracy: 0.2889 - val_loss: 1.8409 - val_accuracy: 0.3256\n",
            "Epoch 98/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8518 - accuracy: 0.2864 - val_loss: 1.8378 - val_accuracy: 0.3279\n",
            "Epoch 99/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8602 - accuracy: 0.2810 - val_loss: 1.8369 - val_accuracy: 0.3279\n",
            "Epoch 100/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8522 - accuracy: 0.2909 - val_loss: 1.8345 - val_accuracy: 0.3245\n",
            "Epoch 101/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8606 - accuracy: 0.2830 - val_loss: 1.8342 - val_accuracy: 0.3256\n",
            "Epoch 102/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8545 - accuracy: 0.3008 - val_loss: 1.8309 - val_accuracy: 0.3349\n",
            "Epoch 103/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.8439 - accuracy: 0.3008 - val_loss: 1.8291 - val_accuracy: 0.3326\n",
            "Epoch 104/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.8553 - accuracy: 0.2968 - val_loss: 1.8284 - val_accuracy: 0.3303\n",
            "Epoch 105/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.8397 - accuracy: 0.3112 - val_loss: 1.8257 - val_accuracy: 0.3314\n",
            "Epoch 106/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8455 - accuracy: 0.3043 - val_loss: 1.8238 - val_accuracy: 0.3291\n",
            "Epoch 107/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8430 - accuracy: 0.2948 - val_loss: 1.8230 - val_accuracy: 0.3337\n",
            "Epoch 108/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8447 - accuracy: 0.3023 - val_loss: 1.8232 - val_accuracy: 0.3245\n",
            "Epoch 109/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8390 - accuracy: 0.3048 - val_loss: 1.8208 - val_accuracy: 0.3187\n",
            "Epoch 110/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8460 - accuracy: 0.2958 - val_loss: 1.8202 - val_accuracy: 0.3268\n",
            "Epoch 111/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8382 - accuracy: 0.2973 - val_loss: 1.8180 - val_accuracy: 0.3199\n",
            "Epoch 112/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8518 - accuracy: 0.2958 - val_loss: 1.8173 - val_accuracy: 0.3256\n",
            "Epoch 113/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8354 - accuracy: 0.3147 - val_loss: 1.8154 - val_accuracy: 0.3210\n",
            "Epoch 114/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8417 - accuracy: 0.2988 - val_loss: 1.8134 - val_accuracy: 0.3233\n",
            "Epoch 115/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8381 - accuracy: 0.3048 - val_loss: 1.8124 - val_accuracy: 0.3233\n",
            "Epoch 116/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8419 - accuracy: 0.2899 - val_loss: 1.8110 - val_accuracy: 0.3245\n",
            "Epoch 117/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8258 - accuracy: 0.3107 - val_loss: 1.8100 - val_accuracy: 0.3233\n",
            "Epoch 118/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8239 - accuracy: 0.3028 - val_loss: 1.8111 - val_accuracy: 0.3233\n",
            "Epoch 119/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8354 - accuracy: 0.3048 - val_loss: 1.8064 - val_accuracy: 0.3268\n",
            "Epoch 120/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8257 - accuracy: 0.2939 - val_loss: 1.8061 - val_accuracy: 0.3291\n",
            "Epoch 121/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8256 - accuracy: 0.3048 - val_loss: 1.8041 - val_accuracy: 0.3268\n",
            "Epoch 122/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8220 - accuracy: 0.3097 - val_loss: 1.8026 - val_accuracy: 0.3187\n",
            "Epoch 123/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8244 - accuracy: 0.3028 - val_loss: 1.8014 - val_accuracy: 0.3222\n",
            "Epoch 124/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8356 - accuracy: 0.3043 - val_loss: 1.7992 - val_accuracy: 0.3210\n",
            "Epoch 125/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8233 - accuracy: 0.3157 - val_loss: 1.7983 - val_accuracy: 0.3268\n",
            "Epoch 126/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8253 - accuracy: 0.2924 - val_loss: 1.7985 - val_accuracy: 0.3187\n",
            "Epoch 127/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8035 - accuracy: 0.3097 - val_loss: 1.7963 - val_accuracy: 0.3268\n",
            "Epoch 128/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8233 - accuracy: 0.2998 - val_loss: 1.7965 - val_accuracy: 0.3210\n",
            "Epoch 129/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8204 - accuracy: 0.2958 - val_loss: 1.7954 - val_accuracy: 0.3141\n",
            "Epoch 130/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8178 - accuracy: 0.3003 - val_loss: 1.7938 - val_accuracy: 0.3152\n",
            "Epoch 131/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8316 - accuracy: 0.2914 - val_loss: 1.7916 - val_accuracy: 0.3176\n",
            "Epoch 132/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8185 - accuracy: 0.3137 - val_loss: 1.7912 - val_accuracy: 0.3176\n",
            "Epoch 133/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8164 - accuracy: 0.3171 - val_loss: 1.7916 - val_accuracy: 0.3164\n",
            "Epoch 134/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8158 - accuracy: 0.3053 - val_loss: 1.7891 - val_accuracy: 0.3187\n",
            "Epoch 135/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8089 - accuracy: 0.3147 - val_loss: 1.7869 - val_accuracy: 0.3176\n",
            "Epoch 136/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8119 - accuracy: 0.3186 - val_loss: 1.7860 - val_accuracy: 0.3164\n",
            "Epoch 137/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8114 - accuracy: 0.3127 - val_loss: 1.7848 - val_accuracy: 0.3187\n",
            "Epoch 138/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.8141 - accuracy: 0.3097 - val_loss: 1.7837 - val_accuracy: 0.3291\n",
            "Epoch 139/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8071 - accuracy: 0.3028 - val_loss: 1.7825 - val_accuracy: 0.3268\n",
            "Epoch 140/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8081 - accuracy: 0.3062 - val_loss: 1.7812 - val_accuracy: 0.3245\n",
            "Epoch 141/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8132 - accuracy: 0.3082 - val_loss: 1.7793 - val_accuracy: 0.3199\n",
            "Epoch 142/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8125 - accuracy: 0.3077 - val_loss: 1.7788 - val_accuracy: 0.3268\n",
            "Epoch 143/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.8093 - accuracy: 0.3082 - val_loss: 1.7778 - val_accuracy: 0.3210\n",
            "Epoch 144/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7972 - accuracy: 0.3176 - val_loss: 1.7766 - val_accuracy: 0.3233\n",
            "Epoch 145/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7999 - accuracy: 0.3147 - val_loss: 1.7789 - val_accuracy: 0.3233\n",
            "Epoch 146/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7985 - accuracy: 0.3087 - val_loss: 1.7762 - val_accuracy: 0.3233\n",
            "Epoch 147/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8022 - accuracy: 0.3152 - val_loss: 1.7743 - val_accuracy: 0.3256\n",
            "Epoch 148/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8075 - accuracy: 0.2988 - val_loss: 1.7720 - val_accuracy: 0.3314\n",
            "Epoch 149/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.8026 - accuracy: 0.3142 - val_loss: 1.7709 - val_accuracy: 0.3268\n",
            "Epoch 150/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7998 - accuracy: 0.3152 - val_loss: 1.7701 - val_accuracy: 0.3279\n",
            "Epoch 151/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.8022 - accuracy: 0.3067 - val_loss: 1.7689 - val_accuracy: 0.3303\n",
            "Epoch 152/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7958 - accuracy: 0.3290 - val_loss: 1.7680 - val_accuracy: 0.3314\n",
            "Epoch 153/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7990 - accuracy: 0.3028 - val_loss: 1.7683 - val_accuracy: 0.3233\n",
            "Epoch 154/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7967 - accuracy: 0.3132 - val_loss: 1.7673 - val_accuracy: 0.3314\n",
            "Epoch 155/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7962 - accuracy: 0.3102 - val_loss: 1.7662 - val_accuracy: 0.3303\n",
            "Epoch 156/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7932 - accuracy: 0.3053 - val_loss: 1.7670 - val_accuracy: 0.3222\n",
            "Epoch 157/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7870 - accuracy: 0.3206 - val_loss: 1.7644 - val_accuracy: 0.3279\n",
            "Epoch 158/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7887 - accuracy: 0.3102 - val_loss: 1.7625 - val_accuracy: 0.3303\n",
            "Epoch 159/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7958 - accuracy: 0.3107 - val_loss: 1.7625 - val_accuracy: 0.3256\n",
            "Epoch 160/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7943 - accuracy: 0.3102 - val_loss: 1.7618 - val_accuracy: 0.3326\n",
            "Epoch 161/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7880 - accuracy: 0.3147 - val_loss: 1.7602 - val_accuracy: 0.3337\n",
            "Epoch 162/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7915 - accuracy: 0.3231 - val_loss: 1.7595 - val_accuracy: 0.3256\n",
            "Epoch 163/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7842 - accuracy: 0.3147 - val_loss: 1.7577 - val_accuracy: 0.3303\n",
            "Epoch 164/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7775 - accuracy: 0.3157 - val_loss: 1.7574 - val_accuracy: 0.3256\n",
            "Epoch 165/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7872 - accuracy: 0.3186 - val_loss: 1.7564 - val_accuracy: 0.3256\n",
            "Epoch 166/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7860 - accuracy: 0.3092 - val_loss: 1.7563 - val_accuracy: 0.3268\n",
            "Epoch 167/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7870 - accuracy: 0.3152 - val_loss: 1.7564 - val_accuracy: 0.3268\n",
            "Epoch 168/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7858 - accuracy: 0.3271 - val_loss: 1.7545 - val_accuracy: 0.3233\n",
            "Epoch 169/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7827 - accuracy: 0.3251 - val_loss: 1.7534 - val_accuracy: 0.3291\n",
            "Epoch 170/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7829 - accuracy: 0.3137 - val_loss: 1.7519 - val_accuracy: 0.3349\n",
            "Epoch 171/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7825 - accuracy: 0.3271 - val_loss: 1.7511 - val_accuracy: 0.3337\n",
            "Epoch 172/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7809 - accuracy: 0.3266 - val_loss: 1.7513 - val_accuracy: 0.3337\n",
            "Epoch 173/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7730 - accuracy: 0.3271 - val_loss: 1.7519 - val_accuracy: 0.3268\n",
            "Epoch 174/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7742 - accuracy: 0.3251 - val_loss: 1.7505 - val_accuracy: 0.3279\n",
            "Epoch 175/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7808 - accuracy: 0.3241 - val_loss: 1.7489 - val_accuracy: 0.3349\n",
            "Epoch 176/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7826 - accuracy: 0.3191 - val_loss: 1.7485 - val_accuracy: 0.3279\n",
            "Epoch 177/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7841 - accuracy: 0.3211 - val_loss: 1.7484 - val_accuracy: 0.3268\n",
            "Epoch 178/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7666 - accuracy: 0.3251 - val_loss: 1.7484 - val_accuracy: 0.3360\n",
            "Epoch 179/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7789 - accuracy: 0.3102 - val_loss: 1.7469 - val_accuracy: 0.3395\n",
            "Epoch 180/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7714 - accuracy: 0.3280 - val_loss: 1.7480 - val_accuracy: 0.3268\n",
            "Epoch 181/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7847 - accuracy: 0.3295 - val_loss: 1.7465 - val_accuracy: 0.3245\n",
            "Epoch 182/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7624 - accuracy: 0.3246 - val_loss: 1.7442 - val_accuracy: 0.3383\n",
            "Epoch 183/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7594 - accuracy: 0.3285 - val_loss: 1.7441 - val_accuracy: 0.3360\n",
            "Epoch 184/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7803 - accuracy: 0.3142 - val_loss: 1.7447 - val_accuracy: 0.3291\n",
            "Epoch 185/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7782 - accuracy: 0.3216 - val_loss: 1.7430 - val_accuracy: 0.3372\n",
            "Epoch 186/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7631 - accuracy: 0.3380 - val_loss: 1.7428 - val_accuracy: 0.3372\n",
            "Epoch 187/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7624 - accuracy: 0.3340 - val_loss: 1.7423 - val_accuracy: 0.3453\n",
            "Epoch 188/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7672 - accuracy: 0.3236 - val_loss: 1.7413 - val_accuracy: 0.3383\n",
            "Epoch 189/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7639 - accuracy: 0.3290 - val_loss: 1.7408 - val_accuracy: 0.3360\n",
            "Epoch 190/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7640 - accuracy: 0.3251 - val_loss: 1.7393 - val_accuracy: 0.3395\n",
            "Epoch 191/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7669 - accuracy: 0.3375 - val_loss: 1.7385 - val_accuracy: 0.3487\n",
            "Epoch 192/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7697 - accuracy: 0.3241 - val_loss: 1.7382 - val_accuracy: 0.3383\n",
            "Epoch 193/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7596 - accuracy: 0.3276 - val_loss: 1.7379 - val_accuracy: 0.3372\n",
            "Epoch 194/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7633 - accuracy: 0.3345 - val_loss: 1.7373 - val_accuracy: 0.3395\n",
            "Epoch 195/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7544 - accuracy: 0.3310 - val_loss: 1.7360 - val_accuracy: 0.3383\n",
            "Epoch 196/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7583 - accuracy: 0.3266 - val_loss: 1.7349 - val_accuracy: 0.3441\n",
            "Epoch 197/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7595 - accuracy: 0.3345 - val_loss: 1.7357 - val_accuracy: 0.3430\n",
            "Epoch 198/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7648 - accuracy: 0.3261 - val_loss: 1.7344 - val_accuracy: 0.3453\n",
            "Epoch 199/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7623 - accuracy: 0.3295 - val_loss: 1.7343 - val_accuracy: 0.3383\n",
            "Epoch 200/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7603 - accuracy: 0.3305 - val_loss: 1.7349 - val_accuracy: 0.3430\n",
            "Epoch 201/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7637 - accuracy: 0.3280 - val_loss: 1.7331 - val_accuracy: 0.3453\n",
            "Epoch 202/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7588 - accuracy: 0.3251 - val_loss: 1.7331 - val_accuracy: 0.3418\n",
            "Epoch 203/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7527 - accuracy: 0.3305 - val_loss: 1.7323 - val_accuracy: 0.3430\n",
            "Epoch 204/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7653 - accuracy: 0.3380 - val_loss: 1.7320 - val_accuracy: 0.3406\n",
            "Epoch 205/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7512 - accuracy: 0.3340 - val_loss: 1.7306 - val_accuracy: 0.3441\n",
            "Epoch 206/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.7628 - accuracy: 0.3280 - val_loss: 1.7307 - val_accuracy: 0.3418\n",
            "Epoch 207/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7560 - accuracy: 0.3394 - val_loss: 1.7302 - val_accuracy: 0.3441\n",
            "Epoch 208/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7601 - accuracy: 0.3211 - val_loss: 1.7296 - val_accuracy: 0.3372\n",
            "Epoch 209/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7594 - accuracy: 0.3320 - val_loss: 1.7287 - val_accuracy: 0.3372\n",
            "Epoch 210/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7512 - accuracy: 0.3449 - val_loss: 1.7274 - val_accuracy: 0.3441\n",
            "Epoch 211/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7608 - accuracy: 0.3246 - val_loss: 1.7273 - val_accuracy: 0.3441\n",
            "Epoch 212/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7596 - accuracy: 0.3320 - val_loss: 1.7279 - val_accuracy: 0.3395\n",
            "Epoch 213/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7483 - accuracy: 0.3409 - val_loss: 1.7269 - val_accuracy: 0.3430\n",
            "Epoch 214/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7495 - accuracy: 0.3385 - val_loss: 1.7269 - val_accuracy: 0.3453\n",
            "Epoch 215/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7532 - accuracy: 0.3330 - val_loss: 1.7264 - val_accuracy: 0.3406\n",
            "Epoch 216/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7499 - accuracy: 0.3330 - val_loss: 1.7255 - val_accuracy: 0.3418\n",
            "Epoch 217/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7530 - accuracy: 0.3295 - val_loss: 1.7256 - val_accuracy: 0.3406\n",
            "Epoch 218/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7371 - accuracy: 0.3305 - val_loss: 1.7246 - val_accuracy: 0.3441\n",
            "Epoch 219/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7488 - accuracy: 0.3360 - val_loss: 1.7248 - val_accuracy: 0.3383\n",
            "Epoch 220/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7421 - accuracy: 0.3424 - val_loss: 1.7240 - val_accuracy: 0.3453\n",
            "Epoch 221/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7411 - accuracy: 0.3399 - val_loss: 1.7219 - val_accuracy: 0.3418\n",
            "Epoch 222/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7446 - accuracy: 0.3424 - val_loss: 1.7219 - val_accuracy: 0.3360\n",
            "Epoch 223/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7507 - accuracy: 0.3360 - val_loss: 1.7226 - val_accuracy: 0.3418\n",
            "Epoch 224/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7446 - accuracy: 0.3236 - val_loss: 1.7219 - val_accuracy: 0.3360\n",
            "Epoch 225/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7444 - accuracy: 0.3389 - val_loss: 1.7209 - val_accuracy: 0.3395\n",
            "Epoch 226/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7500 - accuracy: 0.3375 - val_loss: 1.7206 - val_accuracy: 0.3418\n",
            "Epoch 227/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7508 - accuracy: 0.3256 - val_loss: 1.7202 - val_accuracy: 0.3395\n",
            "Epoch 228/1000\n",
            "127/127 [==============================] - 6s 45ms/step - loss: 1.7405 - accuracy: 0.3454 - val_loss: 1.7200 - val_accuracy: 0.3406\n",
            "Epoch 229/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7345 - accuracy: 0.3419 - val_loss: 1.7201 - val_accuracy: 0.3383\n",
            "Epoch 230/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7312 - accuracy: 0.3484 - val_loss: 1.7188 - val_accuracy: 0.3395\n",
            "Epoch 231/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7407 - accuracy: 0.3365 - val_loss: 1.7182 - val_accuracy: 0.3383\n",
            "Epoch 232/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7408 - accuracy: 0.3365 - val_loss: 1.7174 - val_accuracy: 0.3395\n",
            "Epoch 233/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7374 - accuracy: 0.3370 - val_loss: 1.7177 - val_accuracy: 0.3406\n",
            "Epoch 234/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7424 - accuracy: 0.3355 - val_loss: 1.7171 - val_accuracy: 0.3395\n",
            "Epoch 235/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7414 - accuracy: 0.3300 - val_loss: 1.7157 - val_accuracy: 0.3453\n",
            "Epoch 236/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7323 - accuracy: 0.3414 - val_loss: 1.7155 - val_accuracy: 0.3418\n",
            "Epoch 237/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7354 - accuracy: 0.3414 - val_loss: 1.7158 - val_accuracy: 0.3406\n",
            "Epoch 238/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7295 - accuracy: 0.3503 - val_loss: 1.7150 - val_accuracy: 0.3395\n",
            "Epoch 239/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7369 - accuracy: 0.3419 - val_loss: 1.7157 - val_accuracy: 0.3383\n",
            "Epoch 240/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7367 - accuracy: 0.3399 - val_loss: 1.7144 - val_accuracy: 0.3441\n",
            "Epoch 241/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7314 - accuracy: 0.3434 - val_loss: 1.7134 - val_accuracy: 0.3383\n",
            "Epoch 242/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7313 - accuracy: 0.3414 - val_loss: 1.7123 - val_accuracy: 0.3418\n",
            "Epoch 243/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7306 - accuracy: 0.3290 - val_loss: 1.7130 - val_accuracy: 0.3453\n",
            "Epoch 244/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7331 - accuracy: 0.3474 - val_loss: 1.7121 - val_accuracy: 0.3406\n",
            "Epoch 245/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7432 - accuracy: 0.3340 - val_loss: 1.7125 - val_accuracy: 0.3441\n",
            "Epoch 246/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7304 - accuracy: 0.3394 - val_loss: 1.7125 - val_accuracy: 0.3395\n",
            "Epoch 247/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7255 - accuracy: 0.3394 - val_loss: 1.7111 - val_accuracy: 0.3406\n",
            "Epoch 248/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7309 - accuracy: 0.3380 - val_loss: 1.7117 - val_accuracy: 0.3441\n",
            "Epoch 249/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7303 - accuracy: 0.3365 - val_loss: 1.7115 - val_accuracy: 0.3441\n",
            "Epoch 250/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7284 - accuracy: 0.3444 - val_loss: 1.7098 - val_accuracy: 0.3453\n",
            "Epoch 251/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.7232 - accuracy: 0.3434 - val_loss: 1.7105 - val_accuracy: 0.3372\n",
            "Epoch 252/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7388 - accuracy: 0.3271 - val_loss: 1.7095 - val_accuracy: 0.3487\n",
            "Epoch 253/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7194 - accuracy: 0.3459 - val_loss: 1.7091 - val_accuracy: 0.3441\n",
            "Epoch 254/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7219 - accuracy: 0.3538 - val_loss: 1.7085 - val_accuracy: 0.3406\n",
            "Epoch 255/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7224 - accuracy: 0.3518 - val_loss: 1.7082 - val_accuracy: 0.3453\n",
            "Epoch 256/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7341 - accuracy: 0.3404 - val_loss: 1.7081 - val_accuracy: 0.3418\n",
            "Epoch 257/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.7166 - accuracy: 0.3553 - val_loss: 1.7073 - val_accuracy: 0.3453\n",
            "Epoch 258/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7174 - accuracy: 0.3444 - val_loss: 1.7064 - val_accuracy: 0.3464\n",
            "Epoch 259/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7201 - accuracy: 0.3484 - val_loss: 1.7066 - val_accuracy: 0.3476\n",
            "Epoch 260/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7194 - accuracy: 0.3429 - val_loss: 1.7061 - val_accuracy: 0.3499\n",
            "Epoch 261/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7283 - accuracy: 0.3365 - val_loss: 1.7059 - val_accuracy: 0.3510\n",
            "Epoch 262/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.7215 - accuracy: 0.3434 - val_loss: 1.7062 - val_accuracy: 0.3464\n",
            "Epoch 263/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7206 - accuracy: 0.3389 - val_loss: 1.7057 - val_accuracy: 0.3406\n",
            "Epoch 264/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7251 - accuracy: 0.3355 - val_loss: 1.7045 - val_accuracy: 0.3441\n",
            "Epoch 265/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.7188 - accuracy: 0.3484 - val_loss: 1.7033 - val_accuracy: 0.3406\n",
            "Epoch 266/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7188 - accuracy: 0.3399 - val_loss: 1.7037 - val_accuracy: 0.3476\n",
            "Epoch 267/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7156 - accuracy: 0.3508 - val_loss: 1.7038 - val_accuracy: 0.3453\n",
            "Epoch 268/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7201 - accuracy: 0.3578 - val_loss: 1.7032 - val_accuracy: 0.3464\n",
            "Epoch 269/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7142 - accuracy: 0.3553 - val_loss: 1.7023 - val_accuracy: 0.3476\n",
            "Epoch 270/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7199 - accuracy: 0.3424 - val_loss: 1.7015 - val_accuracy: 0.3464\n",
            "Epoch 271/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7164 - accuracy: 0.3464 - val_loss: 1.7017 - val_accuracy: 0.3453\n",
            "Epoch 272/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7168 - accuracy: 0.3479 - val_loss: 1.7016 - val_accuracy: 0.3487\n",
            "Epoch 273/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7086 - accuracy: 0.3389 - val_loss: 1.6999 - val_accuracy: 0.3464\n",
            "Epoch 274/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7145 - accuracy: 0.3419 - val_loss: 1.7004 - val_accuracy: 0.3383\n",
            "Epoch 275/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7104 - accuracy: 0.3494 - val_loss: 1.6992 - val_accuracy: 0.3476\n",
            "Epoch 276/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7196 - accuracy: 0.3548 - val_loss: 1.6995 - val_accuracy: 0.3453\n",
            "Epoch 277/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7199 - accuracy: 0.3399 - val_loss: 1.6977 - val_accuracy: 0.3418\n",
            "Epoch 278/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7143 - accuracy: 0.3469 - val_loss: 1.6973 - val_accuracy: 0.3418\n",
            "Epoch 279/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7106 - accuracy: 0.3513 - val_loss: 1.6977 - val_accuracy: 0.3453\n",
            "Epoch 280/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7062 - accuracy: 0.3434 - val_loss: 1.6966 - val_accuracy: 0.3441\n",
            "Epoch 281/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7110 - accuracy: 0.3508 - val_loss: 1.6960 - val_accuracy: 0.3476\n",
            "Epoch 282/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7178 - accuracy: 0.3568 - val_loss: 1.6971 - val_accuracy: 0.3487\n",
            "Epoch 283/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7143 - accuracy: 0.3642 - val_loss: 1.6958 - val_accuracy: 0.3533\n",
            "Epoch 284/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7016 - accuracy: 0.3573 - val_loss: 1.6959 - val_accuracy: 0.3510\n",
            "Epoch 285/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7059 - accuracy: 0.3494 - val_loss: 1.6947 - val_accuracy: 0.3487\n",
            "Epoch 286/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.7040 - accuracy: 0.3563 - val_loss: 1.6939 - val_accuracy: 0.3476\n",
            "Epoch 287/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7055 - accuracy: 0.3538 - val_loss: 1.6944 - val_accuracy: 0.3430\n",
            "Epoch 288/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7116 - accuracy: 0.3503 - val_loss: 1.6957 - val_accuracy: 0.3464\n",
            "Epoch 289/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7032 - accuracy: 0.3479 - val_loss: 1.6928 - val_accuracy: 0.3453\n",
            "Epoch 290/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7046 - accuracy: 0.3598 - val_loss: 1.6930 - val_accuracy: 0.3487\n",
            "Epoch 291/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7070 - accuracy: 0.3543 - val_loss: 1.6917 - val_accuracy: 0.3453\n",
            "Epoch 292/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.7085 - accuracy: 0.3523 - val_loss: 1.6910 - val_accuracy: 0.3476\n",
            "Epoch 293/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7046 - accuracy: 0.3503 - val_loss: 1.6902 - val_accuracy: 0.3453\n",
            "Epoch 294/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7089 - accuracy: 0.3573 - val_loss: 1.6901 - val_accuracy: 0.3476\n",
            "Epoch 295/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7096 - accuracy: 0.3459 - val_loss: 1.6893 - val_accuracy: 0.3545\n",
            "Epoch 296/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.6967 - accuracy: 0.3469 - val_loss: 1.6893 - val_accuracy: 0.3476\n",
            "Epoch 297/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6922 - accuracy: 0.3573 - val_loss: 1.6894 - val_accuracy: 0.3545\n",
            "Epoch 298/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7008 - accuracy: 0.3494 - val_loss: 1.6877 - val_accuracy: 0.3557\n",
            "Epoch 299/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6927 - accuracy: 0.3583 - val_loss: 1.6862 - val_accuracy: 0.3487\n",
            "Epoch 300/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.7070 - accuracy: 0.3484 - val_loss: 1.6863 - val_accuracy: 0.3487\n",
            "Epoch 301/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.7087 - accuracy: 0.3499 - val_loss: 1.6865 - val_accuracy: 0.3568\n",
            "Epoch 302/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6860 - accuracy: 0.3573 - val_loss: 1.6850 - val_accuracy: 0.3522\n",
            "Epoch 303/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6927 - accuracy: 0.3558 - val_loss: 1.6855 - val_accuracy: 0.3441\n",
            "Epoch 304/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6913 - accuracy: 0.3533 - val_loss: 1.6837 - val_accuracy: 0.3591\n",
            "Epoch 305/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.6869 - accuracy: 0.3593 - val_loss: 1.6845 - val_accuracy: 0.3476\n",
            "Epoch 306/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6936 - accuracy: 0.3563 - val_loss: 1.6839 - val_accuracy: 0.3510\n",
            "Epoch 307/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6905 - accuracy: 0.3503 - val_loss: 1.6829 - val_accuracy: 0.3464\n",
            "Epoch 308/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6837 - accuracy: 0.3558 - val_loss: 1.6820 - val_accuracy: 0.3533\n",
            "Epoch 309/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6975 - accuracy: 0.3528 - val_loss: 1.6810 - val_accuracy: 0.3522\n",
            "Epoch 310/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6871 - accuracy: 0.3518 - val_loss: 1.6802 - val_accuracy: 0.3522\n",
            "Epoch 311/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6881 - accuracy: 0.3543 - val_loss: 1.6796 - val_accuracy: 0.3557\n",
            "Epoch 312/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6940 - accuracy: 0.3578 - val_loss: 1.6796 - val_accuracy: 0.3533\n",
            "Epoch 313/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.6786 - accuracy: 0.3647 - val_loss: 1.6827 - val_accuracy: 0.3487\n",
            "Epoch 314/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6809 - accuracy: 0.3657 - val_loss: 1.6788 - val_accuracy: 0.3580\n",
            "Epoch 315/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6886 - accuracy: 0.3662 - val_loss: 1.6772 - val_accuracy: 0.3557\n",
            "Epoch 316/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6835 - accuracy: 0.3677 - val_loss: 1.6798 - val_accuracy: 0.3487\n",
            "Epoch 317/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6866 - accuracy: 0.3642 - val_loss: 1.6782 - val_accuracy: 0.3499\n",
            "Epoch 318/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6830 - accuracy: 0.3652 - val_loss: 1.6770 - val_accuracy: 0.3510\n",
            "Epoch 319/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6845 - accuracy: 0.3548 - val_loss: 1.6770 - val_accuracy: 0.3522\n",
            "Epoch 320/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.6811 - accuracy: 0.3563 - val_loss: 1.6761 - val_accuracy: 0.3522\n",
            "Epoch 321/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6827 - accuracy: 0.3682 - val_loss: 1.6758 - val_accuracy: 0.3533\n",
            "Epoch 322/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6810 - accuracy: 0.3707 - val_loss: 1.6763 - val_accuracy: 0.3510\n",
            "Epoch 323/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6777 - accuracy: 0.3598 - val_loss: 1.6763 - val_accuracy: 0.3441\n",
            "Epoch 324/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.6827 - accuracy: 0.3632 - val_loss: 1.6744 - val_accuracy: 0.3499\n",
            "Epoch 325/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6725 - accuracy: 0.3622 - val_loss: 1.6731 - val_accuracy: 0.3557\n",
            "Epoch 326/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6787 - accuracy: 0.3583 - val_loss: 1.6741 - val_accuracy: 0.3522\n",
            "Epoch 327/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.6610 - accuracy: 0.3702 - val_loss: 1.6728 - val_accuracy: 0.3487\n",
            "Epoch 328/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6691 - accuracy: 0.3627 - val_loss: 1.6723 - val_accuracy: 0.3568\n",
            "Epoch 329/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6775 - accuracy: 0.3697 - val_loss: 1.6722 - val_accuracy: 0.3545\n",
            "Epoch 330/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6709 - accuracy: 0.3513 - val_loss: 1.6702 - val_accuracy: 0.3580\n",
            "Epoch 331/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6763 - accuracy: 0.3667 - val_loss: 1.6704 - val_accuracy: 0.3522\n",
            "Epoch 332/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6714 - accuracy: 0.3603 - val_loss: 1.6712 - val_accuracy: 0.3545\n",
            "Epoch 333/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6734 - accuracy: 0.3603 - val_loss: 1.6697 - val_accuracy: 0.3533\n",
            "Epoch 334/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6743 - accuracy: 0.3632 - val_loss: 1.6682 - val_accuracy: 0.3522\n",
            "Epoch 335/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6562 - accuracy: 0.3726 - val_loss: 1.6683 - val_accuracy: 0.3499\n",
            "Epoch 336/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6714 - accuracy: 0.3627 - val_loss: 1.6673 - val_accuracy: 0.3499\n",
            "Epoch 337/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6705 - accuracy: 0.3692 - val_loss: 1.6671 - val_accuracy: 0.3533\n",
            "Epoch 338/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6705 - accuracy: 0.3603 - val_loss: 1.6658 - val_accuracy: 0.3522\n",
            "Epoch 339/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6574 - accuracy: 0.3637 - val_loss: 1.6660 - val_accuracy: 0.3522\n",
            "Epoch 340/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6663 - accuracy: 0.3687 - val_loss: 1.6662 - val_accuracy: 0.3557\n",
            "Epoch 341/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6622 - accuracy: 0.3608 - val_loss: 1.6655 - val_accuracy: 0.3522\n",
            "Epoch 342/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6641 - accuracy: 0.3647 - val_loss: 1.6648 - val_accuracy: 0.3522\n",
            "Epoch 343/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6646 - accuracy: 0.3622 - val_loss: 1.6641 - val_accuracy: 0.3533\n",
            "Epoch 344/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6569 - accuracy: 0.3672 - val_loss: 1.6632 - val_accuracy: 0.3603\n",
            "Epoch 345/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.6595 - accuracy: 0.3603 - val_loss: 1.6628 - val_accuracy: 0.3603\n",
            "Epoch 346/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6577 - accuracy: 0.3662 - val_loss: 1.6624 - val_accuracy: 0.3580\n",
            "Epoch 347/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6464 - accuracy: 0.3717 - val_loss: 1.6618 - val_accuracy: 0.3591\n",
            "Epoch 348/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6539 - accuracy: 0.3731 - val_loss: 1.6627 - val_accuracy: 0.3557\n",
            "Epoch 349/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6446 - accuracy: 0.3741 - val_loss: 1.6598 - val_accuracy: 0.3545\n",
            "Epoch 350/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6450 - accuracy: 0.3756 - val_loss: 1.6612 - val_accuracy: 0.3568\n",
            "Epoch 351/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6451 - accuracy: 0.3583 - val_loss: 1.6604 - val_accuracy: 0.3557\n",
            "Epoch 352/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6464 - accuracy: 0.3751 - val_loss: 1.6602 - val_accuracy: 0.3591\n",
            "Epoch 353/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6482 - accuracy: 0.3736 - val_loss: 1.6588 - val_accuracy: 0.3522\n",
            "Epoch 354/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.6383 - accuracy: 0.3791 - val_loss: 1.6588 - val_accuracy: 0.3499\n",
            "Epoch 355/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6554 - accuracy: 0.3717 - val_loss: 1.6584 - val_accuracy: 0.3545\n",
            "Epoch 356/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6518 - accuracy: 0.3786 - val_loss: 1.6567 - val_accuracy: 0.3522\n",
            "Epoch 357/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6445 - accuracy: 0.3697 - val_loss: 1.6557 - val_accuracy: 0.3557\n",
            "Epoch 358/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6445 - accuracy: 0.3717 - val_loss: 1.6564 - val_accuracy: 0.3580\n",
            "Epoch 359/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6553 - accuracy: 0.3736 - val_loss: 1.6538 - val_accuracy: 0.3510\n",
            "Epoch 360/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6491 - accuracy: 0.3726 - val_loss: 1.6549 - val_accuracy: 0.3545\n",
            "Epoch 361/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6419 - accuracy: 0.3831 - val_loss: 1.6562 - val_accuracy: 0.3441\n",
            "Epoch 362/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6437 - accuracy: 0.3667 - val_loss: 1.6537 - val_accuracy: 0.3441\n",
            "Epoch 363/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6423 - accuracy: 0.3736 - val_loss: 1.6530 - val_accuracy: 0.3545\n",
            "Epoch 364/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6412 - accuracy: 0.3726 - val_loss: 1.6523 - val_accuracy: 0.3418\n",
            "Epoch 365/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6415 - accuracy: 0.3821 - val_loss: 1.6528 - val_accuracy: 0.3545\n",
            "Epoch 366/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6379 - accuracy: 0.3736 - val_loss: 1.6505 - val_accuracy: 0.3453\n",
            "Epoch 367/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6378 - accuracy: 0.3826 - val_loss: 1.6499 - val_accuracy: 0.3580\n",
            "Epoch 368/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6291 - accuracy: 0.3811 - val_loss: 1.6507 - val_accuracy: 0.3510\n",
            "Epoch 369/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6323 - accuracy: 0.3910 - val_loss: 1.6508 - val_accuracy: 0.3568\n",
            "Epoch 370/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6235 - accuracy: 0.3806 - val_loss: 1.6512 - val_accuracy: 0.3430\n",
            "Epoch 371/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6298 - accuracy: 0.3786 - val_loss: 1.6492 - val_accuracy: 0.3406\n",
            "Epoch 372/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.6485 - accuracy: 0.3771 - val_loss: 1.6478 - val_accuracy: 0.3453\n",
            "Epoch 373/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6324 - accuracy: 0.3806 - val_loss: 1.6483 - val_accuracy: 0.3441\n",
            "Epoch 374/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6383 - accuracy: 0.3627 - val_loss: 1.6470 - val_accuracy: 0.3557\n",
            "Epoch 375/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6419 - accuracy: 0.3791 - val_loss: 1.6482 - val_accuracy: 0.3568\n",
            "Epoch 376/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6408 - accuracy: 0.3865 - val_loss: 1.6456 - val_accuracy: 0.3568\n",
            "Epoch 377/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6346 - accuracy: 0.3880 - val_loss: 1.6449 - val_accuracy: 0.3591\n",
            "Epoch 378/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6281 - accuracy: 0.3811 - val_loss: 1.6448 - val_accuracy: 0.3637\n",
            "Epoch 379/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6329 - accuracy: 0.3652 - val_loss: 1.6447 - val_accuracy: 0.3580\n",
            "Epoch 380/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6324 - accuracy: 0.3875 - val_loss: 1.6445 - val_accuracy: 0.3522\n",
            "Epoch 381/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6373 - accuracy: 0.3816 - val_loss: 1.6446 - val_accuracy: 0.3476\n",
            "Epoch 382/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6227 - accuracy: 0.3835 - val_loss: 1.6445 - val_accuracy: 0.3545\n",
            "Epoch 383/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6262 - accuracy: 0.3935 - val_loss: 1.6436 - val_accuracy: 0.3476\n",
            "Epoch 384/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6344 - accuracy: 0.3865 - val_loss: 1.6418 - val_accuracy: 0.3510\n",
            "Epoch 385/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6313 - accuracy: 0.3840 - val_loss: 1.6424 - val_accuracy: 0.3476\n",
            "Epoch 386/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6185 - accuracy: 0.4019 - val_loss: 1.6418 - val_accuracy: 0.3476\n",
            "Epoch 387/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6250 - accuracy: 0.3949 - val_loss: 1.6384 - val_accuracy: 0.3545\n",
            "Epoch 388/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6246 - accuracy: 0.3826 - val_loss: 1.6402 - val_accuracy: 0.3707\n",
            "Epoch 389/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6133 - accuracy: 0.3855 - val_loss: 1.6391 - val_accuracy: 0.3545\n",
            "Epoch 390/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6223 - accuracy: 0.3915 - val_loss: 1.6388 - val_accuracy: 0.3499\n",
            "Epoch 391/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6266 - accuracy: 0.3865 - val_loss: 1.6384 - val_accuracy: 0.3626\n",
            "Epoch 392/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6278 - accuracy: 0.3850 - val_loss: 1.6445 - val_accuracy: 0.3614\n",
            "Epoch 393/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6150 - accuracy: 0.3895 - val_loss: 1.6393 - val_accuracy: 0.3487\n",
            "Epoch 394/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6346 - accuracy: 0.3751 - val_loss: 1.6420 - val_accuracy: 0.3591\n",
            "Epoch 395/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6254 - accuracy: 0.3949 - val_loss: 1.6354 - val_accuracy: 0.3614\n",
            "Epoch 396/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6186 - accuracy: 0.3845 - val_loss: 1.6351 - val_accuracy: 0.3591\n",
            "Epoch 397/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6107 - accuracy: 0.3880 - val_loss: 1.6357 - val_accuracy: 0.3580\n",
            "Epoch 398/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6146 - accuracy: 0.3845 - val_loss: 1.6333 - val_accuracy: 0.3580\n",
            "Epoch 399/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6183 - accuracy: 0.3845 - val_loss: 1.6342 - val_accuracy: 0.3672\n",
            "Epoch 400/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6142 - accuracy: 0.3880 - val_loss: 1.6347 - val_accuracy: 0.3522\n",
            "Epoch 401/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6186 - accuracy: 0.3935 - val_loss: 1.6364 - val_accuracy: 0.3614\n",
            "Epoch 402/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6204 - accuracy: 0.3845 - val_loss: 1.6321 - val_accuracy: 0.3637\n",
            "Epoch 403/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.6061 - accuracy: 0.3954 - val_loss: 1.6321 - val_accuracy: 0.3672\n",
            "Epoch 404/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6055 - accuracy: 0.3845 - val_loss: 1.6324 - val_accuracy: 0.3626\n",
            "Epoch 405/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.6066 - accuracy: 0.3885 - val_loss: 1.6293 - val_accuracy: 0.3603\n",
            "Epoch 406/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6029 - accuracy: 0.3954 - val_loss: 1.6354 - val_accuracy: 0.3626\n",
            "Epoch 407/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6017 - accuracy: 0.3865 - val_loss: 1.6298 - val_accuracy: 0.3510\n",
            "Epoch 408/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5898 - accuracy: 0.3984 - val_loss: 1.6282 - val_accuracy: 0.3603\n",
            "Epoch 409/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5943 - accuracy: 0.3940 - val_loss: 1.6275 - val_accuracy: 0.3661\n",
            "Epoch 410/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6060 - accuracy: 0.3826 - val_loss: 1.6255 - val_accuracy: 0.3649\n",
            "Epoch 411/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.6011 - accuracy: 0.3885 - val_loss: 1.6271 - val_accuracy: 0.3661\n",
            "Epoch 412/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6102 - accuracy: 0.3905 - val_loss: 1.6243 - val_accuracy: 0.3637\n",
            "Epoch 413/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.6096 - accuracy: 0.3935 - val_loss: 1.6264 - val_accuracy: 0.3591\n",
            "Epoch 414/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.6017 - accuracy: 0.3811 - val_loss: 1.6237 - val_accuracy: 0.3649\n",
            "Epoch 415/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.5979 - accuracy: 0.3816 - val_loss: 1.6245 - val_accuracy: 0.3661\n",
            "Epoch 416/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6097 - accuracy: 0.3880 - val_loss: 1.6246 - val_accuracy: 0.3603\n",
            "Epoch 417/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5940 - accuracy: 0.3969 - val_loss: 1.6230 - val_accuracy: 0.3649\n",
            "Epoch 418/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5876 - accuracy: 0.4014 - val_loss: 1.6228 - val_accuracy: 0.3684\n",
            "Epoch 419/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5960 - accuracy: 0.3989 - val_loss: 1.6227 - val_accuracy: 0.3672\n",
            "Epoch 420/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5913 - accuracy: 0.4029 - val_loss: 1.6200 - val_accuracy: 0.3661\n",
            "Epoch 421/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5945 - accuracy: 0.3900 - val_loss: 1.6195 - val_accuracy: 0.3741\n",
            "Epoch 422/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5951 - accuracy: 0.3940 - val_loss: 1.6203 - val_accuracy: 0.3672\n",
            "Epoch 423/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.6023 - accuracy: 0.3949 - val_loss: 1.6188 - val_accuracy: 0.3707\n",
            "Epoch 424/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5924 - accuracy: 0.4029 - val_loss: 1.6191 - val_accuracy: 0.3626\n",
            "Epoch 425/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.5853 - accuracy: 0.4034 - val_loss: 1.6182 - val_accuracy: 0.3661\n",
            "Epoch 426/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5934 - accuracy: 0.3930 - val_loss: 1.6167 - val_accuracy: 0.3661\n",
            "Epoch 427/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5887 - accuracy: 0.3979 - val_loss: 1.6172 - val_accuracy: 0.3730\n",
            "Epoch 428/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5771 - accuracy: 0.4088 - val_loss: 1.6175 - val_accuracy: 0.3741\n",
            "Epoch 429/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5899 - accuracy: 0.4063 - val_loss: 1.6177 - val_accuracy: 0.3753\n",
            "Epoch 430/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5893 - accuracy: 0.4049 - val_loss: 1.6151 - val_accuracy: 0.3695\n",
            "Epoch 431/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5889 - accuracy: 0.3999 - val_loss: 1.6150 - val_accuracy: 0.3695\n",
            "Epoch 432/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5880 - accuracy: 0.3910 - val_loss: 1.6148 - val_accuracy: 0.3672\n",
            "Epoch 433/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5875 - accuracy: 0.4009 - val_loss: 1.6137 - val_accuracy: 0.3695\n",
            "Epoch 434/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5874 - accuracy: 0.4063 - val_loss: 1.6116 - val_accuracy: 0.3695\n",
            "Epoch 435/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5780 - accuracy: 0.3994 - val_loss: 1.6136 - val_accuracy: 0.3684\n",
            "Epoch 436/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5888 - accuracy: 0.3989 - val_loss: 1.6103 - val_accuracy: 0.3753\n",
            "Epoch 437/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5843 - accuracy: 0.3994 - val_loss: 1.6117 - val_accuracy: 0.3730\n",
            "Epoch 438/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5976 - accuracy: 0.3964 - val_loss: 1.6120 - val_accuracy: 0.3753\n",
            "Epoch 439/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5802 - accuracy: 0.4044 - val_loss: 1.6108 - val_accuracy: 0.3718\n",
            "Epoch 440/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5709 - accuracy: 0.4103 - val_loss: 1.6120 - val_accuracy: 0.3718\n",
            "Epoch 441/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5761 - accuracy: 0.4138 - val_loss: 1.6099 - val_accuracy: 0.3764\n",
            "Epoch 442/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5712 - accuracy: 0.4039 - val_loss: 1.6107 - val_accuracy: 0.3730\n",
            "Epoch 443/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5811 - accuracy: 0.3974 - val_loss: 1.6063 - val_accuracy: 0.3695\n",
            "Epoch 444/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5732 - accuracy: 0.4118 - val_loss: 1.6058 - val_accuracy: 0.3661\n",
            "Epoch 445/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5663 - accuracy: 0.4187 - val_loss: 1.6075 - val_accuracy: 0.3753\n",
            "Epoch 446/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5790 - accuracy: 0.4049 - val_loss: 1.6064 - val_accuracy: 0.3788\n",
            "Epoch 447/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5794 - accuracy: 0.4128 - val_loss: 1.6047 - val_accuracy: 0.3753\n",
            "Epoch 448/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5739 - accuracy: 0.4068 - val_loss: 1.6048 - val_accuracy: 0.3753\n",
            "Epoch 449/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5754 - accuracy: 0.4083 - val_loss: 1.6061 - val_accuracy: 0.3695\n",
            "Epoch 450/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5827 - accuracy: 0.4029 - val_loss: 1.6025 - val_accuracy: 0.3788\n",
            "Epoch 451/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5696 - accuracy: 0.4088 - val_loss: 1.6023 - val_accuracy: 0.3764\n",
            "Epoch 452/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5701 - accuracy: 0.4078 - val_loss: 1.6022 - val_accuracy: 0.3834\n",
            "Epoch 453/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5667 - accuracy: 0.4128 - val_loss: 1.6013 - val_accuracy: 0.3776\n",
            "Epoch 454/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5690 - accuracy: 0.4187 - val_loss: 1.5998 - val_accuracy: 0.3730\n",
            "Epoch 455/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5722 - accuracy: 0.4108 - val_loss: 1.5991 - val_accuracy: 0.3764\n",
            "Epoch 456/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5656 - accuracy: 0.4039 - val_loss: 1.6029 - val_accuracy: 0.3776\n",
            "Epoch 457/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5668 - accuracy: 0.4212 - val_loss: 1.6069 - val_accuracy: 0.3788\n",
            "Epoch 458/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5694 - accuracy: 0.4093 - val_loss: 1.5989 - val_accuracy: 0.3822\n",
            "Epoch 459/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5635 - accuracy: 0.3935 - val_loss: 1.5985 - val_accuracy: 0.3857\n",
            "Epoch 460/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5758 - accuracy: 0.4014 - val_loss: 1.5972 - val_accuracy: 0.3845\n",
            "Epoch 461/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5605 - accuracy: 0.4073 - val_loss: 1.5987 - val_accuracy: 0.3730\n",
            "Epoch 462/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5685 - accuracy: 0.4029 - val_loss: 1.5945 - val_accuracy: 0.3788\n",
            "Epoch 463/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5699 - accuracy: 0.4039 - val_loss: 1.5954 - val_accuracy: 0.3822\n",
            "Epoch 464/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5736 - accuracy: 0.4024 - val_loss: 1.5954 - val_accuracy: 0.3834\n",
            "Epoch 465/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5751 - accuracy: 0.4024 - val_loss: 1.5930 - val_accuracy: 0.3811\n",
            "Epoch 466/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5648 - accuracy: 0.4108 - val_loss: 1.5984 - val_accuracy: 0.3811\n",
            "Epoch 467/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5634 - accuracy: 0.4073 - val_loss: 1.5936 - val_accuracy: 0.3788\n",
            "Epoch 468/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5524 - accuracy: 0.4207 - val_loss: 1.5958 - val_accuracy: 0.3811\n",
            "Epoch 469/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.5621 - accuracy: 0.4019 - val_loss: 1.5923 - val_accuracy: 0.3868\n",
            "Epoch 470/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5573 - accuracy: 0.4103 - val_loss: 1.5926 - val_accuracy: 0.3880\n",
            "Epoch 471/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5616 - accuracy: 0.3969 - val_loss: 1.5901 - val_accuracy: 0.3822\n",
            "Epoch 472/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5648 - accuracy: 0.4029 - val_loss: 1.5898 - val_accuracy: 0.3788\n",
            "Epoch 473/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5403 - accuracy: 0.4167 - val_loss: 1.5964 - val_accuracy: 0.3903\n",
            "Epoch 474/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5502 - accuracy: 0.4148 - val_loss: 1.5870 - val_accuracy: 0.3891\n",
            "Epoch 475/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5641 - accuracy: 0.3999 - val_loss: 1.5881 - val_accuracy: 0.3868\n",
            "Epoch 476/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.5463 - accuracy: 0.4093 - val_loss: 1.5864 - val_accuracy: 0.3822\n",
            "Epoch 477/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5642 - accuracy: 0.4083 - val_loss: 1.5870 - val_accuracy: 0.3822\n",
            "Epoch 478/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5485 - accuracy: 0.4133 - val_loss: 1.5856 - val_accuracy: 0.3845\n",
            "Epoch 479/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5448 - accuracy: 0.4187 - val_loss: 1.5864 - val_accuracy: 0.3891\n",
            "Epoch 480/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.5443 - accuracy: 0.4113 - val_loss: 1.5861 - val_accuracy: 0.3891\n",
            "Epoch 481/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5412 - accuracy: 0.4148 - val_loss: 1.5849 - val_accuracy: 0.3915\n",
            "Epoch 482/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5471 - accuracy: 0.4153 - val_loss: 1.5846 - val_accuracy: 0.3834\n",
            "Epoch 483/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5515 - accuracy: 0.4098 - val_loss: 1.5821 - val_accuracy: 0.3868\n",
            "Epoch 484/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5460 - accuracy: 0.4133 - val_loss: 1.5834 - val_accuracy: 0.3915\n",
            "Epoch 485/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5433 - accuracy: 0.4252 - val_loss: 1.5815 - val_accuracy: 0.3845\n",
            "Epoch 486/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5362 - accuracy: 0.4098 - val_loss: 1.5806 - val_accuracy: 0.3880\n",
            "Epoch 487/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5366 - accuracy: 0.4222 - val_loss: 1.5820 - val_accuracy: 0.3857\n",
            "Epoch 488/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5506 - accuracy: 0.4262 - val_loss: 1.5787 - val_accuracy: 0.3915\n",
            "Epoch 489/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5370 - accuracy: 0.4163 - val_loss: 1.5785 - val_accuracy: 0.3880\n",
            "Epoch 490/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5449 - accuracy: 0.4073 - val_loss: 1.5799 - val_accuracy: 0.3822\n",
            "Epoch 491/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5533 - accuracy: 0.4073 - val_loss: 1.5775 - val_accuracy: 0.3891\n",
            "Epoch 492/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5297 - accuracy: 0.4207 - val_loss: 1.5752 - val_accuracy: 0.3857\n",
            "Epoch 493/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5540 - accuracy: 0.4118 - val_loss: 1.5749 - val_accuracy: 0.3834\n",
            "Epoch 494/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5362 - accuracy: 0.4158 - val_loss: 1.5753 - val_accuracy: 0.3868\n",
            "Epoch 495/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5457 - accuracy: 0.4108 - val_loss: 1.5740 - val_accuracy: 0.3903\n",
            "Epoch 496/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5393 - accuracy: 0.4192 - val_loss: 1.5756 - val_accuracy: 0.3903\n",
            "Epoch 497/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5369 - accuracy: 0.4247 - val_loss: 1.5742 - val_accuracy: 0.3880\n",
            "Epoch 498/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5332 - accuracy: 0.4232 - val_loss: 1.5764 - val_accuracy: 0.3857\n",
            "Epoch 499/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5278 - accuracy: 0.4212 - val_loss: 1.5721 - val_accuracy: 0.3938\n",
            "Epoch 500/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5430 - accuracy: 0.4237 - val_loss: 1.5700 - val_accuracy: 0.3880\n",
            "Epoch 501/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5368 - accuracy: 0.4207 - val_loss: 1.5714 - val_accuracy: 0.3926\n",
            "Epoch 502/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5310 - accuracy: 0.4167 - val_loss: 1.5697 - val_accuracy: 0.3903\n",
            "Epoch 503/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5168 - accuracy: 0.4291 - val_loss: 1.5707 - val_accuracy: 0.3926\n",
            "Epoch 504/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5259 - accuracy: 0.4187 - val_loss: 1.5694 - val_accuracy: 0.3880\n",
            "Epoch 505/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5322 - accuracy: 0.4301 - val_loss: 1.5833 - val_accuracy: 0.3834\n",
            "Epoch 506/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5279 - accuracy: 0.4257 - val_loss: 1.5730 - val_accuracy: 0.3995\n",
            "Epoch 507/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5174 - accuracy: 0.4331 - val_loss: 1.5656 - val_accuracy: 0.3926\n",
            "Epoch 508/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5277 - accuracy: 0.4232 - val_loss: 1.5649 - val_accuracy: 0.3891\n",
            "Epoch 509/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5311 - accuracy: 0.4346 - val_loss: 1.5696 - val_accuracy: 0.3926\n",
            "Epoch 510/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5225 - accuracy: 0.4078 - val_loss: 1.5753 - val_accuracy: 0.3857\n",
            "Epoch 511/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5362 - accuracy: 0.4167 - val_loss: 1.5670 - val_accuracy: 0.3995\n",
            "Epoch 512/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5323 - accuracy: 0.4163 - val_loss: 1.5628 - val_accuracy: 0.3915\n",
            "Epoch 513/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5237 - accuracy: 0.4192 - val_loss: 1.5628 - val_accuracy: 0.3961\n",
            "Epoch 514/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5257 - accuracy: 0.4267 - val_loss: 1.5617 - val_accuracy: 0.3915\n",
            "Epoch 515/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5213 - accuracy: 0.4286 - val_loss: 1.5617 - val_accuracy: 0.3984\n",
            "Epoch 516/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5240 - accuracy: 0.4227 - val_loss: 1.5611 - val_accuracy: 0.3961\n",
            "Epoch 517/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5170 - accuracy: 0.4272 - val_loss: 1.5612 - val_accuracy: 0.3972\n",
            "Epoch 518/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5132 - accuracy: 0.4390 - val_loss: 1.5582 - val_accuracy: 0.3995\n",
            "Epoch 519/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5156 - accuracy: 0.4395 - val_loss: 1.5597 - val_accuracy: 0.3915\n",
            "Epoch 520/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5091 - accuracy: 0.4227 - val_loss: 1.5566 - val_accuracy: 0.4007\n",
            "Epoch 521/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5256 - accuracy: 0.4301 - val_loss: 1.5584 - val_accuracy: 0.3972\n",
            "Epoch 522/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5268 - accuracy: 0.4177 - val_loss: 1.5561 - val_accuracy: 0.3961\n",
            "Epoch 523/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.5180 - accuracy: 0.4281 - val_loss: 1.5569 - val_accuracy: 0.4007\n",
            "Epoch 524/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5046 - accuracy: 0.4217 - val_loss: 1.5558 - val_accuracy: 0.4042\n",
            "Epoch 525/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5189 - accuracy: 0.4306 - val_loss: 1.5557 - val_accuracy: 0.4030\n",
            "Epoch 526/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5139 - accuracy: 0.4281 - val_loss: 1.5561 - val_accuracy: 0.4076\n",
            "Epoch 527/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5098 - accuracy: 0.4301 - val_loss: 1.5537 - val_accuracy: 0.4042\n",
            "Epoch 528/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5232 - accuracy: 0.4257 - val_loss: 1.5525 - val_accuracy: 0.4030\n",
            "Epoch 529/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.5059 - accuracy: 0.4296 - val_loss: 1.5605 - val_accuracy: 0.3915\n",
            "Epoch 530/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5147 - accuracy: 0.4222 - val_loss: 1.5503 - val_accuracy: 0.4065\n",
            "Epoch 531/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4956 - accuracy: 0.4450 - val_loss: 1.5505 - val_accuracy: 0.3949\n",
            "Epoch 532/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5072 - accuracy: 0.4440 - val_loss: 1.5482 - val_accuracy: 0.4030\n",
            "Epoch 533/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5178 - accuracy: 0.4217 - val_loss: 1.5498 - val_accuracy: 0.3995\n",
            "Epoch 534/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5144 - accuracy: 0.4222 - val_loss: 1.5487 - val_accuracy: 0.4053\n",
            "Epoch 535/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5019 - accuracy: 0.4346 - val_loss: 1.5485 - val_accuracy: 0.4088\n",
            "Epoch 536/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5089 - accuracy: 0.4346 - val_loss: 1.5482 - val_accuracy: 0.4042\n",
            "Epoch 537/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5065 - accuracy: 0.4361 - val_loss: 1.5462 - val_accuracy: 0.4042\n",
            "Epoch 538/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5087 - accuracy: 0.4177 - val_loss: 1.5447 - val_accuracy: 0.4076\n",
            "Epoch 539/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.5162 - accuracy: 0.4291 - val_loss: 1.5455 - val_accuracy: 0.4076\n",
            "Epoch 540/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5137 - accuracy: 0.4227 - val_loss: 1.5443 - val_accuracy: 0.4042\n",
            "Epoch 541/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5115 - accuracy: 0.4311 - val_loss: 1.5442 - val_accuracy: 0.4076\n",
            "Epoch 542/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.5038 - accuracy: 0.4242 - val_loss: 1.5417 - val_accuracy: 0.4065\n",
            "Epoch 543/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4890 - accuracy: 0.4346 - val_loss: 1.5464 - val_accuracy: 0.4042\n",
            "Epoch 544/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4900 - accuracy: 0.4445 - val_loss: 1.5540 - val_accuracy: 0.4065\n",
            "Epoch 545/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4976 - accuracy: 0.4291 - val_loss: 1.5452 - val_accuracy: 0.4053\n",
            "Epoch 546/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5054 - accuracy: 0.4296 - val_loss: 1.5415 - val_accuracy: 0.4053\n",
            "Epoch 547/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4923 - accuracy: 0.4371 - val_loss: 1.5533 - val_accuracy: 0.3972\n",
            "Epoch 548/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5061 - accuracy: 0.4242 - val_loss: 1.5405 - val_accuracy: 0.4099\n",
            "Epoch 549/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5003 - accuracy: 0.4386 - val_loss: 1.5396 - val_accuracy: 0.4099\n",
            "Epoch 550/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4958 - accuracy: 0.4306 - val_loss: 1.5373 - val_accuracy: 0.4088\n",
            "Epoch 551/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5026 - accuracy: 0.4386 - val_loss: 1.5414 - val_accuracy: 0.4099\n",
            "Epoch 552/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4968 - accuracy: 0.4316 - val_loss: 1.5345 - val_accuracy: 0.4134\n",
            "Epoch 553/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4863 - accuracy: 0.4455 - val_loss: 1.5365 - val_accuracy: 0.4145\n",
            "Epoch 554/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4950 - accuracy: 0.4311 - val_loss: 1.5354 - val_accuracy: 0.4088\n",
            "Epoch 555/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4866 - accuracy: 0.4445 - val_loss: 1.5334 - val_accuracy: 0.4145\n",
            "Epoch 556/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5055 - accuracy: 0.4321 - val_loss: 1.5326 - val_accuracy: 0.4122\n",
            "Epoch 557/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4827 - accuracy: 0.4311 - val_loss: 1.5328 - val_accuracy: 0.4088\n",
            "Epoch 558/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4855 - accuracy: 0.4519 - val_loss: 1.5376 - val_accuracy: 0.4053\n",
            "Epoch 559/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4848 - accuracy: 0.4420 - val_loss: 1.5309 - val_accuracy: 0.4088\n",
            "Epoch 560/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4878 - accuracy: 0.4395 - val_loss: 1.5310 - val_accuracy: 0.4076\n",
            "Epoch 561/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4963 - accuracy: 0.4376 - val_loss: 1.5305 - val_accuracy: 0.4180\n",
            "Epoch 562/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4935 - accuracy: 0.4341 - val_loss: 1.5287 - val_accuracy: 0.4111\n",
            "Epoch 563/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.5005 - accuracy: 0.4346 - val_loss: 1.5278 - val_accuracy: 0.4122\n",
            "Epoch 564/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4731 - accuracy: 0.4326 - val_loss: 1.5367 - val_accuracy: 0.3995\n",
            "Epoch 565/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4872 - accuracy: 0.4465 - val_loss: 1.5287 - val_accuracy: 0.4169\n",
            "Epoch 566/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4924 - accuracy: 0.4425 - val_loss: 1.5278 - val_accuracy: 0.4145\n",
            "Epoch 567/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4837 - accuracy: 0.4316 - val_loss: 1.5289 - val_accuracy: 0.4145\n",
            "Epoch 568/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4823 - accuracy: 0.4321 - val_loss: 1.5252 - val_accuracy: 0.4099\n",
            "Epoch 569/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4812 - accuracy: 0.4390 - val_loss: 1.5327 - val_accuracy: 0.4099\n",
            "Epoch 570/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4842 - accuracy: 0.4440 - val_loss: 1.5385 - val_accuracy: 0.4111\n",
            "Epoch 571/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4766 - accuracy: 0.4321 - val_loss: 1.5236 - val_accuracy: 0.4099\n",
            "Epoch 572/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4916 - accuracy: 0.4351 - val_loss: 1.5221 - val_accuracy: 0.4088\n",
            "Epoch 573/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4768 - accuracy: 0.4509 - val_loss: 1.5241 - val_accuracy: 0.4042\n",
            "Epoch 574/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4835 - accuracy: 0.4381 - val_loss: 1.5187 - val_accuracy: 0.4111\n",
            "Epoch 575/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4790 - accuracy: 0.4455 - val_loss: 1.5215 - val_accuracy: 0.4122\n",
            "Epoch 576/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4702 - accuracy: 0.4400 - val_loss: 1.5235 - val_accuracy: 0.4180\n",
            "Epoch 577/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4796 - accuracy: 0.4430 - val_loss: 1.5192 - val_accuracy: 0.4122\n",
            "Epoch 578/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4822 - accuracy: 0.4341 - val_loss: 1.5148 - val_accuracy: 0.4122\n",
            "Epoch 579/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4823 - accuracy: 0.4400 - val_loss: 1.5164 - val_accuracy: 0.4134\n",
            "Epoch 580/1000\n",
            "127/127 [==============================] - 6s 46ms/step - loss: 1.4809 - accuracy: 0.4405 - val_loss: 1.5220 - val_accuracy: 0.4215\n",
            "Epoch 581/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4585 - accuracy: 0.4455 - val_loss: 1.5170 - val_accuracy: 0.4169\n",
            "Epoch 582/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4749 - accuracy: 0.4420 - val_loss: 1.5180 - val_accuracy: 0.4157\n",
            "Epoch 583/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4833 - accuracy: 0.4311 - val_loss: 1.5151 - val_accuracy: 0.4111\n",
            "Epoch 584/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4708 - accuracy: 0.4400 - val_loss: 1.5119 - val_accuracy: 0.4099\n",
            "Epoch 585/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4642 - accuracy: 0.4425 - val_loss: 1.5147 - val_accuracy: 0.4122\n",
            "Epoch 586/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4740 - accuracy: 0.4425 - val_loss: 1.5139 - val_accuracy: 0.4215\n",
            "Epoch 587/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4650 - accuracy: 0.4435 - val_loss: 1.5112 - val_accuracy: 0.4145\n",
            "Epoch 588/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4698 - accuracy: 0.4465 - val_loss: 1.5108 - val_accuracy: 0.4053\n",
            "Epoch 589/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4732 - accuracy: 0.4465 - val_loss: 1.5103 - val_accuracy: 0.4169\n",
            "Epoch 590/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4720 - accuracy: 0.4336 - val_loss: 1.5092 - val_accuracy: 0.4203\n",
            "Epoch 591/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4528 - accuracy: 0.4504 - val_loss: 1.5092 - val_accuracy: 0.4203\n",
            "Epoch 592/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4647 - accuracy: 0.4460 - val_loss: 1.5088 - val_accuracy: 0.4180\n",
            "Epoch 593/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4573 - accuracy: 0.4549 - val_loss: 1.5053 - val_accuracy: 0.4215\n",
            "Epoch 594/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4555 - accuracy: 0.4465 - val_loss: 1.5050 - val_accuracy: 0.4192\n",
            "Epoch 595/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4694 - accuracy: 0.4470 - val_loss: 1.5057 - val_accuracy: 0.4088\n",
            "Epoch 596/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4648 - accuracy: 0.4381 - val_loss: 1.5098 - val_accuracy: 0.4203\n",
            "Epoch 597/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4562 - accuracy: 0.4549 - val_loss: 1.5040 - val_accuracy: 0.4203\n",
            "Epoch 598/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.4521 - accuracy: 0.4618 - val_loss: 1.5028 - val_accuracy: 0.4192\n",
            "Epoch 599/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4600 - accuracy: 0.4450 - val_loss: 1.5035 - val_accuracy: 0.4192\n",
            "Epoch 600/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4578 - accuracy: 0.4524 - val_loss: 1.5051 - val_accuracy: 0.4215\n",
            "Epoch 601/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4576 - accuracy: 0.4504 - val_loss: 1.5000 - val_accuracy: 0.4215\n",
            "Epoch 602/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4521 - accuracy: 0.4579 - val_loss: 1.5014 - val_accuracy: 0.4192\n",
            "Epoch 603/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4540 - accuracy: 0.4524 - val_loss: 1.5016 - val_accuracy: 0.4203\n",
            "Epoch 604/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4604 - accuracy: 0.4529 - val_loss: 1.5006 - val_accuracy: 0.4249\n",
            "Epoch 605/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4799 - accuracy: 0.4381 - val_loss: 1.4977 - val_accuracy: 0.4249\n",
            "Epoch 606/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4592 - accuracy: 0.4465 - val_loss: 1.4969 - val_accuracy: 0.4226\n",
            "Epoch 607/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4545 - accuracy: 0.4514 - val_loss: 1.4982 - val_accuracy: 0.4238\n",
            "Epoch 608/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4554 - accuracy: 0.4504 - val_loss: 1.4958 - val_accuracy: 0.4238\n",
            "Epoch 609/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4535 - accuracy: 0.4509 - val_loss: 1.4949 - val_accuracy: 0.4238\n",
            "Epoch 610/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4527 - accuracy: 0.4559 - val_loss: 1.4954 - val_accuracy: 0.4203\n",
            "Epoch 611/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4466 - accuracy: 0.4574 - val_loss: 1.4946 - val_accuracy: 0.4203\n",
            "Epoch 612/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4357 - accuracy: 0.4524 - val_loss: 1.4903 - val_accuracy: 0.4169\n",
            "Epoch 613/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4491 - accuracy: 0.4554 - val_loss: 1.4945 - val_accuracy: 0.4273\n",
            "Epoch 614/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4477 - accuracy: 0.4514 - val_loss: 1.4891 - val_accuracy: 0.4215\n",
            "Epoch 615/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4521 - accuracy: 0.4579 - val_loss: 1.4989 - val_accuracy: 0.4261\n",
            "Epoch 616/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4424 - accuracy: 0.4584 - val_loss: 1.4953 - val_accuracy: 0.4342\n",
            "Epoch 617/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4329 - accuracy: 0.4618 - val_loss: 1.4877 - val_accuracy: 0.4215\n",
            "Epoch 618/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4389 - accuracy: 0.4589 - val_loss: 1.4964 - val_accuracy: 0.4307\n",
            "Epoch 619/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4481 - accuracy: 0.4554 - val_loss: 1.4898 - val_accuracy: 0.4307\n",
            "Epoch 620/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4444 - accuracy: 0.4579 - val_loss: 1.4895 - val_accuracy: 0.4249\n",
            "Epoch 621/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4444 - accuracy: 0.4599 - val_loss: 1.4869 - val_accuracy: 0.4249\n",
            "Epoch 622/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4337 - accuracy: 0.4618 - val_loss: 1.4856 - val_accuracy: 0.4203\n",
            "Epoch 623/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4415 - accuracy: 0.4727 - val_loss: 1.4938 - val_accuracy: 0.4284\n",
            "Epoch 624/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4312 - accuracy: 0.4599 - val_loss: 1.4866 - val_accuracy: 0.4307\n",
            "Epoch 625/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4468 - accuracy: 0.4490 - val_loss: 1.4909 - val_accuracy: 0.4319\n",
            "Epoch 626/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4490 - accuracy: 0.4594 - val_loss: 1.4840 - val_accuracy: 0.4330\n",
            "Epoch 627/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4338 - accuracy: 0.4594 - val_loss: 1.4906 - val_accuracy: 0.4423\n",
            "Epoch 628/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.4331 - accuracy: 0.4554 - val_loss: 1.4856 - val_accuracy: 0.4284\n",
            "Epoch 629/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4382 - accuracy: 0.4618 - val_loss: 1.4862 - val_accuracy: 0.4330\n",
            "Epoch 630/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4446 - accuracy: 0.4549 - val_loss: 1.4795 - val_accuracy: 0.4215\n",
            "Epoch 631/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4416 - accuracy: 0.4539 - val_loss: 1.4844 - val_accuracy: 0.4307\n",
            "Epoch 632/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4261 - accuracy: 0.4698 - val_loss: 1.4938 - val_accuracy: 0.4249\n",
            "Epoch 633/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4426 - accuracy: 0.4599 - val_loss: 1.4844 - val_accuracy: 0.4319\n",
            "Epoch 634/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4309 - accuracy: 0.4579 - val_loss: 1.4782 - val_accuracy: 0.4238\n",
            "Epoch 635/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4406 - accuracy: 0.4648 - val_loss: 1.4801 - val_accuracy: 0.4330\n",
            "Epoch 636/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4276 - accuracy: 0.4628 - val_loss: 1.4805 - val_accuracy: 0.4319\n",
            "Epoch 637/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.4245 - accuracy: 0.4718 - val_loss: 1.4798 - val_accuracy: 0.4342\n",
            "Epoch 638/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4335 - accuracy: 0.4658 - val_loss: 1.4776 - val_accuracy: 0.4342\n",
            "Epoch 639/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4271 - accuracy: 0.4633 - val_loss: 1.4842 - val_accuracy: 0.4342\n",
            "Epoch 640/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4411 - accuracy: 0.4549 - val_loss: 1.4791 - val_accuracy: 0.4330\n",
            "Epoch 641/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4198 - accuracy: 0.4623 - val_loss: 1.4761 - val_accuracy: 0.4353\n",
            "Epoch 642/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4136 - accuracy: 0.4772 - val_loss: 1.4808 - val_accuracy: 0.4296\n",
            "Epoch 643/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4266 - accuracy: 0.4767 - val_loss: 1.4716 - val_accuracy: 0.4353\n",
            "Epoch 644/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4340 - accuracy: 0.4623 - val_loss: 1.4717 - val_accuracy: 0.4307\n",
            "Epoch 645/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4451 - accuracy: 0.4376 - val_loss: 1.4741 - val_accuracy: 0.4319\n",
            "Epoch 646/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4222 - accuracy: 0.4628 - val_loss: 1.4767 - val_accuracy: 0.4376\n",
            "Epoch 647/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4217 - accuracy: 0.4772 - val_loss: 1.4721 - val_accuracy: 0.4388\n",
            "Epoch 648/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4299 - accuracy: 0.4609 - val_loss: 1.4704 - val_accuracy: 0.4400\n",
            "Epoch 649/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4266 - accuracy: 0.4628 - val_loss: 1.4712 - val_accuracy: 0.4411\n",
            "Epoch 650/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4248 - accuracy: 0.4653 - val_loss: 1.4745 - val_accuracy: 0.4388\n",
            "Epoch 651/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4283 - accuracy: 0.4579 - val_loss: 1.4682 - val_accuracy: 0.4457\n",
            "Epoch 652/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.4096 - accuracy: 0.4757 - val_loss: 1.4674 - val_accuracy: 0.4319\n",
            "Epoch 653/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4281 - accuracy: 0.4658 - val_loss: 1.4641 - val_accuracy: 0.4330\n",
            "Epoch 654/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4113 - accuracy: 0.4747 - val_loss: 1.4648 - val_accuracy: 0.4423\n",
            "Epoch 655/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4180 - accuracy: 0.4643 - val_loss: 1.4673 - val_accuracy: 0.4423\n",
            "Epoch 656/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4093 - accuracy: 0.4698 - val_loss: 1.4619 - val_accuracy: 0.4319\n",
            "Epoch 657/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4043 - accuracy: 0.4747 - val_loss: 1.4653 - val_accuracy: 0.4296\n",
            "Epoch 658/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.4063 - accuracy: 0.4693 - val_loss: 1.4697 - val_accuracy: 0.4434\n",
            "Epoch 659/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4071 - accuracy: 0.4658 - val_loss: 1.4592 - val_accuracy: 0.4434\n",
            "Epoch 660/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4102 - accuracy: 0.4777 - val_loss: 1.4641 - val_accuracy: 0.4446\n",
            "Epoch 661/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4199 - accuracy: 0.4623 - val_loss: 1.4582 - val_accuracy: 0.4400\n",
            "Epoch 662/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4152 - accuracy: 0.4633 - val_loss: 1.4621 - val_accuracy: 0.4388\n",
            "Epoch 663/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4152 - accuracy: 0.4752 - val_loss: 1.4583 - val_accuracy: 0.4411\n",
            "Epoch 664/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4113 - accuracy: 0.4812 - val_loss: 1.4604 - val_accuracy: 0.4411\n",
            "Epoch 665/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3984 - accuracy: 0.4713 - val_loss: 1.4572 - val_accuracy: 0.4423\n",
            "Epoch 666/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4194 - accuracy: 0.4727 - val_loss: 1.4531 - val_accuracy: 0.4457\n",
            "Epoch 667/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4090 - accuracy: 0.4653 - val_loss: 1.4539 - val_accuracy: 0.4469\n",
            "Epoch 668/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4004 - accuracy: 0.4802 - val_loss: 1.4532 - val_accuracy: 0.4469\n",
            "Epoch 669/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4057 - accuracy: 0.4693 - val_loss: 1.4539 - val_accuracy: 0.4480\n",
            "Epoch 670/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4077 - accuracy: 0.4732 - val_loss: 1.4554 - val_accuracy: 0.4446\n",
            "Epoch 671/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4088 - accuracy: 0.4693 - val_loss: 1.4502 - val_accuracy: 0.4423\n",
            "Epoch 672/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4136 - accuracy: 0.4762 - val_loss: 1.4482 - val_accuracy: 0.4503\n",
            "Epoch 673/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4011 - accuracy: 0.4832 - val_loss: 1.4525 - val_accuracy: 0.4515\n",
            "Epoch 674/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3987 - accuracy: 0.4683 - val_loss: 1.4531 - val_accuracy: 0.4457\n",
            "Epoch 675/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4046 - accuracy: 0.4718 - val_loss: 1.4512 - val_accuracy: 0.4503\n",
            "Epoch 676/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3869 - accuracy: 0.4822 - val_loss: 1.4586 - val_accuracy: 0.4503\n",
            "Epoch 677/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.4022 - accuracy: 0.4693 - val_loss: 1.4498 - val_accuracy: 0.4446\n",
            "Epoch 678/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3920 - accuracy: 0.4866 - val_loss: 1.4588 - val_accuracy: 0.4434\n",
            "Epoch 679/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3934 - accuracy: 0.4678 - val_loss: 1.4464 - val_accuracy: 0.4480\n",
            "Epoch 680/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3864 - accuracy: 0.4856 - val_loss: 1.4533 - val_accuracy: 0.4423\n",
            "Epoch 681/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3790 - accuracy: 0.4970 - val_loss: 1.4506 - val_accuracy: 0.4423\n",
            "Epoch 682/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.3886 - accuracy: 0.4926 - val_loss: 1.4427 - val_accuracy: 0.4515\n",
            "Epoch 683/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.4010 - accuracy: 0.4827 - val_loss: 1.4402 - val_accuracy: 0.4503\n",
            "Epoch 684/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3941 - accuracy: 0.4742 - val_loss: 1.4409 - val_accuracy: 0.4550\n",
            "Epoch 685/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.4027 - accuracy: 0.4732 - val_loss: 1.4464 - val_accuracy: 0.4503\n",
            "Epoch 686/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3940 - accuracy: 0.4782 - val_loss: 1.4429 - val_accuracy: 0.4480\n",
            "Epoch 687/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3991 - accuracy: 0.4812 - val_loss: 1.4438 - val_accuracy: 0.4515\n",
            "Epoch 688/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3815 - accuracy: 0.4782 - val_loss: 1.4403 - val_accuracy: 0.4492\n",
            "Epoch 689/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3864 - accuracy: 0.4817 - val_loss: 1.4433 - val_accuracy: 0.4515\n",
            "Epoch 690/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3800 - accuracy: 0.4856 - val_loss: 1.4386 - val_accuracy: 0.4550\n",
            "Epoch 691/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3850 - accuracy: 0.4822 - val_loss: 1.4384 - val_accuracy: 0.4538\n",
            "Epoch 692/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3767 - accuracy: 0.4851 - val_loss: 1.4378 - val_accuracy: 0.4492\n",
            "Epoch 693/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3818 - accuracy: 0.4866 - val_loss: 1.4372 - val_accuracy: 0.4630\n",
            "Epoch 694/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3695 - accuracy: 0.4827 - val_loss: 1.4403 - val_accuracy: 0.4550\n",
            "Epoch 695/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3794 - accuracy: 0.4822 - val_loss: 1.4374 - val_accuracy: 0.4550\n",
            "Epoch 696/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3811 - accuracy: 0.4861 - val_loss: 1.4342 - val_accuracy: 0.4503\n",
            "Epoch 697/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3800 - accuracy: 0.4817 - val_loss: 1.4337 - val_accuracy: 0.4596\n",
            "Epoch 698/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3903 - accuracy: 0.4737 - val_loss: 1.4346 - val_accuracy: 0.4607\n",
            "Epoch 699/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.3912 - accuracy: 0.4732 - val_loss: 1.4344 - val_accuracy: 0.4527\n",
            "Epoch 700/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.3843 - accuracy: 0.4936 - val_loss: 1.4351 - val_accuracy: 0.4596\n",
            "Epoch 701/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.3737 - accuracy: 0.4851 - val_loss: 1.4318 - val_accuracy: 0.4550\n",
            "Epoch 702/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3790 - accuracy: 0.4782 - val_loss: 1.4291 - val_accuracy: 0.4630\n",
            "Epoch 703/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3703 - accuracy: 0.4896 - val_loss: 1.4343 - val_accuracy: 0.4642\n",
            "Epoch 704/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3682 - accuracy: 0.4807 - val_loss: 1.4310 - val_accuracy: 0.4607\n",
            "Epoch 705/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3791 - accuracy: 0.4827 - val_loss: 1.4360 - val_accuracy: 0.4584\n",
            "Epoch 706/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.3692 - accuracy: 0.4901 - val_loss: 1.4308 - val_accuracy: 0.4492\n",
            "Epoch 707/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3793 - accuracy: 0.4906 - val_loss: 1.4281 - val_accuracy: 0.4561\n",
            "Epoch 708/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3832 - accuracy: 0.4812 - val_loss: 1.4285 - val_accuracy: 0.4688\n",
            "Epoch 709/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3734 - accuracy: 0.4856 - val_loss: 1.4306 - val_accuracy: 0.4515\n",
            "Epoch 710/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3721 - accuracy: 0.4807 - val_loss: 1.4281 - val_accuracy: 0.4619\n",
            "Epoch 711/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3710 - accuracy: 0.4836 - val_loss: 1.4272 - val_accuracy: 0.4677\n",
            "Epoch 712/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3552 - accuracy: 0.4916 - val_loss: 1.4291 - val_accuracy: 0.4688\n",
            "Epoch 713/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3628 - accuracy: 0.4990 - val_loss: 1.4274 - val_accuracy: 0.4573\n",
            "Epoch 714/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3718 - accuracy: 0.4817 - val_loss: 1.4278 - val_accuracy: 0.4665\n",
            "Epoch 715/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3639 - accuracy: 0.4871 - val_loss: 1.4242 - val_accuracy: 0.4619\n",
            "Epoch 716/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3600 - accuracy: 0.5000 - val_loss: 1.4234 - val_accuracy: 0.4665\n",
            "Epoch 717/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3766 - accuracy: 0.4901 - val_loss: 1.4232 - val_accuracy: 0.4723\n",
            "Epoch 718/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3602 - accuracy: 0.4931 - val_loss: 1.4229 - val_accuracy: 0.4630\n",
            "Epoch 719/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3548 - accuracy: 0.5040 - val_loss: 1.4337 - val_accuracy: 0.4642\n",
            "Epoch 720/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3630 - accuracy: 0.4817 - val_loss: 1.4223 - val_accuracy: 0.4654\n",
            "Epoch 721/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3699 - accuracy: 0.4871 - val_loss: 1.4190 - val_accuracy: 0.4677\n",
            "Epoch 722/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3564 - accuracy: 0.4911 - val_loss: 1.4250 - val_accuracy: 0.4711\n",
            "Epoch 723/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3739 - accuracy: 0.4945 - val_loss: 1.4204 - val_accuracy: 0.4584\n",
            "Epoch 724/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3727 - accuracy: 0.4891 - val_loss: 1.4186 - val_accuracy: 0.4734\n",
            "Epoch 725/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3599 - accuracy: 0.4980 - val_loss: 1.4394 - val_accuracy: 0.4607\n",
            "Epoch 726/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3749 - accuracy: 0.4757 - val_loss: 1.4169 - val_accuracy: 0.4584\n",
            "Epoch 727/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3728 - accuracy: 0.4901 - val_loss: 1.4156 - val_accuracy: 0.4607\n",
            "Epoch 728/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3461 - accuracy: 0.4995 - val_loss: 1.4169 - val_accuracy: 0.4573\n",
            "Epoch 729/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3638 - accuracy: 0.4891 - val_loss: 1.4211 - val_accuracy: 0.4607\n",
            "Epoch 730/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3617 - accuracy: 0.4901 - val_loss: 1.4260 - val_accuracy: 0.4677\n",
            "Epoch 731/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3701 - accuracy: 0.4881 - val_loss: 1.4117 - val_accuracy: 0.4584\n",
            "Epoch 732/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3484 - accuracy: 0.5084 - val_loss: 1.4138 - val_accuracy: 0.4654\n",
            "Epoch 733/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3548 - accuracy: 0.5000 - val_loss: 1.4138 - val_accuracy: 0.4619\n",
            "Epoch 734/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.3575 - accuracy: 0.4896 - val_loss: 1.4129 - val_accuracy: 0.4758\n",
            "Epoch 735/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.3711 - accuracy: 0.4856 - val_loss: 1.4096 - val_accuracy: 0.4688\n",
            "Epoch 736/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3712 - accuracy: 0.4812 - val_loss: 1.4139 - val_accuracy: 0.4607\n",
            "Epoch 737/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3533 - accuracy: 0.4990 - val_loss: 1.4195 - val_accuracy: 0.4654\n",
            "Epoch 738/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3404 - accuracy: 0.5084 - val_loss: 1.4083 - val_accuracy: 0.4642\n",
            "Epoch 739/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3484 - accuracy: 0.5050 - val_loss: 1.4149 - val_accuracy: 0.4677\n",
            "Epoch 740/1000\n",
            "127/127 [==============================] - 6s 47ms/step - loss: 1.3622 - accuracy: 0.4871 - val_loss: 1.4124 - val_accuracy: 0.4654\n",
            "Epoch 741/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3369 - accuracy: 0.4970 - val_loss: 1.4095 - val_accuracy: 0.4619\n",
            "Epoch 742/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3468 - accuracy: 0.5045 - val_loss: 1.4147 - val_accuracy: 0.4677\n",
            "Epoch 743/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3435 - accuracy: 0.5035 - val_loss: 1.4063 - val_accuracy: 0.4700\n",
            "Epoch 744/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3450 - accuracy: 0.4965 - val_loss: 1.4058 - val_accuracy: 0.4688\n",
            "Epoch 745/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3269 - accuracy: 0.5154 - val_loss: 1.4169 - val_accuracy: 0.4665\n",
            "Epoch 746/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3460 - accuracy: 0.5020 - val_loss: 1.4121 - val_accuracy: 0.4677\n",
            "Epoch 747/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.3480 - accuracy: 0.5000 - val_loss: 1.4058 - val_accuracy: 0.4700\n",
            "Epoch 748/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3547 - accuracy: 0.4906 - val_loss: 1.4095 - val_accuracy: 0.4642\n",
            "Epoch 749/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3460 - accuracy: 0.4965 - val_loss: 1.4026 - val_accuracy: 0.4746\n",
            "Epoch 750/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3522 - accuracy: 0.4950 - val_loss: 1.4039 - val_accuracy: 0.4711\n",
            "Epoch 751/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3411 - accuracy: 0.4931 - val_loss: 1.4008 - val_accuracy: 0.4723\n",
            "Epoch 752/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3480 - accuracy: 0.5025 - val_loss: 1.4139 - val_accuracy: 0.4654\n",
            "Epoch 753/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3249 - accuracy: 0.5069 - val_loss: 1.4022 - val_accuracy: 0.4723\n",
            "Epoch 754/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3367 - accuracy: 0.5149 - val_loss: 1.3992 - val_accuracy: 0.4734\n",
            "Epoch 755/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3429 - accuracy: 0.4866 - val_loss: 1.4098 - val_accuracy: 0.4700\n",
            "Epoch 756/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3472 - accuracy: 0.4941 - val_loss: 1.4015 - val_accuracy: 0.4734\n",
            "Epoch 757/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3299 - accuracy: 0.5010 - val_loss: 1.4081 - val_accuracy: 0.4642\n",
            "Epoch 758/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3455 - accuracy: 0.5114 - val_loss: 1.3987 - val_accuracy: 0.4723\n",
            "Epoch 759/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3436 - accuracy: 0.5159 - val_loss: 1.4013 - val_accuracy: 0.4723\n",
            "Epoch 760/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3425 - accuracy: 0.5010 - val_loss: 1.4101 - val_accuracy: 0.4677\n",
            "Epoch 761/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3282 - accuracy: 0.5114 - val_loss: 1.4020 - val_accuracy: 0.4734\n",
            "Epoch 762/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.3320 - accuracy: 0.5114 - val_loss: 1.4026 - val_accuracy: 0.4746\n",
            "Epoch 763/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3271 - accuracy: 0.5074 - val_loss: 1.3998 - val_accuracy: 0.4815\n",
            "Epoch 764/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3477 - accuracy: 0.5005 - val_loss: 1.3974 - val_accuracy: 0.4769\n",
            "Epoch 765/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3191 - accuracy: 0.5040 - val_loss: 1.3936 - val_accuracy: 0.4792\n",
            "Epoch 766/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3216 - accuracy: 0.5134 - val_loss: 1.3987 - val_accuracy: 0.4758\n",
            "Epoch 767/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3360 - accuracy: 0.5084 - val_loss: 1.4023 - val_accuracy: 0.4723\n",
            "Epoch 768/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3200 - accuracy: 0.5114 - val_loss: 1.3998 - val_accuracy: 0.4746\n",
            "Epoch 769/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3386 - accuracy: 0.4990 - val_loss: 1.3944 - val_accuracy: 0.4746\n",
            "Epoch 770/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3222 - accuracy: 0.5000 - val_loss: 1.3997 - val_accuracy: 0.4758\n",
            "Epoch 771/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3246 - accuracy: 0.5099 - val_loss: 1.3937 - val_accuracy: 0.4827\n",
            "Epoch 772/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3289 - accuracy: 0.5064 - val_loss: 1.3946 - val_accuracy: 0.4700\n",
            "Epoch 773/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3206 - accuracy: 0.5094 - val_loss: 1.3916 - val_accuracy: 0.4769\n",
            "Epoch 774/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3296 - accuracy: 0.4990 - val_loss: 1.3869 - val_accuracy: 0.4804\n",
            "Epoch 775/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3241 - accuracy: 0.5055 - val_loss: 1.3916 - val_accuracy: 0.4815\n",
            "Epoch 776/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3191 - accuracy: 0.5045 - val_loss: 1.3900 - val_accuracy: 0.4734\n",
            "Epoch 777/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3125 - accuracy: 0.5278 - val_loss: 1.3896 - val_accuracy: 0.4746\n",
            "Epoch 778/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3245 - accuracy: 0.5035 - val_loss: 1.3925 - val_accuracy: 0.4746\n",
            "Epoch 779/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3144 - accuracy: 0.5124 - val_loss: 1.3930 - val_accuracy: 0.4792\n",
            "Epoch 780/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3232 - accuracy: 0.5253 - val_loss: 1.3953 - val_accuracy: 0.4688\n",
            "Epoch 781/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3105 - accuracy: 0.5124 - val_loss: 1.3932 - val_accuracy: 0.4804\n",
            "Epoch 782/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3154 - accuracy: 0.5104 - val_loss: 1.3902 - val_accuracy: 0.4769\n",
            "Epoch 783/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3118 - accuracy: 0.5084 - val_loss: 1.3886 - val_accuracy: 0.4723\n",
            "Epoch 784/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3250 - accuracy: 0.5064 - val_loss: 1.3886 - val_accuracy: 0.4827\n",
            "Epoch 785/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3211 - accuracy: 0.5213 - val_loss: 1.4030 - val_accuracy: 0.4677\n",
            "Epoch 786/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3203 - accuracy: 0.5000 - val_loss: 1.3854 - val_accuracy: 0.4711\n",
            "Epoch 787/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3165 - accuracy: 0.5089 - val_loss: 1.3885 - val_accuracy: 0.4850\n",
            "Epoch 788/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3176 - accuracy: 0.5164 - val_loss: 1.3832 - val_accuracy: 0.4885\n",
            "Epoch 789/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3169 - accuracy: 0.5129 - val_loss: 1.3821 - val_accuracy: 0.4815\n",
            "Epoch 790/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3147 - accuracy: 0.5159 - val_loss: 1.3831 - val_accuracy: 0.4804\n",
            "Epoch 791/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.3134 - accuracy: 0.5104 - val_loss: 1.3839 - val_accuracy: 0.4792\n",
            "Epoch 792/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3176 - accuracy: 0.5149 - val_loss: 1.3939 - val_accuracy: 0.4758\n",
            "Epoch 793/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.3222 - accuracy: 0.5025 - val_loss: 1.3847 - val_accuracy: 0.4885\n",
            "Epoch 794/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3138 - accuracy: 0.5178 - val_loss: 1.3850 - val_accuracy: 0.4908\n",
            "Epoch 795/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2996 - accuracy: 0.5228 - val_loss: 1.3841 - val_accuracy: 0.4723\n",
            "Epoch 796/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3291 - accuracy: 0.5059 - val_loss: 1.3846 - val_accuracy: 0.4769\n",
            "Epoch 797/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3027 - accuracy: 0.5273 - val_loss: 1.3867 - val_accuracy: 0.4850\n",
            "Epoch 798/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3012 - accuracy: 0.5154 - val_loss: 1.3800 - val_accuracy: 0.4758\n",
            "Epoch 799/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3036 - accuracy: 0.5228 - val_loss: 1.3771 - val_accuracy: 0.4896\n",
            "Epoch 800/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3141 - accuracy: 0.5089 - val_loss: 1.3805 - val_accuracy: 0.4781\n",
            "Epoch 801/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3063 - accuracy: 0.5178 - val_loss: 1.3771 - val_accuracy: 0.4896\n",
            "Epoch 802/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3043 - accuracy: 0.5377 - val_loss: 1.3817 - val_accuracy: 0.4792\n",
            "Epoch 803/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3045 - accuracy: 0.5109 - val_loss: 1.3795 - val_accuracy: 0.4873\n",
            "Epoch 804/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3043 - accuracy: 0.5178 - val_loss: 1.3765 - val_accuracy: 0.4781\n",
            "Epoch 805/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3005 - accuracy: 0.5253 - val_loss: 1.3760 - val_accuracy: 0.4827\n",
            "Epoch 806/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3086 - accuracy: 0.5183 - val_loss: 1.3748 - val_accuracy: 0.4815\n",
            "Epoch 807/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3016 - accuracy: 0.5164 - val_loss: 1.3876 - val_accuracy: 0.4711\n",
            "Epoch 808/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3070 - accuracy: 0.5144 - val_loss: 1.3747 - val_accuracy: 0.4815\n",
            "Epoch 809/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2920 - accuracy: 0.5292 - val_loss: 1.3755 - val_accuracy: 0.4873\n",
            "Epoch 810/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.3062 - accuracy: 0.5183 - val_loss: 1.3728 - val_accuracy: 0.4965\n",
            "Epoch 811/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2969 - accuracy: 0.5173 - val_loss: 1.3786 - val_accuracy: 0.4827\n",
            "Epoch 812/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2916 - accuracy: 0.5188 - val_loss: 1.3756 - val_accuracy: 0.4838\n",
            "Epoch 813/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2981 - accuracy: 0.5178 - val_loss: 1.3793 - val_accuracy: 0.4873\n",
            "Epoch 814/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3011 - accuracy: 0.5099 - val_loss: 1.3739 - val_accuracy: 0.4919\n",
            "Epoch 815/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.3003 - accuracy: 0.5193 - val_loss: 1.3748 - val_accuracy: 0.4781\n",
            "Epoch 816/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.3020 - accuracy: 0.5154 - val_loss: 1.3746 - val_accuracy: 0.4815\n",
            "Epoch 817/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2858 - accuracy: 0.5183 - val_loss: 1.3746 - val_accuracy: 0.4873\n",
            "Epoch 818/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.3020 - accuracy: 0.5104 - val_loss: 1.3772 - val_accuracy: 0.4885\n",
            "Epoch 819/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.3020 - accuracy: 0.5218 - val_loss: 1.3676 - val_accuracy: 0.4873\n",
            "Epoch 820/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2844 - accuracy: 0.5287 - val_loss: 1.3695 - val_accuracy: 0.4827\n",
            "Epoch 821/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2860 - accuracy: 0.5218 - val_loss: 1.3721 - val_accuracy: 0.4908\n",
            "Epoch 822/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2823 - accuracy: 0.5307 - val_loss: 1.3705 - val_accuracy: 0.4861\n",
            "Epoch 823/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2941 - accuracy: 0.5238 - val_loss: 1.3655 - val_accuracy: 0.4965\n",
            "Epoch 824/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2838 - accuracy: 0.5223 - val_loss: 1.3710 - val_accuracy: 0.4896\n",
            "Epoch 825/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2994 - accuracy: 0.5213 - val_loss: 1.3685 - val_accuracy: 0.4873\n",
            "Epoch 826/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2812 - accuracy: 0.5223 - val_loss: 1.3646 - val_accuracy: 0.4885\n",
            "Epoch 827/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2981 - accuracy: 0.5173 - val_loss: 1.3701 - val_accuracy: 0.4861\n",
            "Epoch 828/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2783 - accuracy: 0.5238 - val_loss: 1.3662 - val_accuracy: 0.4850\n",
            "Epoch 829/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2953 - accuracy: 0.5168 - val_loss: 1.3680 - val_accuracy: 0.4919\n",
            "Epoch 830/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2864 - accuracy: 0.5337 - val_loss: 1.3625 - val_accuracy: 0.4931\n",
            "Epoch 831/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2710 - accuracy: 0.5382 - val_loss: 1.3719 - val_accuracy: 0.4781\n",
            "Epoch 832/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2898 - accuracy: 0.5248 - val_loss: 1.3644 - val_accuracy: 0.4861\n",
            "Epoch 833/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2875 - accuracy: 0.5268 - val_loss: 1.3658 - val_accuracy: 0.4885\n",
            "Epoch 834/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2716 - accuracy: 0.5273 - val_loss: 1.3605 - val_accuracy: 0.4954\n",
            "Epoch 835/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2767 - accuracy: 0.5223 - val_loss: 1.3683 - val_accuracy: 0.4931\n",
            "Epoch 836/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2702 - accuracy: 0.5307 - val_loss: 1.3612 - val_accuracy: 0.4885\n",
            "Epoch 837/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2879 - accuracy: 0.5188 - val_loss: 1.3689 - val_accuracy: 0.4815\n",
            "Epoch 838/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2679 - accuracy: 0.5312 - val_loss: 1.3598 - val_accuracy: 0.4885\n",
            "Epoch 839/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2781 - accuracy: 0.5248 - val_loss: 1.3662 - val_accuracy: 0.4896\n",
            "Epoch 840/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2779 - accuracy: 0.5168 - val_loss: 1.3654 - val_accuracy: 0.4896\n",
            "Epoch 841/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2816 - accuracy: 0.5243 - val_loss: 1.3569 - val_accuracy: 0.4861\n",
            "Epoch 842/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2847 - accuracy: 0.5243 - val_loss: 1.3658 - val_accuracy: 0.4850\n",
            "Epoch 843/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2692 - accuracy: 0.5302 - val_loss: 1.3648 - val_accuracy: 0.4861\n",
            "Epoch 844/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2696 - accuracy: 0.5391 - val_loss: 1.3554 - val_accuracy: 0.4977\n",
            "Epoch 845/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2824 - accuracy: 0.5282 - val_loss: 1.3620 - val_accuracy: 0.4919\n",
            "Epoch 846/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2693 - accuracy: 0.5317 - val_loss: 1.3654 - val_accuracy: 0.4873\n",
            "Epoch 847/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2671 - accuracy: 0.5238 - val_loss: 1.3566 - val_accuracy: 0.4885\n",
            "Epoch 848/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2669 - accuracy: 0.5223 - val_loss: 1.3553 - val_accuracy: 0.4827\n",
            "Epoch 849/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2642 - accuracy: 0.5367 - val_loss: 1.3575 - val_accuracy: 0.4873\n",
            "Epoch 850/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2781 - accuracy: 0.5337 - val_loss: 1.3567 - val_accuracy: 0.4965\n",
            "Epoch 851/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2621 - accuracy: 0.5203 - val_loss: 1.3579 - val_accuracy: 0.4942\n",
            "Epoch 852/1000\n",
            "127/127 [==============================] - 6s 48ms/step - loss: 1.2667 - accuracy: 0.5253 - val_loss: 1.3559 - val_accuracy: 0.4896\n",
            "Epoch 853/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2664 - accuracy: 0.5258 - val_loss: 1.3578 - val_accuracy: 0.4908\n",
            "Epoch 854/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2686 - accuracy: 0.5278 - val_loss: 1.3531 - val_accuracy: 0.4919\n",
            "Epoch 855/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2625 - accuracy: 0.5347 - val_loss: 1.3549 - val_accuracy: 0.4931\n",
            "Epoch 856/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2666 - accuracy: 0.5218 - val_loss: 1.3596 - val_accuracy: 0.4919\n",
            "Epoch 857/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2674 - accuracy: 0.5302 - val_loss: 1.3553 - val_accuracy: 0.4965\n",
            "Epoch 858/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2672 - accuracy: 0.5367 - val_loss: 1.3585 - val_accuracy: 0.4873\n",
            "Epoch 859/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2623 - accuracy: 0.5273 - val_loss: 1.3509 - val_accuracy: 0.4896\n",
            "Epoch 860/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2680 - accuracy: 0.5327 - val_loss: 1.3521 - val_accuracy: 0.4954\n",
            "Epoch 861/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2671 - accuracy: 0.5233 - val_loss: 1.3501 - val_accuracy: 0.5046\n",
            "Epoch 862/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2701 - accuracy: 0.5223 - val_loss: 1.3527 - val_accuracy: 0.5012\n",
            "Epoch 863/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2577 - accuracy: 0.5441 - val_loss: 1.3540 - val_accuracy: 0.4931\n",
            "Epoch 864/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2641 - accuracy: 0.5238 - val_loss: 1.3504 - val_accuracy: 0.4942\n",
            "Epoch 865/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2634 - accuracy: 0.5396 - val_loss: 1.3555 - val_accuracy: 0.4919\n",
            "Epoch 866/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2653 - accuracy: 0.5188 - val_loss: 1.3504 - val_accuracy: 0.4988\n",
            "Epoch 867/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.2611 - accuracy: 0.5342 - val_loss: 1.3641 - val_accuracy: 0.4908\n",
            "Epoch 868/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2474 - accuracy: 0.5347 - val_loss: 1.3515 - val_accuracy: 0.4942\n",
            "Epoch 869/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2556 - accuracy: 0.5401 - val_loss: 1.3499 - val_accuracy: 0.4954\n",
            "Epoch 870/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2522 - accuracy: 0.5396 - val_loss: 1.3466 - val_accuracy: 0.5081\n",
            "Epoch 871/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2543 - accuracy: 0.5278 - val_loss: 1.3472 - val_accuracy: 0.5035\n",
            "Epoch 872/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2534 - accuracy: 0.5411 - val_loss: 1.3486 - val_accuracy: 0.4977\n",
            "Epoch 873/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2602 - accuracy: 0.5263 - val_loss: 1.3549 - val_accuracy: 0.4954\n",
            "Epoch 874/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2240 - accuracy: 0.5416 - val_loss: 1.3446 - val_accuracy: 0.5012\n",
            "Epoch 875/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2455 - accuracy: 0.5446 - val_loss: 1.3500 - val_accuracy: 0.4942\n",
            "Epoch 876/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2558 - accuracy: 0.5396 - val_loss: 1.3419 - val_accuracy: 0.5035\n",
            "Epoch 877/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2522 - accuracy: 0.5347 - val_loss: 1.3485 - val_accuracy: 0.4931\n",
            "Epoch 878/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2460 - accuracy: 0.5396 - val_loss: 1.3445 - val_accuracy: 0.4919\n",
            "Epoch 879/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2428 - accuracy: 0.5278 - val_loss: 1.3386 - val_accuracy: 0.5023\n",
            "Epoch 880/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2541 - accuracy: 0.5278 - val_loss: 1.3403 - val_accuracy: 0.5035\n",
            "Epoch 881/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2440 - accuracy: 0.5416 - val_loss: 1.3500 - val_accuracy: 0.4919\n",
            "Epoch 882/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2407 - accuracy: 0.5461 - val_loss: 1.3446 - val_accuracy: 0.5058\n",
            "Epoch 883/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2505 - accuracy: 0.5248 - val_loss: 1.3401 - val_accuracy: 0.5023\n",
            "Epoch 884/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2348 - accuracy: 0.5471 - val_loss: 1.3428 - val_accuracy: 0.5035\n",
            "Epoch 885/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2626 - accuracy: 0.5337 - val_loss: 1.3390 - val_accuracy: 0.5035\n",
            "Epoch 886/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2405 - accuracy: 0.5416 - val_loss: 1.3611 - val_accuracy: 0.4861\n",
            "Epoch 887/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2369 - accuracy: 0.5471 - val_loss: 1.3483 - val_accuracy: 0.4977\n",
            "Epoch 888/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2445 - accuracy: 0.5416 - val_loss: 1.3539 - val_accuracy: 0.4919\n",
            "Epoch 889/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.2354 - accuracy: 0.5525 - val_loss: 1.3408 - val_accuracy: 0.5092\n",
            "Epoch 890/1000\n",
            "127/127 [==============================] - 6s 49ms/step - loss: 1.2341 - accuracy: 0.5416 - val_loss: 1.3383 - val_accuracy: 0.5046\n",
            "Epoch 891/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2318 - accuracy: 0.5307 - val_loss: 1.3422 - val_accuracy: 0.4954\n",
            "Epoch 892/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2334 - accuracy: 0.5431 - val_loss: 1.3352 - val_accuracy: 0.5058\n",
            "Epoch 893/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2314 - accuracy: 0.5530 - val_loss: 1.3397 - val_accuracy: 0.5092\n",
            "Epoch 894/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.2435 - accuracy: 0.5461 - val_loss: 1.3405 - val_accuracy: 0.5081\n",
            "Epoch 895/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2601 - accuracy: 0.5302 - val_loss: 1.3372 - val_accuracy: 0.5012\n",
            "Epoch 896/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2333 - accuracy: 0.5476 - val_loss: 1.3371 - val_accuracy: 0.5092\n",
            "Epoch 897/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2433 - accuracy: 0.5307 - val_loss: 1.3326 - val_accuracy: 0.5081\n",
            "Epoch 898/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2314 - accuracy: 0.5426 - val_loss: 1.3442 - val_accuracy: 0.4988\n",
            "Epoch 899/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2298 - accuracy: 0.5456 - val_loss: 1.3505 - val_accuracy: 0.5000\n",
            "Epoch 900/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2409 - accuracy: 0.5456 - val_loss: 1.3460 - val_accuracy: 0.4965\n",
            "Epoch 901/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2358 - accuracy: 0.5382 - val_loss: 1.3394 - val_accuracy: 0.5012\n",
            "Epoch 902/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2238 - accuracy: 0.5466 - val_loss: 1.3346 - val_accuracy: 0.5081\n",
            "Epoch 903/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2192 - accuracy: 0.5481 - val_loss: 1.3317 - val_accuracy: 0.5069\n",
            "Epoch 904/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2293 - accuracy: 0.5560 - val_loss: 1.3466 - val_accuracy: 0.4977\n",
            "Epoch 905/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2112 - accuracy: 0.5515 - val_loss: 1.3299 - val_accuracy: 0.5069\n",
            "Epoch 906/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2265 - accuracy: 0.5555 - val_loss: 1.3410 - val_accuracy: 0.4931\n",
            "Epoch 907/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2275 - accuracy: 0.5520 - val_loss: 1.3320 - val_accuracy: 0.5081\n",
            "Epoch 908/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2282 - accuracy: 0.5431 - val_loss: 1.3320 - val_accuracy: 0.5139\n",
            "Epoch 909/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2218 - accuracy: 0.5654 - val_loss: 1.3272 - val_accuracy: 0.5058\n",
            "Epoch 910/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2201 - accuracy: 0.5461 - val_loss: 1.3355 - val_accuracy: 0.5127\n",
            "Epoch 911/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2187 - accuracy: 0.5510 - val_loss: 1.3321 - val_accuracy: 0.5035\n",
            "Epoch 912/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2108 - accuracy: 0.5669 - val_loss: 1.3290 - val_accuracy: 0.5139\n",
            "Epoch 913/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2145 - accuracy: 0.5545 - val_loss: 1.3284 - val_accuracy: 0.5081\n",
            "Epoch 914/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2180 - accuracy: 0.5496 - val_loss: 1.3355 - val_accuracy: 0.5012\n",
            "Epoch 915/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2173 - accuracy: 0.5570 - val_loss: 1.3330 - val_accuracy: 0.5035\n",
            "Epoch 916/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2169 - accuracy: 0.5540 - val_loss: 1.3390 - val_accuracy: 0.5058\n",
            "Epoch 917/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2074 - accuracy: 0.5614 - val_loss: 1.3397 - val_accuracy: 0.5023\n",
            "Epoch 918/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2175 - accuracy: 0.5540 - val_loss: 1.3377 - val_accuracy: 0.5035\n",
            "Epoch 919/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2134 - accuracy: 0.5570 - val_loss: 1.3359 - val_accuracy: 0.5069\n",
            "Epoch 920/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2223 - accuracy: 0.5446 - val_loss: 1.3311 - val_accuracy: 0.5092\n",
            "Epoch 921/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2236 - accuracy: 0.5600 - val_loss: 1.3304 - val_accuracy: 0.4977\n",
            "Epoch 922/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2121 - accuracy: 0.5466 - val_loss: 1.3364 - val_accuracy: 0.5023\n",
            "Epoch 923/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2227 - accuracy: 0.5634 - val_loss: 1.3247 - val_accuracy: 0.5046\n",
            "Epoch 924/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2121 - accuracy: 0.5560 - val_loss: 1.3296 - val_accuracy: 0.5012\n",
            "Epoch 925/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2232 - accuracy: 0.5515 - val_loss: 1.3264 - val_accuracy: 0.4988\n",
            "Epoch 926/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2116 - accuracy: 0.5535 - val_loss: 1.3281 - val_accuracy: 0.5012\n",
            "Epoch 927/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2185 - accuracy: 0.5515 - val_loss: 1.3191 - val_accuracy: 0.5069\n",
            "Epoch 928/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.1958 - accuracy: 0.5550 - val_loss: 1.3185 - val_accuracy: 0.5104\n",
            "Epoch 929/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2154 - accuracy: 0.5525 - val_loss: 1.3259 - val_accuracy: 0.5069\n",
            "Epoch 930/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2088 - accuracy: 0.5491 - val_loss: 1.3207 - val_accuracy: 0.5035\n",
            "Epoch 931/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2175 - accuracy: 0.5456 - val_loss: 1.3147 - val_accuracy: 0.5127\n",
            "Epoch 932/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.1996 - accuracy: 0.5644 - val_loss: 1.3206 - val_accuracy: 0.5150\n",
            "Epoch 933/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2123 - accuracy: 0.5436 - val_loss: 1.3289 - val_accuracy: 0.5150\n",
            "Epoch 934/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.2049 - accuracy: 0.5575 - val_loss: 1.3292 - val_accuracy: 0.5069\n",
            "Epoch 935/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2116 - accuracy: 0.5619 - val_loss: 1.3282 - val_accuracy: 0.4919\n",
            "Epoch 936/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2032 - accuracy: 0.5496 - val_loss: 1.3266 - val_accuracy: 0.5104\n",
            "Epoch 937/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.1948 - accuracy: 0.5476 - val_loss: 1.3211 - val_accuracy: 0.5150\n",
            "Epoch 938/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.2055 - accuracy: 0.5481 - val_loss: 1.3295 - val_accuracy: 0.5035\n",
            "Epoch 939/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2166 - accuracy: 0.5496 - val_loss: 1.3143 - val_accuracy: 0.5219\n",
            "Epoch 940/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2019 - accuracy: 0.5530 - val_loss: 1.3260 - val_accuracy: 0.4942\n",
            "Epoch 941/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2002 - accuracy: 0.5585 - val_loss: 1.3211 - val_accuracy: 0.5069\n",
            "Epoch 942/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1890 - accuracy: 0.5674 - val_loss: 1.3170 - val_accuracy: 0.5035\n",
            "Epoch 943/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2201 - accuracy: 0.5441 - val_loss: 1.3198 - val_accuracy: 0.5127\n",
            "Epoch 944/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2001 - accuracy: 0.5565 - val_loss: 1.3101 - val_accuracy: 0.5139\n",
            "Epoch 945/1000\n",
            "127/127 [==============================] - 6s 50ms/step - loss: 1.1947 - accuracy: 0.5565 - val_loss: 1.3144 - val_accuracy: 0.5139\n",
            "Epoch 946/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1831 - accuracy: 0.5610 - val_loss: 1.3108 - val_accuracy: 0.5162\n",
            "Epoch 947/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.1916 - accuracy: 0.5714 - val_loss: 1.3138 - val_accuracy: 0.5115\n",
            "Epoch 948/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.2047 - accuracy: 0.5644 - val_loss: 1.3145 - val_accuracy: 0.5092\n",
            "Epoch 949/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.2022 - accuracy: 0.5570 - val_loss: 1.3085 - val_accuracy: 0.5196\n",
            "Epoch 950/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.1895 - accuracy: 0.5674 - val_loss: 1.3158 - val_accuracy: 0.5081\n",
            "Epoch 951/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.1896 - accuracy: 0.5610 - val_loss: 1.3101 - val_accuracy: 0.5127\n",
            "Epoch 952/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.1920 - accuracy: 0.5575 - val_loss: 1.3112 - val_accuracy: 0.5092\n",
            "Epoch 953/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2059 - accuracy: 0.5396 - val_loss: 1.3132 - val_accuracy: 0.5150\n",
            "Epoch 954/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1945 - accuracy: 0.5595 - val_loss: 1.3189 - val_accuracy: 0.5000\n",
            "Epoch 955/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2015 - accuracy: 0.5555 - val_loss: 1.3235 - val_accuracy: 0.5000\n",
            "Epoch 956/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.1938 - accuracy: 0.5510 - val_loss: 1.3076 - val_accuracy: 0.5104\n",
            "Epoch 957/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1955 - accuracy: 0.5619 - val_loss: 1.3119 - val_accuracy: 0.5127\n",
            "Epoch 958/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1846 - accuracy: 0.5570 - val_loss: 1.3143 - val_accuracy: 0.5012\n",
            "Epoch 959/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1905 - accuracy: 0.5530 - val_loss: 1.3104 - val_accuracy: 0.5081\n",
            "Epoch 960/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1951 - accuracy: 0.5644 - val_loss: 1.3118 - val_accuracy: 0.5115\n",
            "Epoch 961/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.2113 - accuracy: 0.5520 - val_loss: 1.3095 - val_accuracy: 0.5127\n",
            "Epoch 962/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1891 - accuracy: 0.5649 - val_loss: 1.3248 - val_accuracy: 0.5023\n",
            "Epoch 963/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1845 - accuracy: 0.5595 - val_loss: 1.3095 - val_accuracy: 0.5139\n",
            "Epoch 964/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1782 - accuracy: 0.5733 - val_loss: 1.3035 - val_accuracy: 0.5104\n",
            "Epoch 965/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1885 - accuracy: 0.5709 - val_loss: 1.3110 - val_accuracy: 0.5092\n",
            "Epoch 966/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1875 - accuracy: 0.5689 - val_loss: 1.3056 - val_accuracy: 0.5173\n",
            "Epoch 967/1000\n",
            "127/127 [==============================] - 7s 57ms/step - loss: 1.1791 - accuracy: 0.5654 - val_loss: 1.3021 - val_accuracy: 0.5185\n",
            "Epoch 968/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1794 - accuracy: 0.5595 - val_loss: 1.3087 - val_accuracy: 0.5115\n",
            "Epoch 969/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1907 - accuracy: 0.5500 - val_loss: 1.3045 - val_accuracy: 0.5185\n",
            "Epoch 970/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1736 - accuracy: 0.5723 - val_loss: 1.3126 - val_accuracy: 0.5139\n",
            "Epoch 971/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1841 - accuracy: 0.5560 - val_loss: 1.3032 - val_accuracy: 0.5196\n",
            "Epoch 972/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1808 - accuracy: 0.5723 - val_loss: 1.3038 - val_accuracy: 0.5115\n",
            "Epoch 973/1000\n",
            "127/127 [==============================] - 7s 56ms/step - loss: 1.1860 - accuracy: 0.5689 - val_loss: 1.3023 - val_accuracy: 0.5173\n",
            "Epoch 974/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1754 - accuracy: 0.5649 - val_loss: 1.3174 - val_accuracy: 0.5115\n",
            "Epoch 975/1000\n",
            "127/127 [==============================] - 7s 55ms/step - loss: 1.1697 - accuracy: 0.5580 - val_loss: 1.3084 - val_accuracy: 0.5058\n",
            "Epoch 976/1000\n",
            "127/127 [==============================] - 7s 56ms/step - loss: 1.1643 - accuracy: 0.5778 - val_loss: 1.3072 - val_accuracy: 0.5104\n",
            "Epoch 977/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1812 - accuracy: 0.5629 - val_loss: 1.3129 - val_accuracy: 0.5162\n",
            "Epoch 978/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1797 - accuracy: 0.5590 - val_loss: 1.2995 - val_accuracy: 0.5127\n",
            "Epoch 979/1000\n",
            "127/127 [==============================] - 7s 57ms/step - loss: 1.1664 - accuracy: 0.5748 - val_loss: 1.3067 - val_accuracy: 0.5139\n",
            "Epoch 980/1000\n",
            "127/127 [==============================] - 7s 55ms/step - loss: 1.1774 - accuracy: 0.5634 - val_loss: 1.3083 - val_accuracy: 0.5081\n",
            "Epoch 981/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1621 - accuracy: 0.5719 - val_loss: 1.3087 - val_accuracy: 0.5069\n",
            "Epoch 982/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1837 - accuracy: 0.5644 - val_loss: 1.3031 - val_accuracy: 0.5219\n",
            "Epoch 983/1000\n",
            "127/127 [==============================] - 6s 51ms/step - loss: 1.1792 - accuracy: 0.5644 - val_loss: 1.2976 - val_accuracy: 0.5196\n",
            "Epoch 984/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1704 - accuracy: 0.5649 - val_loss: 1.2987 - val_accuracy: 0.5150\n",
            "Epoch 985/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1701 - accuracy: 0.5778 - val_loss: 1.3006 - val_accuracy: 0.5196\n",
            "Epoch 986/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1702 - accuracy: 0.5629 - val_loss: 1.3125 - val_accuracy: 0.5069\n",
            "Epoch 987/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1737 - accuracy: 0.5654 - val_loss: 1.2976 - val_accuracy: 0.5115\n",
            "Epoch 988/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.1638 - accuracy: 0.5778 - val_loss: 1.3032 - val_accuracy: 0.5081\n",
            "Epoch 989/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1591 - accuracy: 0.5738 - val_loss: 1.2912 - val_accuracy: 0.5162\n",
            "Epoch 990/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1882 - accuracy: 0.5575 - val_loss: 1.3130 - val_accuracy: 0.5139\n",
            "Epoch 991/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1737 - accuracy: 0.5699 - val_loss: 1.2939 - val_accuracy: 0.5173\n",
            "Epoch 992/1000\n",
            "127/127 [==============================] - 7s 51ms/step - loss: 1.1740 - accuracy: 0.5629 - val_loss: 1.2976 - val_accuracy: 0.5127\n",
            "Epoch 993/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1779 - accuracy: 0.5684 - val_loss: 1.3050 - val_accuracy: 0.5092\n",
            "Epoch 994/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1725 - accuracy: 0.5639 - val_loss: 1.3019 - val_accuracy: 0.4977\n",
            "Epoch 995/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1557 - accuracy: 0.5803 - val_loss: 1.2956 - val_accuracy: 0.5173\n",
            "Epoch 996/1000\n",
            "127/127 [==============================] - 7s 52ms/step - loss: 1.1715 - accuracy: 0.5664 - val_loss: 1.2918 - val_accuracy: 0.5231\n",
            "Epoch 997/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1681 - accuracy: 0.5709 - val_loss: 1.3037 - val_accuracy: 0.5127\n",
            "Epoch 998/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1673 - accuracy: 0.5758 - val_loss: 1.3022 - val_accuracy: 0.5081\n",
            "Epoch 999/1000\n",
            "127/127 [==============================] - 7s 53ms/step - loss: 1.1740 - accuracy: 0.5723 - val_loss: 1.2985 - val_accuracy: 0.5069\n",
            "Epoch 1000/1000\n",
            "127/127 [==============================] - 7s 54ms/step - loss: 1.1615 - accuracy: 0.5793 - val_loss: 1.2959 - val_accuracy: 0.5173\n",
            "28/28 - 0s - loss: 1.2959 - accuracy: 0.5173 - 354ms/epoch - 13ms/step\n",
            "LSTM model test accuracy:  0.517320990562439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP\n"
      ],
      "metadata": {
        "id": "mEUfWLu3xcBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(activation='relu', solver='adam', random_state=1)\n",
        "nsamples, nx, ny = X_train.shape\n",
        "d1_X_train = X_train.reshape((nsamples,nx*ny))\n",
        "\n",
        "nsamples, nx, ny = X_test.shape\n",
        "d2_X_test = X_test.reshape((nsamples,nx*ny))\n",
        "\n",
        "mlp.fit(d1_X_train, y_train)\n",
        "mlp.score(d2_X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WS4DDa8Pdyk",
        "outputId": "8e4877b3-d229-4c88-bf1c-18863eea2288"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48498845265588914"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDaH3smvCfaB"
      },
      "source": [
        "####                                                                                THANK YOU !!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "SpeechRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}