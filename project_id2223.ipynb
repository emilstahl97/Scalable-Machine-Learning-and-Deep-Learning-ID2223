{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilstahl97/Scalable-Machine-Learning-and-Deep-Learning-ID2223/blob/notebooks/project_id2223.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2a0tOM7v0Em",
        "outputId": "12eea592-b903-4993-9585-60899e27ded7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset already exists\n",
            "Your configuration specifies to merge with the ref 'refs/heads/main'\n",
            "from the remote, but no such ref was fetched.\n",
            "03-01-01-01-01-01-01.wav  03-01-04-01-01-01-01.wav  03-01-06-02-01-01-01.wav\n",
            "03-01-01-01-01-01-02.wav  03-01-04-01-01-01-02.wav  03-01-06-02-01-01-02.wav\n",
            "03-01-01-01-01-01-03.wav  03-01-04-01-01-01-03.wav  03-01-06-02-01-01-03.wav\n",
            "03-01-01-01-01-01-04.wav  03-01-04-01-01-01-04.wav  03-01-06-02-01-01-04.wav\n",
            "03-01-01-01-01-01-06.wav  03-01-04-01-01-01-06.wav  03-01-06-02-01-01-06.wav\n",
            "03-01-01-01-01-01-07.wav  03-01-04-01-01-01-07.wav  03-01-06-02-01-01-07.wav\n",
            "03-01-01-01-01-01-08.wav  03-01-04-01-01-01-08.wav  03-01-06-02-01-01-08.wav\n",
            "03-01-01-01-01-01-09.wav  03-01-04-01-01-01-09.wav  03-01-06-02-01-01-09.wav\n",
            "03-01-01-01-01-01-10.wav  03-01-04-01-01-01-10.wav  03-01-06-02-01-01-10.wav\n",
            "03-01-01-01-01-01-11.wav  03-01-04-01-01-01-11.wav  03-01-06-02-01-01-11.wav\n",
            "03-01-01-01-01-01-12.wav  03-01-04-01-01-01-12.wav  03-01-06-02-01-01-12.wav\n",
            "03-01-01-01-01-01-13.wav  03-01-04-01-01-01-13.wav  03-01-06-02-01-01-13.wav\n",
            "03-01-01-01-01-01-14.wav  03-01-04-01-01-01-14.wav  03-01-06-02-01-01-14.wav\n",
            "03-01-01-01-01-01-15.wav  03-01-04-01-01-01-15.wav  03-01-06-02-01-01-15.wav\n",
            "03-01-01-01-01-01-16.wav  03-01-04-01-01-01-16.wav  03-01-06-02-01-01-16.wav\n",
            "03-01-01-01-01-01-17.wav  03-01-04-01-01-01-17.wav  03-01-06-02-01-01-17.wav\n",
            "03-01-01-01-01-01-18.wav  03-01-04-01-01-01-18.wav  03-01-06-02-01-01-18.wav\n",
            "03-01-01-01-01-01-19.wav  03-01-04-01-01-01-19.wav  03-01-06-02-01-01-19.wav\n",
            "03-01-01-01-01-01-20.wav  03-01-04-01-01-01-20.wav  03-01-06-02-01-01-20.wav\n",
            "03-01-01-01-01-01-21.wav  03-01-04-01-01-01-21.wav  03-01-06-02-01-01-21.wav\n",
            "03-01-01-01-01-01-22.wav  03-01-04-01-01-01-22.wav  03-01-06-02-01-01-22.wav\n",
            "03-01-01-01-01-01-23.wav  03-01-04-01-01-01-23.wav  03-01-06-02-01-01-23.wav\n",
            "03-01-01-01-01-01-24.wav  03-01-04-01-01-01-24.wav  03-01-06-02-01-01-24.wav\n",
            "03-01-01-01-01-02-01.wav  03-01-04-01-01-02-01.wav  03-01-06-02-01-02-01.wav\n",
            "03-01-01-01-01-02-02.wav  03-01-04-01-01-02-02.wav  03-01-06-02-01-02-02.wav\n",
            "03-01-01-01-01-02-03.wav  03-01-04-01-01-02-03.wav  03-01-06-02-01-02-03.wav\n",
            "03-01-01-01-01-02-04.wav  03-01-04-01-01-02-04.wav  03-01-06-02-01-02-04.wav\n",
            "03-01-01-01-01-02-06.wav  03-01-04-01-01-02-06.wav  03-01-06-02-01-02-06.wav\n",
            "03-01-01-01-01-02-07.wav  03-01-04-01-01-02-07.wav  03-01-06-02-01-02-07.wav\n",
            "03-01-01-01-01-02-08.wav  03-01-04-01-01-02-08.wav  03-01-06-02-01-02-08.wav\n",
            "03-01-01-01-01-02-09.wav  03-01-04-01-01-02-09.wav  03-01-06-02-01-02-09.wav\n",
            "03-01-01-01-01-02-10.wav  03-01-04-01-01-02-10.wav  03-01-06-02-01-02-10.wav\n",
            "03-01-01-01-01-02-11.wav  03-01-04-01-01-02-11.wav  03-01-06-02-01-02-11.wav\n",
            "03-01-01-01-01-02-12.wav  03-01-04-01-01-02-12.wav  03-01-06-02-01-02-12.wav\n",
            "03-01-01-01-01-02-13.wav  03-01-04-01-01-02-13.wav  03-01-06-02-01-02-13.wav\n",
            "03-01-01-01-01-02-14.wav  03-01-04-01-01-02-14.wav  03-01-06-02-01-02-14.wav\n",
            "03-01-01-01-01-02-15.wav  03-01-04-01-01-02-15.wav  03-01-06-02-01-02-15.wav\n",
            "03-01-01-01-01-02-16.wav  03-01-04-01-01-02-16.wav  03-01-06-02-01-02-16.wav\n",
            "03-01-01-01-01-02-17.wav  03-01-04-01-01-02-17.wav  03-01-06-02-01-02-17.wav\n",
            "03-01-01-01-01-02-18.wav  03-01-04-01-01-02-18.wav  03-01-06-02-01-02-18.wav\n",
            "03-01-01-01-01-02-19.wav  03-01-04-01-01-02-19.wav  03-01-06-02-01-02-19.wav\n",
            "03-01-01-01-01-02-20.wav  03-01-04-01-01-02-20.wav  03-01-06-02-01-02-20.wav\n",
            "03-01-01-01-01-02-21.wav  03-01-04-01-01-02-21.wav  03-01-06-02-01-02-21.wav\n",
            "03-01-01-01-01-02-22.wav  03-01-04-01-01-02-22.wav  03-01-06-02-01-02-22.wav\n",
            "03-01-01-01-01-02-23.wav  03-01-04-01-01-02-23.wav  03-01-06-02-01-02-23.wav\n",
            "03-01-01-01-01-02-24.wav  03-01-04-01-01-02-24.wav  03-01-06-02-01-02-24.wav\n",
            "03-01-01-01-02-01-01.wav  03-01-04-01-02-01-01.wav  03-01-06-02-02-01-01.wav\n",
            "03-01-01-01-02-01-02.wav  03-01-04-01-02-01-02.wav  03-01-06-02-02-01-02.wav\n",
            "03-01-01-01-02-01-03.wav  03-01-04-01-02-01-03.wav  03-01-06-02-02-01-03.wav\n",
            "03-01-01-01-02-01-04.wav  03-01-04-01-02-01-04.wav  03-01-06-02-02-01-04.wav\n",
            "03-01-01-01-02-01-06.wav  03-01-04-01-02-01-06.wav  03-01-06-02-02-01-06.wav\n",
            "03-01-01-01-02-01-07.wav  03-01-04-01-02-01-07.wav  03-01-06-02-02-01-07.wav\n",
            "03-01-01-01-02-01-08.wav  03-01-04-01-02-01-08.wav  03-01-06-02-02-01-08.wav\n",
            "03-01-01-01-02-01-09.wav  03-01-04-01-02-01-09.wav  03-01-06-02-02-01-09.wav\n",
            "03-01-01-01-02-01-10.wav  03-01-04-01-02-01-10.wav  03-01-06-02-02-01-10.wav\n",
            "03-01-01-01-02-01-11.wav  03-01-04-01-02-01-11.wav  03-01-06-02-02-01-11.wav\n",
            "03-01-01-01-02-01-12.wav  03-01-04-01-02-01-12.wav  03-01-06-02-02-01-12.wav\n",
            "03-01-01-01-02-01-13.wav  03-01-04-01-02-01-13.wav  03-01-06-02-02-01-13.wav\n",
            "03-01-01-01-02-01-14.wav  03-01-04-01-02-01-14.wav  03-01-06-02-02-01-14.wav\n",
            "03-01-01-01-02-01-15.wav  03-01-04-01-02-01-15.wav  03-01-06-02-02-01-15.wav\n",
            "03-01-01-01-02-01-16.wav  03-01-04-01-02-01-16.wav  03-01-06-02-02-01-16.wav\n",
            "03-01-01-01-02-01-17.wav  03-01-04-01-02-01-17.wav  03-01-06-02-02-01-17.wav\n",
            "03-01-01-01-02-01-18.wav  03-01-04-01-02-01-18.wav  03-01-06-02-02-01-18.wav\n",
            "03-01-01-01-02-01-19.wav  03-01-04-01-02-01-19.wav  03-01-06-02-02-01-19.wav\n",
            "03-01-01-01-02-01-20.wav  03-01-04-01-02-01-20.wav  03-01-06-02-02-01-20.wav\n",
            "03-01-01-01-02-01-21.wav  03-01-04-01-02-01-21.wav  03-01-06-02-02-01-21.wav\n",
            "03-01-01-01-02-01-22.wav  03-01-04-01-02-01-22.wav  03-01-06-02-02-01-22.wav\n",
            "03-01-01-01-02-01-23.wav  03-01-04-01-02-01-23.wav  03-01-06-02-02-01-23.wav\n",
            "03-01-01-01-02-01-24.wav  03-01-04-01-02-01-24.wav  03-01-06-02-02-01-24.wav\n",
            "03-01-01-01-02-02-01.wav  03-01-04-01-02-02-01.wav  03-01-06-02-02-02-01.wav\n",
            "03-01-01-01-02-02-02.wav  03-01-04-01-02-02-02.wav  03-01-06-02-02-02-02.wav\n",
            "03-01-01-01-02-02-03.wav  03-01-04-01-02-02-03.wav  03-01-06-02-02-02-03.wav\n",
            "03-01-01-01-02-02-04.wav  03-01-04-01-02-02-04.wav  03-01-06-02-02-02-04.wav\n",
            "03-01-01-01-02-02-06.wav  03-01-04-01-02-02-06.wav  03-01-06-02-02-02-06.wav\n",
            "03-01-01-01-02-02-07.wav  03-01-04-01-02-02-07.wav  03-01-06-02-02-02-07.wav\n",
            "03-01-01-01-02-02-08.wav  03-01-04-01-02-02-08.wav  03-01-06-02-02-02-08.wav\n",
            "03-01-01-01-02-02-09.wav  03-01-04-01-02-02-09.wav  03-01-06-02-02-02-09.wav\n",
            "03-01-01-01-02-02-10.wav  03-01-04-01-02-02-10.wav  03-01-06-02-02-02-10.wav\n",
            "03-01-01-01-02-02-11.wav  03-01-04-01-02-02-11.wav  03-01-06-02-02-02-11.wav\n",
            "03-01-01-01-02-02-12.wav  03-01-04-01-02-02-12.wav  03-01-06-02-02-02-12.wav\n",
            "03-01-01-01-02-02-13.wav  03-01-04-01-02-02-13.wav  03-01-06-02-02-02-13.wav\n",
            "03-01-01-01-02-02-14.wav  03-01-04-01-02-02-14.wav  03-01-06-02-02-02-14.wav\n",
            "03-01-01-01-02-02-15.wav  03-01-04-01-02-02-15.wav  03-01-06-02-02-02-15.wav\n",
            "03-01-01-01-02-02-16.wav  03-01-04-01-02-02-16.wav  03-01-06-02-02-02-16.wav\n",
            "03-01-01-01-02-02-17.wav  03-01-04-01-02-02-17.wav  03-01-06-02-02-02-17.wav\n",
            "03-01-01-01-02-02-18.wav  03-01-04-01-02-02-18.wav  03-01-06-02-02-02-18.wav\n",
            "03-01-01-01-02-02-19.wav  03-01-04-01-02-02-19.wav  03-01-06-02-02-02-19.wav\n",
            "03-01-01-01-02-02-20.wav  03-01-04-01-02-02-20.wav  03-01-06-02-02-02-20.wav\n",
            "03-01-01-01-02-02-21.wav  03-01-04-01-02-02-21.wav  03-01-06-02-02-02-21.wav\n",
            "03-01-01-01-02-02-22.wav  03-01-04-01-02-02-22.wav  03-01-06-02-02-02-22.wav\n",
            "03-01-01-01-02-02-23.wav  03-01-04-01-02-02-23.wav  03-01-06-02-02-02-23.wav\n",
            "03-01-01-01-02-02-24.wav  03-01-04-01-02-02-24.wav  03-01-06-02-02-02-24.wav\n",
            "03-01-02-01-01-01-01.wav  03-01-04-02-01-01-01.wav  03-01-07-01-01-01-01.wav\n",
            "03-01-02-01-01-01-02.wav  03-01-04-02-01-01-02.wav  03-01-07-01-01-01-02.wav\n",
            "03-01-02-01-01-01-03.wav  03-01-04-02-01-01-03.wav  03-01-07-01-01-01-03.wav\n",
            "03-01-02-01-01-01-04.wav  03-01-04-02-01-01-04.wav  03-01-07-01-01-01-04.wav\n",
            "03-01-02-01-01-01-06.wav  03-01-04-02-01-01-06.wav  03-01-07-01-01-01-06.wav\n",
            "03-01-02-01-01-01-07.wav  03-01-04-02-01-01-07.wav  03-01-07-01-01-01-07.wav\n",
            "03-01-02-01-01-01-08.wav  03-01-04-02-01-01-08.wav  03-01-07-01-01-01-08.wav\n",
            "03-01-02-01-01-01-09.wav  03-01-04-02-01-01-09.wav  03-01-07-01-01-01-09.wav\n",
            "03-01-02-01-01-01-10.wav  03-01-04-02-01-01-10.wav  03-01-07-01-01-01-10.wav\n",
            "03-01-02-01-01-01-11.wav  03-01-04-02-01-01-11.wav  03-01-07-01-01-01-11.wav\n",
            "03-01-02-01-01-01-12.wav  03-01-04-02-01-01-12.wav  03-01-07-01-01-01-12.wav\n",
            "03-01-02-01-01-01-13.wav  03-01-04-02-01-01-13.wav  03-01-07-01-01-01-13.wav\n",
            "03-01-02-01-01-01-14.wav  03-01-04-02-01-01-14.wav  03-01-07-01-01-01-14.wav\n",
            "03-01-02-01-01-01-15.wav  03-01-04-02-01-01-15.wav  03-01-07-01-01-01-15.wav\n",
            "03-01-02-01-01-01-16.wav  03-01-04-02-01-01-16.wav  03-01-07-01-01-01-16.wav\n",
            "03-01-02-01-01-01-17.wav  03-01-04-02-01-01-17.wav  03-01-07-01-01-01-17.wav\n",
            "03-01-02-01-01-01-18.wav  03-01-04-02-01-01-18.wav  03-01-07-01-01-01-18.wav\n",
            "03-01-02-01-01-01-19.wav  03-01-04-02-01-01-19.wav  03-01-07-01-01-01-19.wav\n",
            "03-01-02-01-01-01-20.wav  03-01-04-02-01-01-20.wav  03-01-07-01-01-01-20.wav\n",
            "03-01-02-01-01-01-21.wav  03-01-04-02-01-01-21.wav  03-01-07-01-01-01-21.wav\n",
            "03-01-02-01-01-01-22.wav  03-01-04-02-01-01-22.wav  03-01-07-01-01-01-22.wav\n",
            "03-01-02-01-01-01-23.wav  03-01-04-02-01-01-23.wav  03-01-07-01-01-01-23.wav\n",
            "03-01-02-01-01-01-24.wav  03-01-04-02-01-01-24.wav  03-01-07-01-01-01-24.wav\n",
            "03-01-02-01-01-02-01.wav  03-01-04-02-01-02-01.wav  03-01-07-01-01-02-01.wav\n",
            "03-01-02-01-01-02-02.wav  03-01-04-02-01-02-02.wav  03-01-07-01-01-02-02.wav\n",
            "03-01-02-01-01-02-03.wav  03-01-04-02-01-02-03.wav  03-01-07-01-01-02-03.wav\n",
            "03-01-02-01-01-02-04.wav  03-01-04-02-01-02-04.wav  03-01-07-01-01-02-04.wav\n",
            "03-01-02-01-01-02-06.wav  03-01-04-02-01-02-06.wav  03-01-07-01-01-02-06.wav\n",
            "03-01-02-01-01-02-07.wav  03-01-04-02-01-02-07.wav  03-01-07-01-01-02-07.wav\n",
            "03-01-02-01-01-02-08.wav  03-01-04-02-01-02-08.wav  03-01-07-01-01-02-08.wav\n",
            "03-01-02-01-01-02-09.wav  03-01-04-02-01-02-09.wav  03-01-07-01-01-02-09.wav\n",
            "03-01-02-01-01-02-10.wav  03-01-04-02-01-02-10.wav  03-01-07-01-01-02-10.wav\n",
            "03-01-02-01-01-02-11.wav  03-01-04-02-01-02-11.wav  03-01-07-01-01-02-11.wav\n",
            "03-01-02-01-01-02-12.wav  03-01-04-02-01-02-12.wav  03-01-07-01-01-02-12.wav\n",
            "03-01-02-01-01-02-13.wav  03-01-04-02-01-02-13.wav  03-01-07-01-01-02-13.wav\n",
            "03-01-02-01-01-02-14.wav  03-01-04-02-01-02-14.wav  03-01-07-01-01-02-14.wav\n",
            "03-01-02-01-01-02-15.wav  03-01-04-02-01-02-15.wav  03-01-07-01-01-02-15.wav\n",
            "03-01-02-01-01-02-16.wav  03-01-04-02-01-02-16.wav  03-01-07-01-01-02-16.wav\n",
            "03-01-02-01-01-02-17.wav  03-01-04-02-01-02-17.wav  03-01-07-01-01-02-17.wav\n",
            "03-01-02-01-01-02-18.wav  03-01-04-02-01-02-18.wav  03-01-07-01-01-02-18.wav\n",
            "03-01-02-01-01-02-19.wav  03-01-04-02-01-02-19.wav  03-01-07-01-01-02-19.wav\n",
            "03-01-02-01-01-02-20.wav  03-01-04-02-01-02-20.wav  03-01-07-01-01-02-20.wav\n",
            "03-01-02-01-01-02-21.wav  03-01-04-02-01-02-21.wav  03-01-07-01-01-02-21.wav\n",
            "03-01-02-01-01-02-22.wav  03-01-04-02-01-02-22.wav  03-01-07-01-01-02-22.wav\n",
            "03-01-02-01-01-02-23.wav  03-01-04-02-01-02-23.wav  03-01-07-01-01-02-23.wav\n",
            "03-01-02-01-01-02-24.wav  03-01-04-02-01-02-24.wav  03-01-07-01-01-02-24.wav\n",
            "03-01-02-01-02-01-01.wav  03-01-04-02-02-01-01.wav  03-01-07-01-02-01-01.wav\n",
            "03-01-02-01-02-01-02.wav  03-01-04-02-02-01-02.wav  03-01-07-01-02-01-02.wav\n",
            "03-01-02-01-02-01-03.wav  03-01-04-02-02-01-03.wav  03-01-07-01-02-01-03.wav\n",
            "03-01-02-01-02-01-04.wav  03-01-04-02-02-01-04.wav  03-01-07-01-02-01-04.wav\n",
            "03-01-02-01-02-01-06.wav  03-01-04-02-02-01-06.wav  03-01-07-01-02-01-06.wav\n",
            "03-01-02-01-02-01-07.wav  03-01-04-02-02-01-07.wav  03-01-07-01-02-01-07.wav\n",
            "03-01-02-01-02-01-08.wav  03-01-04-02-02-01-08.wav  03-01-07-01-02-01-08.wav\n",
            "03-01-02-01-02-01-09.wav  03-01-04-02-02-01-09.wav  03-01-07-01-02-01-09.wav\n",
            "03-01-02-01-02-01-10.wav  03-01-04-02-02-01-10.wav  03-01-07-01-02-01-10.wav\n",
            "03-01-02-01-02-01-11.wav  03-01-04-02-02-01-11.wav  03-01-07-01-02-01-11.wav\n",
            "03-01-02-01-02-01-12.wav  03-01-04-02-02-01-12.wav  03-01-07-01-02-01-12.wav\n",
            "03-01-02-01-02-01-13.wav  03-01-04-02-02-01-13.wav  03-01-07-01-02-01-13.wav\n",
            "03-01-02-01-02-01-14.wav  03-01-04-02-02-01-14.wav  03-01-07-01-02-01-14.wav\n",
            "03-01-02-01-02-01-15.wav  03-01-04-02-02-01-15.wav  03-01-07-01-02-01-15.wav\n",
            "03-01-02-01-02-01-16.wav  03-01-04-02-02-01-16.wav  03-01-07-01-02-01-16.wav\n",
            "03-01-02-01-02-01-17.wav  03-01-04-02-02-01-17.wav  03-01-07-01-02-01-17.wav\n",
            "03-01-02-01-02-01-18.wav  03-01-04-02-02-01-18.wav  03-01-07-01-02-01-18.wav\n",
            "03-01-02-01-02-01-19.wav  03-01-04-02-02-01-19.wav  03-01-07-01-02-01-19.wav\n",
            "03-01-02-01-02-01-20.wav  03-01-04-02-02-01-20.wav  03-01-07-01-02-01-20.wav\n",
            "03-01-02-01-02-01-21.wav  03-01-04-02-02-01-21.wav  03-01-07-01-02-01-21.wav\n",
            "03-01-02-01-02-01-22.wav  03-01-04-02-02-01-22.wav  03-01-07-01-02-01-22.wav\n",
            "03-01-02-01-02-01-23.wav  03-01-04-02-02-01-23.wav  03-01-07-01-02-01-23.wav\n",
            "03-01-02-01-02-01-24.wav  03-01-04-02-02-01-24.wav  03-01-07-01-02-01-24.wav\n",
            "03-01-02-01-02-02-01.wav  03-01-04-02-02-02-01.wav  03-01-07-01-02-02-01.wav\n",
            "03-01-02-01-02-02-02.wav  03-01-04-02-02-02-02.wav  03-01-07-01-02-02-02.wav\n",
            "03-01-02-01-02-02-03.wav  03-01-04-02-02-02-03.wav  03-01-07-01-02-02-03.wav\n",
            "03-01-02-01-02-02-04.wav  03-01-04-02-02-02-04.wav  03-01-07-01-02-02-04.wav\n",
            "03-01-02-01-02-02-06.wav  03-01-04-02-02-02-06.wav  03-01-07-01-02-02-06.wav\n",
            "03-01-02-01-02-02-07.wav  03-01-04-02-02-02-07.wav  03-01-07-01-02-02-07.wav\n",
            "03-01-02-01-02-02-08.wav  03-01-04-02-02-02-08.wav  03-01-07-01-02-02-08.wav\n",
            "03-01-02-01-02-02-09.wav  03-01-04-02-02-02-09.wav  03-01-07-01-02-02-09.wav\n",
            "03-01-02-01-02-02-10.wav  03-01-04-02-02-02-10.wav  03-01-07-01-02-02-10.wav\n",
            "03-01-02-01-02-02-11.wav  03-01-04-02-02-02-11.wav  03-01-07-01-02-02-11.wav\n",
            "03-01-02-01-02-02-12.wav  03-01-04-02-02-02-12.wav  03-01-07-01-02-02-12.wav\n",
            "03-01-02-01-02-02-13.wav  03-01-04-02-02-02-13.wav  03-01-07-01-02-02-13.wav\n",
            "03-01-02-01-02-02-14.wav  03-01-04-02-02-02-14.wav  03-01-07-01-02-02-14.wav\n",
            "03-01-02-01-02-02-15.wav  03-01-04-02-02-02-15.wav  03-01-07-01-02-02-15.wav\n",
            "03-01-02-01-02-02-16.wav  03-01-04-02-02-02-16.wav  03-01-07-01-02-02-16.wav\n",
            "03-01-02-01-02-02-17.wav  03-01-04-02-02-02-17.wav  03-01-07-01-02-02-17.wav\n",
            "03-01-02-01-02-02-18.wav  03-01-04-02-02-02-18.wav  03-01-07-01-02-02-18.wav\n",
            "03-01-02-01-02-02-19.wav  03-01-04-02-02-02-19.wav  03-01-07-01-02-02-19.wav\n",
            "03-01-02-01-02-02-20.wav  03-01-04-02-02-02-20.wav  03-01-07-01-02-02-20.wav\n",
            "03-01-02-01-02-02-21.wav  03-01-04-02-02-02-21.wav  03-01-07-01-02-02-21.wav\n",
            "03-01-02-01-02-02-22.wav  03-01-04-02-02-02-22.wav  03-01-07-01-02-02-22.wav\n",
            "03-01-02-01-02-02-23.wav  03-01-04-02-02-02-23.wav  03-01-07-01-02-02-23.wav\n",
            "03-01-02-01-02-02-24.wav  03-01-04-02-02-02-24.wav  03-01-07-01-02-02-24.wav\n",
            "03-01-02-02-01-01-01.wav  03-01-05-01-01-01-01.wav  03-01-07-02-01-01-01.wav\n",
            "03-01-02-02-01-01-02.wav  03-01-05-01-01-01-02.wav  03-01-07-02-01-01-02.wav\n",
            "03-01-02-02-01-01-03.wav  03-01-05-01-01-01-03.wav  03-01-07-02-01-01-03.wav\n",
            "03-01-02-02-01-01-04.wav  03-01-05-01-01-01-04.wav  03-01-07-02-01-01-04.wav\n",
            "03-01-02-02-01-01-06.wav  03-01-05-01-01-01-06.wav  03-01-07-02-01-01-06.wav\n",
            "03-01-02-02-01-01-07.wav  03-01-05-01-01-01-07.wav  03-01-07-02-01-01-07.wav\n",
            "03-01-02-02-01-01-08.wav  03-01-05-01-01-01-08.wav  03-01-07-02-01-01-08.wav\n",
            "03-01-02-02-01-01-09.wav  03-01-05-01-01-01-09.wav  03-01-07-02-01-01-09.wav\n",
            "03-01-02-02-01-01-10.wav  03-01-05-01-01-01-10.wav  03-01-07-02-01-01-10.wav\n",
            "03-01-02-02-01-01-11.wav  03-01-05-01-01-01-11.wav  03-01-07-02-01-01-11.wav\n",
            "03-01-02-02-01-01-12.wav  03-01-05-01-01-01-12.wav  03-01-07-02-01-01-12.wav\n",
            "03-01-02-02-01-01-13.wav  03-01-05-01-01-01-13.wav  03-01-07-02-01-01-13.wav\n",
            "03-01-02-02-01-01-14.wav  03-01-05-01-01-01-14.wav  03-01-07-02-01-01-14.wav\n",
            "03-01-02-02-01-01-15.wav  03-01-05-01-01-01-15.wav  03-01-07-02-01-01-15.wav\n",
            "03-01-02-02-01-01-16.wav  03-01-05-01-01-01-16.wav  03-01-07-02-01-01-16.wav\n",
            "03-01-02-02-01-01-17.wav  03-01-05-01-01-01-17.wav  03-01-07-02-01-01-17.wav\n",
            "03-01-02-02-01-01-18.wav  03-01-05-01-01-01-18.wav  03-01-07-02-01-01-18.wav\n",
            "03-01-02-02-01-01-19.wav  03-01-05-01-01-01-19.wav  03-01-07-02-01-01-19.wav\n",
            "03-01-02-02-01-01-20.wav  03-01-05-01-01-01-20.wav  03-01-07-02-01-01-20.wav\n",
            "03-01-02-02-01-01-21.wav  03-01-05-01-01-01-21.wav  03-01-07-02-01-01-21.wav\n",
            "03-01-02-02-01-01-22.wav  03-01-05-01-01-01-22.wav  03-01-07-02-01-01-22.wav\n",
            "03-01-02-02-01-01-23.wav  03-01-05-01-01-01-23.wav  03-01-07-02-01-01-23.wav\n",
            "03-01-02-02-01-01-24.wav  03-01-05-01-01-01-24.wav  03-01-07-02-01-01-24.wav\n",
            "03-01-02-02-01-02-01.wav  03-01-05-01-01-02-01.wav  03-01-07-02-01-02-01.wav\n",
            "03-01-02-02-01-02-02.wav  03-01-05-01-01-02-02.wav  03-01-07-02-01-02-02.wav\n",
            "03-01-02-02-01-02-03.wav  03-01-05-01-01-02-03.wav  03-01-07-02-01-02-03.wav\n",
            "03-01-02-02-01-02-04.wav  03-01-05-01-01-02-04.wav  03-01-07-02-01-02-04.wav\n",
            "03-01-02-02-01-02-06.wav  03-01-05-01-01-02-06.wav  03-01-07-02-01-02-06.wav\n",
            "03-01-02-02-01-02-07.wav  03-01-05-01-01-02-07.wav  03-01-07-02-01-02-07.wav\n",
            "03-01-02-02-01-02-08.wav  03-01-05-01-01-02-08.wav  03-01-07-02-01-02-08.wav\n",
            "03-01-02-02-01-02-09.wav  03-01-05-01-01-02-09.wav  03-01-07-02-01-02-09.wav\n",
            "03-01-02-02-01-02-10.wav  03-01-05-01-01-02-10.wav  03-01-07-02-01-02-10.wav\n",
            "03-01-02-02-01-02-11.wav  03-01-05-01-01-02-11.wav  03-01-07-02-01-02-11.wav\n",
            "03-01-02-02-01-02-12.wav  03-01-05-01-01-02-12.wav  03-01-07-02-01-02-12.wav\n",
            "03-01-02-02-01-02-13.wav  03-01-05-01-01-02-13.wav  03-01-07-02-01-02-13.wav\n",
            "03-01-02-02-01-02-14.wav  03-01-05-01-01-02-14.wav  03-01-07-02-01-02-14.wav\n",
            "03-01-02-02-01-02-15.wav  03-01-05-01-01-02-15.wav  03-01-07-02-01-02-15.wav\n",
            "03-01-02-02-01-02-16.wav  03-01-05-01-01-02-16.wav  03-01-07-02-01-02-16.wav\n",
            "03-01-02-02-01-02-17.wav  03-01-05-01-01-02-17.wav  03-01-07-02-01-02-17.wav\n",
            "03-01-02-02-01-02-18.wav  03-01-05-01-01-02-18.wav  03-01-07-02-01-02-18.wav\n",
            "03-01-02-02-01-02-19.wav  03-01-05-01-01-02-19.wav  03-01-07-02-01-02-19.wav\n",
            "03-01-02-02-01-02-20.wav  03-01-05-01-01-02-20.wav  03-01-07-02-01-02-20.wav\n",
            "03-01-02-02-01-02-21.wav  03-01-05-01-01-02-21.wav  03-01-07-02-01-02-21.wav\n",
            "03-01-02-02-01-02-22.wav  03-01-05-01-01-02-22.wav  03-01-07-02-01-02-22.wav\n",
            "03-01-02-02-01-02-23.wav  03-01-05-01-01-02-23.wav  03-01-07-02-01-02-23.wav\n",
            "03-01-02-02-01-02-24.wav  03-01-05-01-01-02-24.wav  03-01-07-02-01-02-24.wav\n",
            "03-01-02-02-02-01-01.wav  03-01-05-01-02-01-01.wav  03-01-07-02-02-01-01.wav\n",
            "03-01-02-02-02-01-02.wav  03-01-05-01-02-01-02.wav  03-01-07-02-02-01-02.wav\n",
            "03-01-02-02-02-01-03.wav  03-01-05-01-02-01-03.wav  03-01-07-02-02-01-03.wav\n",
            "03-01-02-02-02-01-04.wav  03-01-05-01-02-01-04.wav  03-01-07-02-02-01-04.wav\n",
            "03-01-02-02-02-01-06.wav  03-01-05-01-02-01-06.wav  03-01-07-02-02-01-06.wav\n",
            "03-01-02-02-02-01-07.wav  03-01-05-01-02-01-07.wav  03-01-07-02-02-01-07.wav\n",
            "03-01-02-02-02-01-08.wav  03-01-05-01-02-01-08.wav  03-01-07-02-02-01-08.wav\n",
            "03-01-02-02-02-01-09.wav  03-01-05-01-02-01-09.wav  03-01-07-02-02-01-09.wav\n",
            "03-01-02-02-02-01-10.wav  03-01-05-01-02-01-10.wav  03-01-07-02-02-01-10.wav\n",
            "03-01-02-02-02-01-11.wav  03-01-05-01-02-01-11.wav  03-01-07-02-02-01-11.wav\n",
            "03-01-02-02-02-01-12.wav  03-01-05-01-02-01-12.wav  03-01-07-02-02-01-12.wav\n",
            "03-01-02-02-02-01-13.wav  03-01-05-01-02-01-13.wav  03-01-07-02-02-01-13.wav\n",
            "03-01-02-02-02-01-14.wav  03-01-05-01-02-01-14.wav  03-01-07-02-02-01-14.wav\n",
            "03-01-02-02-02-01-15.wav  03-01-05-01-02-01-15.wav  03-01-07-02-02-01-15.wav\n",
            "03-01-02-02-02-01-16.wav  03-01-05-01-02-01-16.wav  03-01-07-02-02-01-16.wav\n",
            "03-01-02-02-02-01-17.wav  03-01-05-01-02-01-17.wav  03-01-07-02-02-01-17.wav\n",
            "03-01-02-02-02-01-18.wav  03-01-05-01-02-01-18.wav  03-01-07-02-02-01-18.wav\n",
            "03-01-02-02-02-01-19.wav  03-01-05-01-02-01-19.wav  03-01-07-02-02-01-19.wav\n",
            "03-01-02-02-02-01-20.wav  03-01-05-01-02-01-20.wav  03-01-07-02-02-01-20.wav\n",
            "03-01-02-02-02-01-21.wav  03-01-05-01-02-01-21.wav  03-01-07-02-02-01-21.wav\n",
            "03-01-02-02-02-01-22.wav  03-01-05-01-02-01-22.wav  03-01-07-02-02-01-22.wav\n",
            "03-01-02-02-02-01-23.wav  03-01-05-01-02-01-23.wav  03-01-07-02-02-01-23.wav\n",
            "03-01-02-02-02-01-24.wav  03-01-05-01-02-01-24.wav  03-01-07-02-02-01-24.wav\n",
            "03-01-02-02-02-02-01.wav  03-01-05-01-02-02-01.wav  03-01-07-02-02-02-01.wav\n",
            "03-01-02-02-02-02-02.wav  03-01-05-01-02-02-02.wav  03-01-07-02-02-02-02.wav\n",
            "03-01-02-02-02-02-03.wav  03-01-05-01-02-02-03.wav  03-01-07-02-02-02-03.wav\n",
            "03-01-02-02-02-02-04.wav  03-01-05-01-02-02-04.wav  03-01-07-02-02-02-04.wav\n",
            "03-01-02-02-02-02-06.wav  03-01-05-01-02-02-06.wav  03-01-07-02-02-02-06.wav\n",
            "03-01-02-02-02-02-07.wav  03-01-05-01-02-02-07.wav  03-01-07-02-02-02-07.wav\n",
            "03-01-02-02-02-02-08.wav  03-01-05-01-02-02-08.wav  03-01-07-02-02-02-08.wav\n",
            "03-01-02-02-02-02-09.wav  03-01-05-01-02-02-09.wav  03-01-07-02-02-02-09.wav\n",
            "03-01-02-02-02-02-10.wav  03-01-05-01-02-02-10.wav  03-01-07-02-02-02-10.wav\n",
            "03-01-02-02-02-02-11.wav  03-01-05-01-02-02-11.wav  03-01-07-02-02-02-11.wav\n",
            "03-01-02-02-02-02-12.wav  03-01-05-01-02-02-12.wav  03-01-07-02-02-02-12.wav\n",
            "03-01-02-02-02-02-13.wav  03-01-05-01-02-02-13.wav  03-01-07-02-02-02-13.wav\n",
            "03-01-02-02-02-02-14.wav  03-01-05-01-02-02-14.wav  03-01-07-02-02-02-14.wav\n",
            "03-01-02-02-02-02-15.wav  03-01-05-01-02-02-15.wav  03-01-07-02-02-02-15.wav\n",
            "03-01-02-02-02-02-16.wav  03-01-05-01-02-02-16.wav  03-01-07-02-02-02-16.wav\n",
            "03-01-02-02-02-02-17.wav  03-01-05-01-02-02-17.wav  03-01-07-02-02-02-17.wav\n",
            "03-01-02-02-02-02-18.wav  03-01-05-01-02-02-18.wav  03-01-07-02-02-02-18.wav\n",
            "03-01-02-02-02-02-19.wav  03-01-05-01-02-02-19.wav  03-01-07-02-02-02-19.wav\n",
            "03-01-02-02-02-02-20.wav  03-01-05-01-02-02-20.wav  03-01-07-02-02-02-20.wav\n",
            "03-01-02-02-02-02-21.wav  03-01-05-01-02-02-21.wav  03-01-07-02-02-02-21.wav\n",
            "03-01-02-02-02-02-22.wav  03-01-05-01-02-02-22.wav  03-01-07-02-02-02-22.wav\n",
            "03-01-02-02-02-02-23.wav  03-01-05-01-02-02-23.wav  03-01-07-02-02-02-23.wav\n",
            "03-01-02-02-02-02-24.wav  03-01-05-01-02-02-24.wav  03-01-07-02-02-02-24.wav\n",
            "03-01-03-01-01-01-01.wav  03-01-05-02-01-01-01.wav  03-01-08-01-01-01-01.wav\n",
            "03-01-03-01-01-01-02.wav  03-01-05-02-01-01-02.wav  03-01-08-01-01-01-02.wav\n",
            "03-01-03-01-01-01-03.wav  03-01-05-02-01-01-03.wav  03-01-08-01-01-01-03.wav\n",
            "03-01-03-01-01-01-04.wav  03-01-05-02-01-01-04.wav  03-01-08-01-01-01-04.wav\n",
            "03-01-03-01-01-01-06.wav  03-01-05-02-01-01-06.wav  03-01-08-01-01-01-06.wav\n",
            "03-01-03-01-01-01-07.wav  03-01-05-02-01-01-07.wav  03-01-08-01-01-01-07.wav\n",
            "03-01-03-01-01-01-08.wav  03-01-05-02-01-01-08.wav  03-01-08-01-01-01-08.wav\n",
            "03-01-03-01-01-01-09.wav  03-01-05-02-01-01-09.wav  03-01-08-01-01-01-09.wav\n",
            "03-01-03-01-01-01-10.wav  03-01-05-02-01-01-10.wav  03-01-08-01-01-01-10.wav\n",
            "03-01-03-01-01-01-11.wav  03-01-05-02-01-01-11.wav  03-01-08-01-01-01-11.wav\n",
            "03-01-03-01-01-01-12.wav  03-01-05-02-01-01-12.wav  03-01-08-01-01-01-12.wav\n",
            "03-01-03-01-01-01-13.wav  03-01-05-02-01-01-13.wav  03-01-08-01-01-01-13.wav\n",
            "03-01-03-01-01-01-14.wav  03-01-05-02-01-01-14.wav  03-01-08-01-01-01-14.wav\n",
            "03-01-03-01-01-01-15.wav  03-01-05-02-01-01-15.wav  03-01-08-01-01-01-15.wav\n",
            "03-01-03-01-01-01-16.wav  03-01-05-02-01-01-16.wav  03-01-08-01-01-01-16.wav\n",
            "03-01-03-01-01-01-17.wav  03-01-05-02-01-01-17.wav  03-01-08-01-01-01-17.wav\n",
            "03-01-03-01-01-01-18.wav  03-01-05-02-01-01-18.wav  03-01-08-01-01-01-18.wav\n",
            "03-01-03-01-01-01-19.wav  03-01-05-02-01-01-19.wav  03-01-08-01-01-01-19.wav\n",
            "03-01-03-01-01-01-20.wav  03-01-05-02-01-01-20.wav  03-01-08-01-01-01-20.wav\n",
            "03-01-03-01-01-01-21.wav  03-01-05-02-01-01-21.wav  03-01-08-01-01-01-21.wav\n",
            "03-01-03-01-01-01-22.wav  03-01-05-02-01-01-22.wav  03-01-08-01-01-01-22.wav\n",
            "03-01-03-01-01-01-23.wav  03-01-05-02-01-01-23.wav  03-01-08-01-01-01-23.wav\n",
            "03-01-03-01-01-01-24.wav  03-01-05-02-01-01-24.wav  03-01-08-01-01-01-24.wav\n",
            "03-01-03-01-01-02-01.wav  03-01-05-02-01-02-01.wav  03-01-08-01-01-02-01.wav\n",
            "03-01-03-01-01-02-02.wav  03-01-05-02-01-02-02.wav  03-01-08-01-01-02-02.wav\n",
            "03-01-03-01-01-02-03.wav  03-01-05-02-01-02-03.wav  03-01-08-01-01-02-03.wav\n",
            "03-01-03-01-01-02-04.wav  03-01-05-02-01-02-04.wav  03-01-08-01-01-02-04.wav\n",
            "03-01-03-01-01-02-06.wav  03-01-05-02-01-02-06.wav  03-01-08-01-01-02-06.wav\n",
            "03-01-03-01-01-02-07.wav  03-01-05-02-01-02-07.wav  03-01-08-01-01-02-07.wav\n",
            "03-01-03-01-01-02-08.wav  03-01-05-02-01-02-08.wav  03-01-08-01-01-02-08.wav\n",
            "03-01-03-01-01-02-09.wav  03-01-05-02-01-02-09.wav  03-01-08-01-01-02-09.wav\n",
            "03-01-03-01-01-02-10.wav  03-01-05-02-01-02-10.wav  03-01-08-01-01-02-10.wav\n",
            "03-01-03-01-01-02-11.wav  03-01-05-02-01-02-11.wav  03-01-08-01-01-02-11.wav\n",
            "03-01-03-01-01-02-12.wav  03-01-05-02-01-02-12.wav  03-01-08-01-01-02-12.wav\n",
            "03-01-03-01-01-02-13.wav  03-01-05-02-01-02-13.wav  03-01-08-01-01-02-13.wav\n",
            "03-01-03-01-01-02-14.wav  03-01-05-02-01-02-14.wav  03-01-08-01-01-02-14.wav\n",
            "03-01-03-01-01-02-15.wav  03-01-05-02-01-02-15.wav  03-01-08-01-01-02-15.wav\n",
            "03-01-03-01-01-02-16.wav  03-01-05-02-01-02-16.wav  03-01-08-01-01-02-16.wav\n",
            "03-01-03-01-01-02-17.wav  03-01-05-02-01-02-17.wav  03-01-08-01-01-02-17.wav\n",
            "03-01-03-01-01-02-18.wav  03-01-05-02-01-02-18.wav  03-01-08-01-01-02-18.wav\n",
            "03-01-03-01-01-02-19.wav  03-01-05-02-01-02-19.wav  03-01-08-01-01-02-19.wav\n",
            "03-01-03-01-01-02-20.wav  03-01-05-02-01-02-20.wav  03-01-08-01-01-02-20.wav\n",
            "03-01-03-01-01-02-21.wav  03-01-05-02-01-02-21.wav  03-01-08-01-01-02-21.wav\n",
            "03-01-03-01-01-02-22.wav  03-01-05-02-01-02-22.wav  03-01-08-01-01-02-22.wav\n",
            "03-01-03-01-01-02-23.wav  03-01-05-02-01-02-23.wav  03-01-08-01-01-02-23.wav\n",
            "03-01-03-01-01-02-24.wav  03-01-05-02-01-02-24.wav  03-01-08-01-01-02-24.wav\n",
            "03-01-03-01-02-01-01.wav  03-01-05-02-02-01-01.wav  03-01-08-01-02-01-01.wav\n",
            "03-01-03-01-02-01-02.wav  03-01-05-02-02-01-02.wav  03-01-08-01-02-01-02.wav\n",
            "03-01-03-01-02-01-03.wav  03-01-05-02-02-01-03.wav  03-01-08-01-02-01-03.wav\n",
            "03-01-03-01-02-01-04.wav  03-01-05-02-02-01-04.wav  03-01-08-01-02-01-04.wav\n",
            "03-01-03-01-02-01-06.wav  03-01-05-02-02-01-06.wav  03-01-08-01-02-01-06.wav\n",
            "03-01-03-01-02-01-07.wav  03-01-05-02-02-01-07.wav  03-01-08-01-02-01-07.wav\n",
            "03-01-03-01-02-01-08.wav  03-01-05-02-02-01-08.wav  03-01-08-01-02-01-08.wav\n",
            "03-01-03-01-02-01-09.wav  03-01-05-02-02-01-09.wav  03-01-08-01-02-01-09.wav\n",
            "03-01-03-01-02-01-10.wav  03-01-05-02-02-01-10.wav  03-01-08-01-02-01-10.wav\n",
            "03-01-03-01-02-01-11.wav  03-01-05-02-02-01-11.wav  03-01-08-01-02-01-11.wav\n",
            "03-01-03-01-02-01-12.wav  03-01-05-02-02-01-12.wav  03-01-08-01-02-01-12.wav\n",
            "03-01-03-01-02-01-13.wav  03-01-05-02-02-01-13.wav  03-01-08-01-02-01-13.wav\n",
            "03-01-03-01-02-01-14.wav  03-01-05-02-02-01-14.wav  03-01-08-01-02-01-14.wav\n",
            "03-01-03-01-02-01-15.wav  03-01-05-02-02-01-15.wav  03-01-08-01-02-01-15.wav\n",
            "03-01-03-01-02-01-16.wav  03-01-05-02-02-01-16.wav  03-01-08-01-02-01-16.wav\n",
            "03-01-03-01-02-01-17.wav  03-01-05-02-02-01-17.wav  03-01-08-01-02-01-17.wav\n",
            "03-01-03-01-02-01-18.wav  03-01-05-02-02-01-18.wav  03-01-08-01-02-01-18.wav\n",
            "03-01-03-01-02-01-19.wav  03-01-05-02-02-01-19.wav  03-01-08-01-02-01-19.wav\n",
            "03-01-03-01-02-01-20.wav  03-01-05-02-02-01-20.wav  03-01-08-01-02-01-20.wav\n",
            "03-01-03-01-02-01-21.wav  03-01-05-02-02-01-21.wav  03-01-08-01-02-01-21.wav\n",
            "03-01-03-01-02-01-22.wav  03-01-05-02-02-01-22.wav  03-01-08-01-02-01-22.wav\n",
            "03-01-03-01-02-01-23.wav  03-01-05-02-02-01-23.wav  03-01-08-01-02-01-23.wav\n",
            "03-01-03-01-02-01-24.wav  03-01-05-02-02-01-24.wav  03-01-08-01-02-01-24.wav\n",
            "03-01-03-01-02-02-01.wav  03-01-05-02-02-02-01.wav  03-01-08-01-02-02-01.wav\n",
            "03-01-03-01-02-02-02.wav  03-01-05-02-02-02-02.wav  03-01-08-01-02-02-02.wav\n",
            "03-01-03-01-02-02-03.wav  03-01-05-02-02-02-03.wav  03-01-08-01-02-02-03.wav\n",
            "03-01-03-01-02-02-04.wav  03-01-05-02-02-02-04.wav  03-01-08-01-02-02-04.wav\n",
            "03-01-03-01-02-02-06.wav  03-01-05-02-02-02-06.wav  03-01-08-01-02-02-06.wav\n",
            "03-01-03-01-02-02-07.wav  03-01-05-02-02-02-07.wav  03-01-08-01-02-02-07.wav\n",
            "03-01-03-01-02-02-08.wav  03-01-05-02-02-02-08.wav  03-01-08-01-02-02-08.wav\n",
            "03-01-03-01-02-02-09.wav  03-01-05-02-02-02-09.wav  03-01-08-01-02-02-09.wav\n",
            "03-01-03-01-02-02-10.wav  03-01-05-02-02-02-10.wav  03-01-08-01-02-02-10.wav\n",
            "03-01-03-01-02-02-11.wav  03-01-05-02-02-02-11.wav  03-01-08-01-02-02-11.wav\n",
            "03-01-03-01-02-02-12.wav  03-01-05-02-02-02-12.wav  03-01-08-01-02-02-12.wav\n",
            "03-01-03-01-02-02-13.wav  03-01-05-02-02-02-13.wav  03-01-08-01-02-02-13.wav\n",
            "03-01-03-01-02-02-14.wav  03-01-05-02-02-02-14.wav  03-01-08-01-02-02-14.wav\n",
            "03-01-03-01-02-02-15.wav  03-01-05-02-02-02-15.wav  03-01-08-01-02-02-15.wav\n",
            "03-01-03-01-02-02-16.wav  03-01-05-02-02-02-16.wav  03-01-08-01-02-02-16.wav\n",
            "03-01-03-01-02-02-17.wav  03-01-05-02-02-02-17.wav  03-01-08-01-02-02-17.wav\n",
            "03-01-03-01-02-02-18.wav  03-01-05-02-02-02-18.wav  03-01-08-01-02-02-18.wav\n",
            "03-01-03-01-02-02-19.wav  03-01-05-02-02-02-19.wav  03-01-08-01-02-02-19.wav\n",
            "03-01-03-01-02-02-20.wav  03-01-05-02-02-02-20.wav  03-01-08-01-02-02-20.wav\n",
            "03-01-03-01-02-02-21.wav  03-01-05-02-02-02-21.wav  03-01-08-01-02-02-21.wav\n",
            "03-01-03-01-02-02-22.wav  03-01-05-02-02-02-22.wav  03-01-08-01-02-02-22.wav\n",
            "03-01-03-01-02-02-23.wav  03-01-05-02-02-02-23.wav  03-01-08-01-02-02-23.wav\n",
            "03-01-03-01-02-02-24.wav  03-01-05-02-02-02-24.wav  03-01-08-01-02-02-24.wav\n",
            "03-01-03-02-01-01-01.wav  03-01-06-01-01-01-01.wav  03-01-08-02-01-01-01.wav\n",
            "03-01-03-02-01-01-02.wav  03-01-06-01-01-01-02.wav  03-01-08-02-01-01-02.wav\n",
            "03-01-03-02-01-01-03.wav  03-01-06-01-01-01-03.wav  03-01-08-02-01-01-03.wav\n",
            "03-01-03-02-01-01-04.wav  03-01-06-01-01-01-04.wav  03-01-08-02-01-01-04.wav\n",
            "03-01-03-02-01-01-06.wav  03-01-06-01-01-01-06.wav  03-01-08-02-01-01-06.wav\n",
            "03-01-03-02-01-01-07.wav  03-01-06-01-01-01-07.wav  03-01-08-02-01-01-07.wav\n",
            "03-01-03-02-01-01-08.wav  03-01-06-01-01-01-08.wav  03-01-08-02-01-01-08.wav\n",
            "03-01-03-02-01-01-09.wav  03-01-06-01-01-01-09.wav  03-01-08-02-01-01-09.wav\n",
            "03-01-03-02-01-01-10.wav  03-01-06-01-01-01-10.wav  03-01-08-02-01-01-10.wav\n",
            "03-01-03-02-01-01-11.wav  03-01-06-01-01-01-11.wav  03-01-08-02-01-01-11.wav\n",
            "03-01-03-02-01-01-12.wav  03-01-06-01-01-01-12.wav  03-01-08-02-01-01-12.wav\n",
            "03-01-03-02-01-01-13.wav  03-01-06-01-01-01-13.wav  03-01-08-02-01-01-13.wav\n",
            "03-01-03-02-01-01-14.wav  03-01-06-01-01-01-14.wav  03-01-08-02-01-01-14.wav\n",
            "03-01-03-02-01-01-15.wav  03-01-06-01-01-01-15.wav  03-01-08-02-01-01-15.wav\n",
            "03-01-03-02-01-01-16.wav  03-01-06-01-01-01-16.wav  03-01-08-02-01-01-16.wav\n",
            "03-01-03-02-01-01-17.wav  03-01-06-01-01-01-17.wav  03-01-08-02-01-01-17.wav\n",
            "03-01-03-02-01-01-18.wav  03-01-06-01-01-01-18.wav  03-01-08-02-01-01-18.wav\n",
            "03-01-03-02-01-01-19.wav  03-01-06-01-01-01-19.wav  03-01-08-02-01-01-19.wav\n",
            "03-01-03-02-01-01-20.wav  03-01-06-01-01-01-20.wav  03-01-08-02-01-01-20.wav\n",
            "03-01-03-02-01-01-21.wav  03-01-06-01-01-01-21.wav  03-01-08-02-01-01-21.wav\n",
            "03-01-03-02-01-01-22.wav  03-01-06-01-01-01-22.wav  03-01-08-02-01-01-22.wav\n",
            "03-01-03-02-01-01-23.wav  03-01-06-01-01-01-23.wav  03-01-08-02-01-01-23.wav\n",
            "03-01-03-02-01-01-24.wav  03-01-06-01-01-01-24.wav  03-01-08-02-01-01-24.wav\n",
            "03-01-03-02-01-02-01.wav  03-01-06-01-01-02-01.wav  03-01-08-02-01-02-01.wav\n",
            "03-01-03-02-01-02-02.wav  03-01-06-01-01-02-02.wav  03-01-08-02-01-02-02.wav\n",
            "03-01-03-02-01-02-03.wav  03-01-06-01-01-02-03.wav  03-01-08-02-01-02-03.wav\n",
            "03-01-03-02-01-02-04.wav  03-01-06-01-01-02-04.wav  03-01-08-02-01-02-04.wav\n",
            "03-01-03-02-01-02-06.wav  03-01-06-01-01-02-06.wav  03-01-08-02-01-02-06.wav\n",
            "03-01-03-02-01-02-07.wav  03-01-06-01-01-02-07.wav  03-01-08-02-01-02-07.wav\n",
            "03-01-03-02-01-02-08.wav  03-01-06-01-01-02-08.wav  03-01-08-02-01-02-08.wav\n",
            "03-01-03-02-01-02-09.wav  03-01-06-01-01-02-09.wav  03-01-08-02-01-02-09.wav\n",
            "03-01-03-02-01-02-10.wav  03-01-06-01-01-02-10.wav  03-01-08-02-01-02-10.wav\n",
            "03-01-03-02-01-02-11.wav  03-01-06-01-01-02-11.wav  03-01-08-02-01-02-11.wav\n",
            "03-01-03-02-01-02-12.wav  03-01-06-01-01-02-12.wav  03-01-08-02-01-02-12.wav\n",
            "03-01-03-02-01-02-13.wav  03-01-06-01-01-02-13.wav  03-01-08-02-01-02-13.wav\n",
            "03-01-03-02-01-02-14.wav  03-01-06-01-01-02-14.wav  03-01-08-02-01-02-14.wav\n",
            "03-01-03-02-01-02-15.wav  03-01-06-01-01-02-15.wav  03-01-08-02-01-02-15.wav\n",
            "03-01-03-02-01-02-16.wav  03-01-06-01-01-02-16.wav  03-01-08-02-01-02-16.wav\n",
            "03-01-03-02-01-02-17.wav  03-01-06-01-01-02-17.wav  03-01-08-02-01-02-17.wav\n",
            "03-01-03-02-01-02-18.wav  03-01-06-01-01-02-18.wav  03-01-08-02-01-02-18.wav\n",
            "03-01-03-02-01-02-19.wav  03-01-06-01-01-02-19.wav  03-01-08-02-01-02-19.wav\n",
            "03-01-03-02-01-02-20.wav  03-01-06-01-01-02-20.wav  03-01-08-02-01-02-20.wav\n",
            "03-01-03-02-01-02-21.wav  03-01-06-01-01-02-21.wav  03-01-08-02-01-02-21.wav\n",
            "03-01-03-02-01-02-22.wav  03-01-06-01-01-02-22.wav  03-01-08-02-01-02-22.wav\n",
            "03-01-03-02-01-02-23.wav  03-01-06-01-01-02-23.wav  03-01-08-02-01-02-23.wav\n",
            "03-01-03-02-01-02-24.wav  03-01-06-01-01-02-24.wav  03-01-08-02-01-02-24.wav\n",
            "03-01-03-02-02-01-01.wav  03-01-06-01-02-01-01.wav  03-01-08-02-02-01-01.wav\n",
            "03-01-03-02-02-01-02.wav  03-01-06-01-02-01-02.wav  03-01-08-02-02-01-02.wav\n",
            "03-01-03-02-02-01-03.wav  03-01-06-01-02-01-03.wav  03-01-08-02-02-01-03.wav\n",
            "03-01-03-02-02-01-04.wav  03-01-06-01-02-01-04.wav  03-01-08-02-02-01-04.wav\n",
            "03-01-03-02-02-01-06.wav  03-01-06-01-02-01-06.wav  03-01-08-02-02-01-06.wav\n",
            "03-01-03-02-02-01-07.wav  03-01-06-01-02-01-07.wav  03-01-08-02-02-01-07.wav\n",
            "03-01-03-02-02-01-08.wav  03-01-06-01-02-01-08.wav  03-01-08-02-02-01-08.wav\n",
            "03-01-03-02-02-01-09.wav  03-01-06-01-02-01-09.wav  03-01-08-02-02-01-09.wav\n",
            "03-01-03-02-02-01-10.wav  03-01-06-01-02-01-10.wav  03-01-08-02-02-01-10.wav\n",
            "03-01-03-02-02-01-11.wav  03-01-06-01-02-01-11.wav  03-01-08-02-02-01-11.wav\n",
            "03-01-03-02-02-01-12.wav  03-01-06-01-02-01-12.wav  03-01-08-02-02-01-12.wav\n",
            "03-01-03-02-02-01-13.wav  03-01-06-01-02-01-13.wav  03-01-08-02-02-01-13.wav\n",
            "03-01-03-02-02-01-14.wav  03-01-06-01-02-01-14.wav  03-01-08-02-02-01-14.wav\n",
            "03-01-03-02-02-01-15.wav  03-01-06-01-02-01-15.wav  03-01-08-02-02-01-15.wav\n",
            "03-01-03-02-02-01-16.wav  03-01-06-01-02-01-16.wav  03-01-08-02-02-01-16.wav\n",
            "03-01-03-02-02-01-17.wav  03-01-06-01-02-01-17.wav  03-01-08-02-02-01-17.wav\n",
            "03-01-03-02-02-01-18.wav  03-01-06-01-02-01-18.wav  03-01-08-02-02-01-18.wav\n",
            "03-01-03-02-02-01-19.wav  03-01-06-01-02-01-19.wav  03-01-08-02-02-01-19.wav\n",
            "03-01-03-02-02-01-20.wav  03-01-06-01-02-01-20.wav  03-01-08-02-02-01-20.wav\n",
            "03-01-03-02-02-01-21.wav  03-01-06-01-02-01-21.wav  03-01-08-02-02-01-21.wav\n",
            "03-01-03-02-02-01-22.wav  03-01-06-01-02-01-22.wav  03-01-08-02-02-01-22.wav\n",
            "03-01-03-02-02-01-23.wav  03-01-06-01-02-01-23.wav  03-01-08-02-02-01-23.wav\n",
            "03-01-03-02-02-01-24.wav  03-01-06-01-02-01-24.wav  03-01-08-02-02-01-24.wav\n",
            "03-01-03-02-02-02-01.wav  03-01-06-01-02-02-01.wav  03-01-08-02-02-02-01.wav\n",
            "03-01-03-02-02-02-02.wav  03-01-06-01-02-02-02.wav  03-01-08-02-02-02-02.wav\n",
            "03-01-03-02-02-02-03.wav  03-01-06-01-02-02-03.wav  03-01-08-02-02-02-03.wav\n",
            "03-01-03-02-02-02-04.wav  03-01-06-01-02-02-04.wav  03-01-08-02-02-02-04.wav\n",
            "03-01-03-02-02-02-06.wav  03-01-06-01-02-02-06.wav  03-01-08-02-02-02-06.wav\n",
            "03-01-03-02-02-02-07.wav  03-01-06-01-02-02-07.wav  03-01-08-02-02-02-07.wav\n",
            "03-01-03-02-02-02-08.wav  03-01-06-01-02-02-08.wav  03-01-08-02-02-02-08.wav\n",
            "03-01-03-02-02-02-09.wav  03-01-06-01-02-02-09.wav  03-01-08-02-02-02-09.wav\n",
            "03-01-03-02-02-02-10.wav  03-01-06-01-02-02-10.wav  03-01-08-02-02-02-10.wav\n",
            "03-01-03-02-02-02-11.wav  03-01-06-01-02-02-11.wav  03-01-08-02-02-02-11.wav\n",
            "03-01-03-02-02-02-12.wav  03-01-06-01-02-02-12.wav  03-01-08-02-02-02-12.wav\n",
            "03-01-03-02-02-02-13.wav  03-01-06-01-02-02-13.wav  03-01-08-02-02-02-13.wav\n",
            "03-01-03-02-02-02-14.wav  03-01-06-01-02-02-14.wav  03-01-08-02-02-02-14.wav\n",
            "03-01-03-02-02-02-15.wav  03-01-06-01-02-02-15.wav  03-01-08-02-02-02-15.wav\n",
            "03-01-03-02-02-02-16.wav  03-01-06-01-02-02-16.wav  03-01-08-02-02-02-16.wav\n",
            "03-01-03-02-02-02-17.wav  03-01-06-01-02-02-17.wav  03-01-08-02-02-02-17.wav\n",
            "03-01-03-02-02-02-18.wav  03-01-06-01-02-02-18.wav  03-01-08-02-02-02-18.wav\n",
            "03-01-03-02-02-02-19.wav  03-01-06-01-02-02-19.wav  03-01-08-02-02-02-19.wav\n",
            "03-01-03-02-02-02-20.wav  03-01-06-01-02-02-20.wav  03-01-08-02-02-02-20.wav\n",
            "03-01-03-02-02-02-21.wav  03-01-06-01-02-02-21.wav  03-01-08-02-02-02-21.wav\n",
            "03-01-03-02-02-02-22.wav  03-01-06-01-02-02-22.wav  03-01-08-02-02-02-22.wav\n",
            "03-01-03-02-02-02-23.wav  03-01-06-01-02-02-23.wav  03-01-08-02-02-02-23.wav\n",
            "03-01-03-02-02-02-24.wav  03-01-06-01-02-02-24.wav  03-01-08-02-02-02-24.wav\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        " import os\n",
        "\n",
        "# README - Execute this cell to mount the notebook in your google drive. \n",
        "# Execute the cell and follow the link to sign and, paste the given key in the little text box. The credentials are only available for you. \n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/audio-dataset\"):\n",
        "  print(\"Pulling dataset\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/audio-dataset\")\n",
        "  !git clone https://github.com/emilstahl97/Audio-dataset.git\n",
        "else:\n",
        "  print(\"Dataset already exists\")\n",
        "\n",
        "#os.chdir(\"/content/drive/MyDrive/audio-dataset/Audio-dataset/Audio-dataset\")\n",
        "os.chdir(\"/content/drive/MyDrive/audio-dataset/Audio-dataset/Rawdata\")\n",
        "\n",
        "!git pull\n",
        "!ls\n",
        "\n",
        "RAVDESS_PATH = \"./RAVDESS\"\n",
        "SAVEE_PATH = \"./SAVEE\"\n",
        "SAVED_MODELS_PATH = \"../saved_models\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbZiFaPP38Hx"
      },
      "source": [
        "##**PIP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "97_tuuAh3_Cc",
        "outputId": "2547907d-e0d5-49ee-d5e9-e9d0ee88dab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     || 15.7 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     || 11.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.28.3-py3-none-any.whl (884 kB)\n",
            "\u001b[K     || 884 kB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Installing collected packages: fonttools, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed fonttools-4.28.3 matplotlib-3.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.4)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     || 38.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.4)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.7.3\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     || 11.3 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade keras\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade matplotlib\n",
        "!pip install --upgrade librosa\n",
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRGucpTn3YnC"
      },
      "source": [
        "##**IMPORTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOis6L1x3dWz",
        "outputId": "1b90f21f-a9da-4122-8719-48a6c368f6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything imported\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io.wavfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "print(\"Everything imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGjXYjveBEXU"
      },
      "source": [
        "## **Make list**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLGlHDznBIP3",
        "outputId": "a4eaca06-9ea7-4c5a-aaaf-fc7ef3afcdfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of audio files: 1380\n",
            "03\n"
          ]
        }
      ],
      "source": [
        "mylist= os.listdir('./')\n",
        "type(mylist)\n",
        "print(\"Number of audio files: {}\".format(len(mylist)))\n",
        "\n",
        "print(mylist[1][6:-16])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozz9uxHjCZTP"
      },
      "source": [
        "##**Get labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wo7TnHteCcjc"
      },
      "outputs": [],
      "source": [
        "feeling_list=[]\n",
        "for item in mylist:\n",
        "    if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_calm')\n",
        "    elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_calm')\n",
        "    elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_happy')\n",
        "    elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_happy')\n",
        "    elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_sad')\n",
        "    elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_sad')\n",
        "    elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_angry')\n",
        "    elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_angry')\n",
        "    elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_fearful')\n",
        "    elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_fearful')\n",
        "    elif item[:1]=='a':\n",
        "        feeling_list.append('male_angry')\n",
        "    elif item[:1]=='f':\n",
        "        feeling_list.append('male_fearful')\n",
        "    elif item[:1]=='h':\n",
        "        feeling_list.append('male_happy')\n",
        "    #elif item[:1]=='n':\n",
        "        #feeling_list.append('neutral')\n",
        "    elif item[:2]=='sa':\n",
        "        feeling_list.append('male_sad')\n",
        "\n",
        "\n",
        "#print(feeling_list)\n",
        "labels = pd.DataFrame(feeling_list)\n",
        "#labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFAv2CDG18S"
      },
      "source": [
        "## Getting the features of audio files using librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "v3m7Wrl_G3Ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56f99fb-d192-4f10-ab34-82419eff3718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             feature\n",
            "0  [-53.306465, -51.239155, -50.09466, -49.656696...\n",
            "1  [-52.999336, -52.90832, -52.419476, -52.407036...\n",
            "2  [-53.141987, -52.67499, -50.214226, -50.819004...\n",
            "3  [-64.933876, -64.933876, -64.933876, -64.93387...\n",
            "4  [-60.311516, -59.17811, -59.345013, -59.10887,...\n",
            "         0          1          2    ...        213        214        215\n",
            "0 -53.306465 -51.239155 -50.094662  ... -57.601852 -56.696125 -58.832737\n",
            "1 -52.999336 -52.908321 -52.419476  ... -53.704510 -53.492561 -53.001705\n",
            "2 -53.141987 -52.674992 -50.214226  ... -54.078613 -56.517986 -57.023308\n",
            "3 -64.933876 -64.933876 -64.933876  ... -64.933876 -64.933876 -64.933876\n",
            "4 -60.311516 -59.178108 -59.345013  ... -46.433640 -47.422729 -48.096535\n",
            "\n",
            "[5 rows x 216 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=['feature'])\n",
        "bookmark=0\n",
        "for index,y in enumerate(mylist):\n",
        "    if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':\n",
        "        X, sample_rate = librosa.load('./'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "        sample_rate = np.array(sample_rate)\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
        "                                            sr=sample_rate, \n",
        "                                            n_mfcc=13),\n",
        "                        axis=0)\n",
        "        feature = mfccs\n",
        "        #[float(i) for i in feature]\n",
        "        #feature1=feature[:135]\n",
        "        df.loc[bookmark] = [feature]\n",
        "        bookmark=bookmark+1\n",
        "\n",
        "\n",
        "\n",
        "print(df[:5])\n",
        "df3 = pd.DataFrame(df['feature'].values.tolist())\n",
        "print(df3[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSMN8I1BcmwP",
        "outputId": "095d920c-41dd-4829-c83d-7320a38e9648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0          1          2    ...        214        215           0  \n",
            "0   -53.306465 -51.239155 -50.094662  ... -56.696125 -58.832737    male_happy\n",
            "1   -52.999336 -52.908321 -52.419476  ... -53.492561 -53.001705  female_happy\n",
            "2   -53.141987 -52.674992 -50.214226  ... -56.517986 -57.023308    male_happy\n",
            "3   -64.933876 -64.933876 -64.933876  ... -64.933876 -64.933876  female_happy\n",
            "4   -60.311516 -59.178108 -59.345013  ... -47.422729 -48.096535    male_happy\n",
            "..         ...        ...        ...  ...        ...        ...           ...\n",
            "915 -48.113033 -48.335693 -50.188248  ... -47.045372 -48.921272  female_happy\n",
            "916 -60.501297 -62.577785 -60.446762  ... -52.179653 -53.992882    male_happy\n",
            "917 -59.302078 -57.921028 -56.350586  ... -54.675850 -56.525337  female_happy\n",
            "918 -57.354610 -56.764095 -56.161160  ... -59.327217 -62.622055    male_happy\n",
            "919 -57.820358 -57.820358 -57.820358  ... -57.820358 -57.820358  female_happy\n",
            "\n",
            "[920 rows x 217 columns]\n"
          ]
        }
      ],
      "source": [
        "df3 = pd.DataFrame(df['feature'].values.tolist())\n",
        "\n",
        "newdf = pd.concat([df3,labels], axis=1)\n",
        "\n",
        "rnewdf = newdf.rename(index=str, columns={\"0\": \"label\"})\n",
        "\n",
        "print(rnewdf)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "rnewdf = shuffle(newdf)\n",
        "\n",
        "rnewdf=rnewdf.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_lhy10dcCl"
      },
      "source": [
        "## Dividing the data into test and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9QVb-vedctd",
        "outputId": "cf51bef5-c469-4d6e-ca73-580f876520e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0          1          2    ...        214        215             0  \n",
            "881 -55.729866 -55.729866 -55.729866  ... -55.690510 -53.630718    female_happy\n",
            "648 -70.267769 -70.267769 -70.267769  ... -61.082741 -60.234661       male_calm\n",
            "217 -53.758949 -53.771038 -53.595619  ... -46.624596 -47.807121      female_sad\n",
            "378 -45.908653 -45.037727 -43.177261  ... -53.422031 -53.422031      male_angry\n",
            "587 -44.614815 -44.014492 -43.963921  ... -42.181381 -42.301792  female_fearful\n",
            "..         ...        ...        ...  ...        ...        ...             ...\n",
            "8   -60.458073 -60.458073 -60.458073  ... -54.711784 -55.621033      male_happy\n",
            "839 -55.940430 -53.966877 -48.489170  ... -54.467537 -56.980522     female_calm\n",
            "16  -62.686787 -62.594624 -63.717712  ... -51.869686 -52.081955      male_happy\n",
            "267 -47.565716 -49.003838 -50.778336  ... -27.407444 -20.110037      female_sad\n",
            "335 -55.958149 -55.958149 -55.958149  ... -55.765541 -55.958149    female_angry\n",
            "\n",
            "[731 rows x 217 columns]\n"
          ]
        }
      ],
      "source": [
        "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
        "train = rnewdf[newdf1]\n",
        "test = rnewdf[~newdf1]\n",
        "\n",
        "print(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OWtB9vs2d2mi"
      },
      "outputs": [],
      "source": [
        "trainfeatures = train.iloc[:, :-1]\n",
        "trainlabel = train.iloc[:, -1:]\n",
        "testfeatures = test.iloc[:, :-1]\n",
        "testlabel = test.iloc[:, -1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NarmNdnzeT0k",
        "outputId": "153523b0-0734-46ef-d8a7-5897741875b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X_train = np.array(trainfeatures)\n",
        "y_train = np.array(trainlabel)\n",
        "X_test = np.array(testfeatures)\n",
        "y_test = np.array(testlabel)\n",
        "\n",
        "lb = LabelEncoder()\n",
        "\n",
        "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBcjgh9uelX9",
        "outputId": "4d30d47d-dcde-4c03-a036-b9c2300f91df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lenB6oTelMx",
        "outputId": "28277269-4230-4944-a220-752f514bb36a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(731, 216)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C904jQhewnK"
      },
      "source": [
        "# Padding sequence for CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdNShUq4e0ty",
        "outputId": "af5dcb05-70f6-4d41-be7f-6f891113cd25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad sequences\n"
          ]
        }
      ],
      "source": [
        "print('Pad sequences')\n",
        "x_traincnn =np.expand_dims(X_train, axis=2)\n",
        "x_testcnn= np.expand_dims(X_test, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nNmxmIo9fIOx"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(216,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "opt = tensorflow.keras.optimizers.RMSprop(learning_rate =0.00001, decay=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbu_xybbfOnh",
        "outputId": "998e62eb-37f6-45c7-fa3d-8a3cd03f298a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 216, 128)          768       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 216, 128)          0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 216, 128)          82048     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 216, 128)          0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 216, 128)          0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 27, 128)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 27, 128)           0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 27, 128)           0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 27, 128)           0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 27, 128)           0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 27, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3456)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                34570     \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 445,578\n",
            "Trainable params: 445,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fyOH0MgMgx2A"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZWRWPKKg0Fu",
        "outputId": "c05c063f-f080-4af6-ad69-fcffff848650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1009\n",
            "23/23 [==============================] - 14s 49ms/step - loss: 2.5986 - accuracy: 0.1122 - val_loss: 2.3010 - val_accuracy: 0.1693\n",
            "Epoch 2/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 2.3486 - accuracy: 0.1450 - val_loss: 2.2534 - val_accuracy: 0.1693\n",
            "Epoch 3/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 2.2907 - accuracy: 0.1532 - val_loss: 2.2356 - val_accuracy: 0.1852\n",
            "Epoch 4/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.2834 - accuracy: 0.1614 - val_loss: 2.2300 - val_accuracy: 0.2063\n",
            "Epoch 5/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.2572 - accuracy: 0.1505 - val_loss: 2.2259 - val_accuracy: 0.1799\n",
            "Epoch 6/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 2.2442 - accuracy: 0.1573 - val_loss: 2.2047 - val_accuracy: 0.2222\n",
            "Epoch 7/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.2205 - accuracy: 0.1833 - val_loss: 2.2011 - val_accuracy: 0.1905\n",
            "Epoch 8/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 2.2133 - accuracy: 0.1737 - val_loss: 2.1822 - val_accuracy: 0.2275\n",
            "Epoch 9/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 2.2226 - accuracy: 0.1560 - val_loss: 2.1757 - val_accuracy: 0.1640\n",
            "Epoch 10/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.1866 - accuracy: 0.1874 - val_loss: 2.1715 - val_accuracy: 0.1746\n",
            "Epoch 11/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 2.1815 - accuracy: 0.1778 - val_loss: 2.1618 - val_accuracy: 0.1958\n",
            "Epoch 12/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.1546 - accuracy: 0.1997 - val_loss: 2.1621 - val_accuracy: 0.2011\n",
            "Epoch 13/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.1583 - accuracy: 0.2093 - val_loss: 2.1508 - val_accuracy: 0.1799\n",
            "Epoch 14/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.1195 - accuracy: 0.2093 - val_loss: 2.1398 - val_accuracy: 0.2063\n",
            "Epoch 15/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.1379 - accuracy: 0.2011 - val_loss: 2.1263 - val_accuracy: 0.1905\n",
            "Epoch 16/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 2.1340 - accuracy: 0.1915 - val_loss: 2.1177 - val_accuracy: 0.2328\n",
            "Epoch 17/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.1033 - accuracy: 0.1943 - val_loss: 2.1119 - val_accuracy: 0.2063\n",
            "Epoch 18/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.0660 - accuracy: 0.2408 - val_loss: 2.0948 - val_accuracy: 0.2169\n",
            "Epoch 19/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 2.0926 - accuracy: 0.2011 - val_loss: 2.0907 - val_accuracy: 0.2011\n",
            "Epoch 20/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.0739 - accuracy: 0.2558 - val_loss: 2.0783 - val_accuracy: 0.2487\n",
            "Epoch 21/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.0591 - accuracy: 0.2066 - val_loss: 2.0681 - val_accuracy: 0.2328\n",
            "Epoch 22/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 2.0639 - accuracy: 0.2312 - val_loss: 2.0631 - val_accuracy: 0.2011\n",
            "Epoch 23/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 2.0563 - accuracy: 0.2216 - val_loss: 2.0561 - val_accuracy: 0.2275\n",
            "Epoch 24/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 2.0265 - accuracy: 0.2326 - val_loss: 2.0498 - val_accuracy: 0.2540\n",
            "Epoch 25/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.0370 - accuracy: 0.2326 - val_loss: 2.0389 - val_accuracy: 0.2381\n",
            "Epoch 26/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 2.0172 - accuracy: 0.2380 - val_loss: 2.0425 - val_accuracy: 0.2011\n",
            "Epoch 27/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 2.0143 - accuracy: 0.2572 - val_loss: 2.0179 - val_accuracy: 0.2540\n",
            "Epoch 28/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.9968 - accuracy: 0.2380 - val_loss: 2.0178 - val_accuracy: 0.2063\n",
            "Epoch 29/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.9797 - accuracy: 0.2722 - val_loss: 2.0153 - val_accuracy: 0.2275\n",
            "Epoch 30/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.9750 - accuracy: 0.2476 - val_loss: 2.0140 - val_accuracy: 0.2222\n",
            "Epoch 31/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.9470 - accuracy: 0.2777 - val_loss: 2.0067 - val_accuracy: 0.2169\n",
            "Epoch 32/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.9512 - accuracy: 0.2613 - val_loss: 1.9823 - val_accuracy: 0.2328\n",
            "Epoch 33/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.9613 - accuracy: 0.2312 - val_loss: 1.9832 - val_accuracy: 0.2593\n",
            "Epoch 34/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.9536 - accuracy: 0.2599 - val_loss: 1.9679 - val_accuracy: 0.2434\n",
            "Epoch 35/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.9403 - accuracy: 0.2804 - val_loss: 1.9695 - val_accuracy: 0.2328\n",
            "Epoch 36/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.9314 - accuracy: 0.2681 - val_loss: 1.9559 - val_accuracy: 0.2857\n",
            "Epoch 37/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.9019 - accuracy: 0.2804 - val_loss: 1.9625 - val_accuracy: 0.1852\n",
            "Epoch 38/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.9150 - accuracy: 0.2517 - val_loss: 1.9440 - val_accuracy: 0.2646\n",
            "Epoch 39/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.9034 - accuracy: 0.2832 - val_loss: 1.9470 - val_accuracy: 0.2646\n",
            "Epoch 40/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.8675 - accuracy: 0.2791 - val_loss: 1.9235 - val_accuracy: 0.2751\n",
            "Epoch 41/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.8858 - accuracy: 0.3010 - val_loss: 1.9138 - val_accuracy: 0.2963\n",
            "Epoch 42/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.8852 - accuracy: 0.2818 - val_loss: 1.9122 - val_accuracy: 0.2646\n",
            "Epoch 43/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.8586 - accuracy: 0.2900 - val_loss: 1.9131 - val_accuracy: 0.2169\n",
            "Epoch 44/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.8438 - accuracy: 0.3269 - val_loss: 1.9110 - val_accuracy: 0.2381\n",
            "Epoch 45/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.8190 - accuracy: 0.3051 - val_loss: 1.9154 - val_accuracy: 0.2275\n",
            "Epoch 46/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.8431 - accuracy: 0.2927 - val_loss: 1.8925 - val_accuracy: 0.2910\n",
            "Epoch 47/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.8396 - accuracy: 0.2900 - val_loss: 1.8848 - val_accuracy: 0.2646\n",
            "Epoch 48/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.8215 - accuracy: 0.3092 - val_loss: 1.8818 - val_accuracy: 0.2698\n",
            "Epoch 49/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.8305 - accuracy: 0.3187 - val_loss: 1.8718 - val_accuracy: 0.2593\n",
            "Epoch 50/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.7901 - accuracy: 0.3352 - val_loss: 1.8730 - val_accuracy: 0.2751\n",
            "Epoch 51/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.8106 - accuracy: 0.3256 - val_loss: 1.8568 - val_accuracy: 0.3228\n",
            "Epoch 52/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.8191 - accuracy: 0.2900 - val_loss: 1.8648 - val_accuracy: 0.2857\n",
            "Epoch 53/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.7873 - accuracy: 0.3215 - val_loss: 1.8501 - val_accuracy: 0.2857\n",
            "Epoch 54/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.7990 - accuracy: 0.3283 - val_loss: 1.8468 - val_accuracy: 0.3122\n",
            "Epoch 55/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.7674 - accuracy: 0.3557 - val_loss: 1.8406 - val_accuracy: 0.2857\n",
            "Epoch 56/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.7559 - accuracy: 0.3406 - val_loss: 1.8289 - val_accuracy: 0.3175\n",
            "Epoch 57/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.7744 - accuracy: 0.3256 - val_loss: 1.8186 - val_accuracy: 0.3333\n",
            "Epoch 58/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.7592 - accuracy: 0.3242 - val_loss: 1.8131 - val_accuracy: 0.3122\n",
            "Epoch 59/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.7570 - accuracy: 0.3502 - val_loss: 1.8165 - val_accuracy: 0.3175\n",
            "Epoch 60/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.7278 - accuracy: 0.3584 - val_loss: 1.8035 - val_accuracy: 0.3122\n",
            "Epoch 61/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.7760 - accuracy: 0.3187 - val_loss: 1.7952 - val_accuracy: 0.3069\n",
            "Epoch 62/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.7440 - accuracy: 0.3488 - val_loss: 1.8091 - val_accuracy: 0.3122\n",
            "Epoch 63/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.7238 - accuracy: 0.3516 - val_loss: 1.7900 - val_accuracy: 0.2963\n",
            "Epoch 64/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.7250 - accuracy: 0.3475 - val_loss: 1.8085 - val_accuracy: 0.3122\n",
            "Epoch 65/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.7001 - accuracy: 0.3557 - val_loss: 1.7859 - val_accuracy: 0.3069\n",
            "Epoch 66/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.7095 - accuracy: 0.3694 - val_loss: 1.7829 - val_accuracy: 0.3069\n",
            "Epoch 67/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.6871 - accuracy: 0.3762 - val_loss: 1.7778 - val_accuracy: 0.3333\n",
            "Epoch 68/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6664 - accuracy: 0.3598 - val_loss: 1.7607 - val_accuracy: 0.3492\n",
            "Epoch 69/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.6914 - accuracy: 0.3570 - val_loss: 1.7538 - val_accuracy: 0.3545\n",
            "Epoch 70/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.6876 - accuracy: 0.3584 - val_loss: 1.7474 - val_accuracy: 0.3333\n",
            "Epoch 71/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6663 - accuracy: 0.3735 - val_loss: 1.7492 - val_accuracy: 0.3439\n",
            "Epoch 72/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.6595 - accuracy: 0.3830 - val_loss: 1.7541 - val_accuracy: 0.3386\n",
            "Epoch 73/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6762 - accuracy: 0.3666 - val_loss: 1.7466 - val_accuracy: 0.3280\n",
            "Epoch 74/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6517 - accuracy: 0.3912 - val_loss: 1.7378 - val_accuracy: 0.3333\n",
            "Epoch 75/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6342 - accuracy: 0.3789 - val_loss: 1.7324 - val_accuracy: 0.3386\n",
            "Epoch 76/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6558 - accuracy: 0.3817 - val_loss: 1.7170 - val_accuracy: 0.3386\n",
            "Epoch 77/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.6319 - accuracy: 0.3844 - val_loss: 1.7031 - val_accuracy: 0.3862\n",
            "Epoch 78/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.6172 - accuracy: 0.4036 - val_loss: 1.7121 - val_accuracy: 0.3492\n",
            "Epoch 79/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6004 - accuracy: 0.3844 - val_loss: 1.7189 - val_accuracy: 0.3704\n",
            "Epoch 80/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6191 - accuracy: 0.3803 - val_loss: 1.7051 - val_accuracy: 0.3704\n",
            "Epoch 81/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5939 - accuracy: 0.3995 - val_loss: 1.6837 - val_accuracy: 0.3439\n",
            "Epoch 82/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.6049 - accuracy: 0.3995 - val_loss: 1.6843 - val_accuracy: 0.3810\n",
            "Epoch 83/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5992 - accuracy: 0.4145 - val_loss: 1.6881 - val_accuracy: 0.3175\n",
            "Epoch 84/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.6249 - accuracy: 0.3899 - val_loss: 1.6815 - val_accuracy: 0.3651\n",
            "Epoch 85/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.6014 - accuracy: 0.4049 - val_loss: 1.6634 - val_accuracy: 0.3968\n",
            "Epoch 86/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5747 - accuracy: 0.3981 - val_loss: 1.6663 - val_accuracy: 0.3545\n",
            "Epoch 87/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5812 - accuracy: 0.3953 - val_loss: 1.6518 - val_accuracy: 0.3651\n",
            "Epoch 88/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.5687 - accuracy: 0.4063 - val_loss: 1.6519 - val_accuracy: 0.3651\n",
            "Epoch 89/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5695 - accuracy: 0.4131 - val_loss: 1.6704 - val_accuracy: 0.3651\n",
            "Epoch 90/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5782 - accuracy: 0.4282 - val_loss: 1.6539 - val_accuracy: 0.3862\n",
            "Epoch 91/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.5553 - accuracy: 0.4254 - val_loss: 1.6338 - val_accuracy: 0.3757\n",
            "Epoch 92/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.5489 - accuracy: 0.4268 - val_loss: 1.6431 - val_accuracy: 0.3545\n",
            "Epoch 93/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.5379 - accuracy: 0.4309 - val_loss: 1.6457 - val_accuracy: 0.3810\n",
            "Epoch 94/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5329 - accuracy: 0.4227 - val_loss: 1.6695 - val_accuracy: 0.3704\n",
            "Epoch 95/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.5262 - accuracy: 0.4282 - val_loss: 1.6273 - val_accuracy: 0.3704\n",
            "Epoch 96/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5337 - accuracy: 0.4186 - val_loss: 1.6501 - val_accuracy: 0.3651\n",
            "Epoch 97/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.5019 - accuracy: 0.4282 - val_loss: 1.6233 - val_accuracy: 0.4233\n",
            "Epoch 98/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.5226 - accuracy: 0.4282 - val_loss: 1.6202 - val_accuracy: 0.3810\n",
            "Epoch 99/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5097 - accuracy: 0.4323 - val_loss: 1.6416 - val_accuracy: 0.3757\n",
            "Epoch 100/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.5030 - accuracy: 0.4405 - val_loss: 1.6160 - val_accuracy: 0.3862\n",
            "Epoch 101/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5181 - accuracy: 0.4268 - val_loss: 1.6119 - val_accuracy: 0.3968\n",
            "Epoch 102/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.4957 - accuracy: 0.4391 - val_loss: 1.6146 - val_accuracy: 0.3492\n",
            "Epoch 103/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.5024 - accuracy: 0.4542 - val_loss: 1.6049 - val_accuracy: 0.4021\n",
            "Epoch 104/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4761 - accuracy: 0.4569 - val_loss: 1.6278 - val_accuracy: 0.3810\n",
            "Epoch 105/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4761 - accuracy: 0.4596 - val_loss: 1.6014 - val_accuracy: 0.4233\n",
            "Epoch 106/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.5148 - accuracy: 0.4254 - val_loss: 1.6097 - val_accuracy: 0.3704\n",
            "Epoch 107/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4658 - accuracy: 0.4460 - val_loss: 1.5915 - val_accuracy: 0.3968\n",
            "Epoch 108/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4971 - accuracy: 0.4460 - val_loss: 1.6035 - val_accuracy: 0.3704\n",
            "Epoch 109/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.4649 - accuracy: 0.4446 - val_loss: 1.5912 - val_accuracy: 0.4074\n",
            "Epoch 110/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4800 - accuracy: 0.4514 - val_loss: 1.5935 - val_accuracy: 0.3810\n",
            "Epoch 111/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4725 - accuracy: 0.4405 - val_loss: 1.5785 - val_accuracy: 0.4233\n",
            "Epoch 112/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4609 - accuracy: 0.4788 - val_loss: 1.5782 - val_accuracy: 0.4180\n",
            "Epoch 113/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4427 - accuracy: 0.4528 - val_loss: 1.5877 - val_accuracy: 0.3757\n",
            "Epoch 114/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4445 - accuracy: 0.4487 - val_loss: 1.5755 - val_accuracy: 0.3915\n",
            "Epoch 115/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4640 - accuracy: 0.4213 - val_loss: 1.5818 - val_accuracy: 0.3915\n",
            "Epoch 116/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.4528 - accuracy: 0.4542 - val_loss: 1.5912 - val_accuracy: 0.3862\n",
            "Epoch 117/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4581 - accuracy: 0.4651 - val_loss: 1.5916 - val_accuracy: 0.3915\n",
            "Epoch 118/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4356 - accuracy: 0.4542 - val_loss: 1.5769 - val_accuracy: 0.3704\n",
            "Epoch 119/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4140 - accuracy: 0.4679 - val_loss: 1.5941 - val_accuracy: 0.3915\n",
            "Epoch 120/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4488 - accuracy: 0.4487 - val_loss: 1.5873 - val_accuracy: 0.4180\n",
            "Epoch 121/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.4309 - accuracy: 0.4569 - val_loss: 1.5909 - val_accuracy: 0.4233\n",
            "Epoch 122/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4330 - accuracy: 0.4446 - val_loss: 1.5718 - val_accuracy: 0.3810\n",
            "Epoch 123/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4316 - accuracy: 0.4542 - val_loss: 1.5657 - val_accuracy: 0.4127\n",
            "Epoch 124/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4516 - accuracy: 0.4432 - val_loss: 1.5829 - val_accuracy: 0.3968\n",
            "Epoch 125/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4212 - accuracy: 0.4569 - val_loss: 1.5785 - val_accuracy: 0.3968\n",
            "Epoch 126/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4130 - accuracy: 0.4692 - val_loss: 1.5641 - val_accuracy: 0.3757\n",
            "Epoch 127/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.4062 - accuracy: 0.4419 - val_loss: 1.5621 - val_accuracy: 0.3915\n",
            "Epoch 128/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4056 - accuracy: 0.4679 - val_loss: 1.6108 - val_accuracy: 0.3915\n",
            "Epoch 129/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4387 - accuracy: 0.4391 - val_loss: 1.5665 - val_accuracy: 0.3757\n",
            "Epoch 130/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3819 - accuracy: 0.5034 - val_loss: 1.5533 - val_accuracy: 0.4021\n",
            "Epoch 131/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4148 - accuracy: 0.4829 - val_loss: 1.5516 - val_accuracy: 0.3915\n",
            "Epoch 132/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4032 - accuracy: 0.4966 - val_loss: 1.5672 - val_accuracy: 0.4180\n",
            "Epoch 133/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.4034 - accuracy: 0.4692 - val_loss: 1.5656 - val_accuracy: 0.3704\n",
            "Epoch 134/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3954 - accuracy: 0.4706 - val_loss: 1.5477 - val_accuracy: 0.4127\n",
            "Epoch 135/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3812 - accuracy: 0.5034 - val_loss: 1.5411 - val_accuracy: 0.4127\n",
            "Epoch 136/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3985 - accuracy: 0.4733 - val_loss: 1.5519 - val_accuracy: 0.3968\n",
            "Epoch 137/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4130 - accuracy: 0.4473 - val_loss: 1.5945 - val_accuracy: 0.3598\n",
            "Epoch 138/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3938 - accuracy: 0.4993 - val_loss: 1.5435 - val_accuracy: 0.4021\n",
            "Epoch 139/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3838 - accuracy: 0.4829 - val_loss: 1.5378 - val_accuracy: 0.4021\n",
            "Epoch 140/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3504 - accuracy: 0.4911 - val_loss: 1.5592 - val_accuracy: 0.4286\n",
            "Epoch 141/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3768 - accuracy: 0.4843 - val_loss: 1.5464 - val_accuracy: 0.3862\n",
            "Epoch 142/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.4016 - accuracy: 0.4843 - val_loss: 1.5555 - val_accuracy: 0.3704\n",
            "Epoch 143/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3799 - accuracy: 0.4692 - val_loss: 1.5380 - val_accuracy: 0.4233\n",
            "Epoch 144/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3749 - accuracy: 0.4692 - val_loss: 1.5370 - val_accuracy: 0.4127\n",
            "Epoch 145/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3693 - accuracy: 0.4843 - val_loss: 1.5541 - val_accuracy: 0.4074\n",
            "Epoch 146/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3695 - accuracy: 0.4843 - val_loss: 1.5442 - val_accuracy: 0.4074\n",
            "Epoch 147/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3769 - accuracy: 0.4925 - val_loss: 1.5249 - val_accuracy: 0.4233\n",
            "Epoch 148/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3894 - accuracy: 0.4897 - val_loss: 1.5398 - val_accuracy: 0.3862\n",
            "Epoch 149/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3536 - accuracy: 0.4979 - val_loss: 1.5332 - val_accuracy: 0.4180\n",
            "Epoch 150/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3419 - accuracy: 0.5048 - val_loss: 1.5286 - val_accuracy: 0.4180\n",
            "Epoch 151/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3569 - accuracy: 0.4925 - val_loss: 1.5345 - val_accuracy: 0.3862\n",
            "Epoch 152/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3534 - accuracy: 0.4938 - val_loss: 1.5423 - val_accuracy: 0.4180\n",
            "Epoch 153/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3704 - accuracy: 0.4952 - val_loss: 1.5381 - val_accuracy: 0.3968\n",
            "Epoch 154/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3644 - accuracy: 0.4911 - val_loss: 1.5349 - val_accuracy: 0.4074\n",
            "Epoch 155/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3580 - accuracy: 0.5171 - val_loss: 1.5313 - val_accuracy: 0.3968\n",
            "Epoch 156/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3568 - accuracy: 0.4925 - val_loss: 1.5337 - val_accuracy: 0.3704\n",
            "Epoch 157/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3415 - accuracy: 0.4802 - val_loss: 1.5200 - val_accuracy: 0.4392\n",
            "Epoch 158/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3155 - accuracy: 0.5212 - val_loss: 1.5373 - val_accuracy: 0.3915\n",
            "Epoch 159/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3572 - accuracy: 0.4815 - val_loss: 1.5284 - val_accuracy: 0.4286\n",
            "Epoch 160/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3482 - accuracy: 0.4897 - val_loss: 1.5375 - val_accuracy: 0.4286\n",
            "Epoch 161/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3421 - accuracy: 0.4815 - val_loss: 1.5116 - val_accuracy: 0.4603\n",
            "Epoch 162/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3319 - accuracy: 0.4911 - val_loss: 1.5118 - val_accuracy: 0.4286\n",
            "Epoch 163/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.3269 - accuracy: 0.5075 - val_loss: 1.5584 - val_accuracy: 0.4021\n",
            "Epoch 164/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3188 - accuracy: 0.5130 - val_loss: 1.5357 - val_accuracy: 0.4127\n",
            "Epoch 165/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3031 - accuracy: 0.5185 - val_loss: 1.5372 - val_accuracy: 0.4074\n",
            "Epoch 166/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3060 - accuracy: 0.4979 - val_loss: 1.5125 - val_accuracy: 0.4074\n",
            "Epoch 167/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.3320 - accuracy: 0.4966 - val_loss: 1.5075 - val_accuracy: 0.4286\n",
            "Epoch 168/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3485 - accuracy: 0.4911 - val_loss: 1.5057 - val_accuracy: 0.4286\n",
            "Epoch 169/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2779 - accuracy: 0.5294 - val_loss: 1.5283 - val_accuracy: 0.4021\n",
            "Epoch 170/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3313 - accuracy: 0.4829 - val_loss: 1.5095 - val_accuracy: 0.4339\n",
            "Epoch 171/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3145 - accuracy: 0.4966 - val_loss: 1.5588 - val_accuracy: 0.4074\n",
            "Epoch 172/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3023 - accuracy: 0.5294 - val_loss: 1.5266 - val_accuracy: 0.4444\n",
            "Epoch 173/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2997 - accuracy: 0.5226 - val_loss: 1.4922 - val_accuracy: 0.4497\n",
            "Epoch 174/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3205 - accuracy: 0.5089 - val_loss: 1.4972 - val_accuracy: 0.3968\n",
            "Epoch 175/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2958 - accuracy: 0.5212 - val_loss: 1.5027 - val_accuracy: 0.4127\n",
            "Epoch 176/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.3149 - accuracy: 0.5185 - val_loss: 1.5036 - val_accuracy: 0.4339\n",
            "Epoch 177/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2984 - accuracy: 0.4952 - val_loss: 1.4835 - val_accuracy: 0.4497\n",
            "Epoch 178/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.3135 - accuracy: 0.5417 - val_loss: 1.5123 - val_accuracy: 0.3810\n",
            "Epoch 179/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2911 - accuracy: 0.5294 - val_loss: 1.4858 - val_accuracy: 0.4286\n",
            "Epoch 180/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3277 - accuracy: 0.4843 - val_loss: 1.5273 - val_accuracy: 0.3968\n",
            "Epoch 181/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2871 - accuracy: 0.5321 - val_loss: 1.5177 - val_accuracy: 0.4021\n",
            "Epoch 182/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2715 - accuracy: 0.5130 - val_loss: 1.4976 - val_accuracy: 0.4233\n",
            "Epoch 183/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.2723 - accuracy: 0.5431 - val_loss: 1.4871 - val_accuracy: 0.4339\n",
            "Epoch 184/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3171 - accuracy: 0.5226 - val_loss: 1.5040 - val_accuracy: 0.4392\n",
            "Epoch 185/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3078 - accuracy: 0.5144 - val_loss: 1.5081 - val_accuracy: 0.4392\n",
            "Epoch 186/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.3006 - accuracy: 0.5198 - val_loss: 1.4926 - val_accuracy: 0.4286\n",
            "Epoch 187/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2861 - accuracy: 0.5321 - val_loss: 1.4947 - val_accuracy: 0.3968\n",
            "Epoch 188/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2605 - accuracy: 0.5308 - val_loss: 1.4897 - val_accuracy: 0.4497\n",
            "Epoch 189/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2498 - accuracy: 0.5472 - val_loss: 1.4954 - val_accuracy: 0.4233\n",
            "Epoch 190/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.2792 - accuracy: 0.5212 - val_loss: 1.4944 - val_accuracy: 0.4444\n",
            "Epoch 191/1009\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.2675 - accuracy: 0.5486 - val_loss: 1.4791 - val_accuracy: 0.4233\n",
            "Epoch 192/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2787 - accuracy: 0.5144 - val_loss: 1.5175 - val_accuracy: 0.4392\n",
            "Epoch 193/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2810 - accuracy: 0.5116 - val_loss: 1.4828 - val_accuracy: 0.4180\n",
            "Epoch 194/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2547 - accuracy: 0.5390 - val_loss: 1.4911 - val_accuracy: 0.4074\n",
            "Epoch 195/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2658 - accuracy: 0.5267 - val_loss: 1.4849 - val_accuracy: 0.4180\n",
            "Epoch 196/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2946 - accuracy: 0.5239 - val_loss: 1.4880 - val_accuracy: 0.4021\n",
            "Epoch 197/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2794 - accuracy: 0.5171 - val_loss: 1.4907 - val_accuracy: 0.4286\n",
            "Epoch 198/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2927 - accuracy: 0.5062 - val_loss: 1.4905 - val_accuracy: 0.4180\n",
            "Epoch 199/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2369 - accuracy: 0.5171 - val_loss: 1.4891 - val_accuracy: 0.4233\n",
            "Epoch 200/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2450 - accuracy: 0.5431 - val_loss: 1.4666 - val_accuracy: 0.4603\n",
            "Epoch 201/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2757 - accuracy: 0.5280 - val_loss: 1.5100 - val_accuracy: 0.4497\n",
            "Epoch 202/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2356 - accuracy: 0.5445 - val_loss: 1.4645 - val_accuracy: 0.4392\n",
            "Epoch 203/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2514 - accuracy: 0.5404 - val_loss: 1.4975 - val_accuracy: 0.4074\n",
            "Epoch 204/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2829 - accuracy: 0.5267 - val_loss: 1.4993 - val_accuracy: 0.4127\n",
            "Epoch 205/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2515 - accuracy: 0.5458 - val_loss: 1.5250 - val_accuracy: 0.4074\n",
            "Epoch 206/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2665 - accuracy: 0.5212 - val_loss: 1.4688 - val_accuracy: 0.4603\n",
            "Epoch 207/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2503 - accuracy: 0.5513 - val_loss: 1.4876 - val_accuracy: 0.4392\n",
            "Epoch 208/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2628 - accuracy: 0.5198 - val_loss: 1.4971 - val_accuracy: 0.4180\n",
            "Epoch 209/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2386 - accuracy: 0.5540 - val_loss: 1.4705 - val_accuracy: 0.4392\n",
            "Epoch 210/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2305 - accuracy: 0.5472 - val_loss: 1.4901 - val_accuracy: 0.4286\n",
            "Epoch 211/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2402 - accuracy: 0.5527 - val_loss: 1.4780 - val_accuracy: 0.4286\n",
            "Epoch 212/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2383 - accuracy: 0.5458 - val_loss: 1.4874 - val_accuracy: 0.4339\n",
            "Epoch 213/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2502 - accuracy: 0.5404 - val_loss: 1.4819 - val_accuracy: 0.4339\n",
            "Epoch 214/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2170 - accuracy: 0.5390 - val_loss: 1.4665 - val_accuracy: 0.4656\n",
            "Epoch 215/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2101 - accuracy: 0.5568 - val_loss: 1.4881 - val_accuracy: 0.4392\n",
            "Epoch 216/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2236 - accuracy: 0.5472 - val_loss: 1.4724 - val_accuracy: 0.4233\n",
            "Epoch 217/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2225 - accuracy: 0.5486 - val_loss: 1.4807 - val_accuracy: 0.4180\n",
            "Epoch 218/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2367 - accuracy: 0.5595 - val_loss: 1.4666 - val_accuracy: 0.4180\n",
            "Epoch 219/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2270 - accuracy: 0.5472 - val_loss: 1.4628 - val_accuracy: 0.4127\n",
            "Epoch 220/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2269 - accuracy: 0.5157 - val_loss: 1.4668 - val_accuracy: 0.4444\n",
            "Epoch 221/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2200 - accuracy: 0.5527 - val_loss: 1.4642 - val_accuracy: 0.4286\n",
            "Epoch 222/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2238 - accuracy: 0.5636 - val_loss: 1.4676 - val_accuracy: 0.4339\n",
            "Epoch 223/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2022 - accuracy: 0.5759 - val_loss: 1.4604 - val_accuracy: 0.4339\n",
            "Epoch 224/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.2063 - accuracy: 0.5472 - val_loss: 1.4540 - val_accuracy: 0.4444\n",
            "Epoch 225/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.1945 - accuracy: 0.5677 - val_loss: 1.4617 - val_accuracy: 0.4180\n",
            "Epoch 226/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2043 - accuracy: 0.5499 - val_loss: 1.4556 - val_accuracy: 0.4233\n",
            "Epoch 227/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.2300 - accuracy: 0.5554 - val_loss: 1.4740 - val_accuracy: 0.4286\n",
            "Epoch 228/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1892 - accuracy: 0.5540 - val_loss: 1.4632 - val_accuracy: 0.4233\n",
            "Epoch 229/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1838 - accuracy: 0.5540 - val_loss: 1.4452 - val_accuracy: 0.4603\n",
            "Epoch 230/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2026 - accuracy: 0.5486 - val_loss: 1.4616 - val_accuracy: 0.4497\n",
            "Epoch 231/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1966 - accuracy: 0.5554 - val_loss: 1.4422 - val_accuracy: 0.4550\n",
            "Epoch 232/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1960 - accuracy: 0.5513 - val_loss: 1.4520 - val_accuracy: 0.4339\n",
            "Epoch 233/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1979 - accuracy: 0.5458 - val_loss: 1.4825 - val_accuracy: 0.4180\n",
            "Epoch 234/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2084 - accuracy: 0.5445 - val_loss: 1.5067 - val_accuracy: 0.3968\n",
            "Epoch 235/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1982 - accuracy: 0.5568 - val_loss: 1.4594 - val_accuracy: 0.4180\n",
            "Epoch 236/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1992 - accuracy: 0.5650 - val_loss: 1.4465 - val_accuracy: 0.4444\n",
            "Epoch 237/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.1986 - accuracy: 0.5554 - val_loss: 1.4424 - val_accuracy: 0.4339\n",
            "Epoch 238/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.1997 - accuracy: 0.5513 - val_loss: 1.4484 - val_accuracy: 0.4286\n",
            "Epoch 239/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1970 - accuracy: 0.5581 - val_loss: 1.4566 - val_accuracy: 0.4392\n",
            "Epoch 240/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1769 - accuracy: 0.5677 - val_loss: 1.4536 - val_accuracy: 0.4392\n",
            "Epoch 241/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1747 - accuracy: 0.5650 - val_loss: 1.4634 - val_accuracy: 0.4497\n",
            "Epoch 242/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.2002 - accuracy: 0.5581 - val_loss: 1.4543 - val_accuracy: 0.4392\n",
            "Epoch 243/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1780 - accuracy: 0.5787 - val_loss: 1.4408 - val_accuracy: 0.4392\n",
            "Epoch 244/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1803 - accuracy: 0.5568 - val_loss: 1.4374 - val_accuracy: 0.4339\n",
            "Epoch 245/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1655 - accuracy: 0.5882 - val_loss: 1.4625 - val_accuracy: 0.4233\n",
            "Epoch 246/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1892 - accuracy: 0.5636 - val_loss: 1.4570 - val_accuracy: 0.4392\n",
            "Epoch 247/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1821 - accuracy: 0.5622 - val_loss: 1.4461 - val_accuracy: 0.4286\n",
            "Epoch 248/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1839 - accuracy: 0.5759 - val_loss: 1.4405 - val_accuracy: 0.4286\n",
            "Epoch 249/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1668 - accuracy: 0.5458 - val_loss: 1.4360 - val_accuracy: 0.4550\n",
            "Epoch 250/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.1865 - accuracy: 0.5663 - val_loss: 1.4447 - val_accuracy: 0.4497\n",
            "Epoch 251/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1703 - accuracy: 0.5622 - val_loss: 1.4810 - val_accuracy: 0.4339\n",
            "Epoch 252/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1812 - accuracy: 0.5636 - val_loss: 1.4647 - val_accuracy: 0.4233\n",
            "Epoch 253/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.1585 - accuracy: 0.5814 - val_loss: 1.4431 - val_accuracy: 0.4339\n",
            "Epoch 254/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1570 - accuracy: 0.5677 - val_loss: 1.4354 - val_accuracy: 0.4233\n",
            "Epoch 255/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1499 - accuracy: 0.5540 - val_loss: 1.4333 - val_accuracy: 0.4444\n",
            "Epoch 256/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1494 - accuracy: 0.5800 - val_loss: 1.4419 - val_accuracy: 0.4550\n",
            "Epoch 257/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1618 - accuracy: 0.5595 - val_loss: 1.4285 - val_accuracy: 0.4497\n",
            "Epoch 258/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1631 - accuracy: 0.5636 - val_loss: 1.4346 - val_accuracy: 0.4444\n",
            "Epoch 259/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1545 - accuracy: 0.5841 - val_loss: 1.4225 - val_accuracy: 0.4550\n",
            "Epoch 260/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1519 - accuracy: 0.5800 - val_loss: 1.4391 - val_accuracy: 0.4339\n",
            "Epoch 261/1009\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 1.1315 - accuracy: 0.5937 - val_loss: 1.4386 - val_accuracy: 0.4286\n",
            "Epoch 262/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1467 - accuracy: 0.5787 - val_loss: 1.4365 - val_accuracy: 0.4444\n",
            "Epoch 263/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1443 - accuracy: 0.5828 - val_loss: 1.4264 - val_accuracy: 0.4444\n",
            "Epoch 264/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1292 - accuracy: 0.5923 - val_loss: 1.4316 - val_accuracy: 0.4550\n",
            "Epoch 265/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1381 - accuracy: 0.5609 - val_loss: 1.4442 - val_accuracy: 0.4444\n",
            "Epoch 266/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1695 - accuracy: 0.5609 - val_loss: 1.4444 - val_accuracy: 0.4444\n",
            "Epoch 267/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.1432 - accuracy: 0.5855 - val_loss: 1.4378 - val_accuracy: 0.4656\n",
            "Epoch 268/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1427 - accuracy: 0.5855 - val_loss: 1.4457 - val_accuracy: 0.4392\n",
            "Epoch 269/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.1572 - accuracy: 0.5910 - val_loss: 1.4609 - val_accuracy: 0.4339\n",
            "Epoch 270/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1364 - accuracy: 0.5677 - val_loss: 1.4360 - val_accuracy: 0.4444\n",
            "Epoch 271/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.1383 - accuracy: 0.5855 - val_loss: 1.4339 - val_accuracy: 0.4286\n",
            "Epoch 272/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1203 - accuracy: 0.5814 - val_loss: 1.4344 - val_accuracy: 0.4339\n",
            "Epoch 273/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1100 - accuracy: 0.5951 - val_loss: 1.4691 - val_accuracy: 0.4286\n",
            "Epoch 274/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1124 - accuracy: 0.5910 - val_loss: 1.4354 - val_accuracy: 0.4339\n",
            "Epoch 275/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.1392 - accuracy: 0.5841 - val_loss: 1.4251 - val_accuracy: 0.4286\n",
            "Epoch 276/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1099 - accuracy: 0.5855 - val_loss: 1.4365 - val_accuracy: 0.4392\n",
            "Epoch 277/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.1333 - accuracy: 0.5882 - val_loss: 1.4261 - val_accuracy: 0.4339\n",
            "Epoch 278/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1302 - accuracy: 0.5787 - val_loss: 1.4486 - val_accuracy: 0.4127\n",
            "Epoch 279/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1328 - accuracy: 0.5773 - val_loss: 1.4339 - val_accuracy: 0.4444\n",
            "Epoch 280/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1290 - accuracy: 0.5896 - val_loss: 1.4165 - val_accuracy: 0.4550\n",
            "Epoch 281/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.1168 - accuracy: 0.6047 - val_loss: 1.4238 - val_accuracy: 0.4444\n",
            "Epoch 282/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1193 - accuracy: 0.5800 - val_loss: 1.4366 - val_accuracy: 0.4286\n",
            "Epoch 283/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1033 - accuracy: 0.6033 - val_loss: 1.4273 - val_accuracy: 0.4444\n",
            "Epoch 284/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.1000 - accuracy: 0.6005 - val_loss: 1.4521 - val_accuracy: 0.4497\n",
            "Epoch 285/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1215 - accuracy: 0.5951 - val_loss: 1.4411 - val_accuracy: 0.4339\n",
            "Epoch 286/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1014 - accuracy: 0.6033 - val_loss: 1.4366 - val_accuracy: 0.4286\n",
            "Epoch 287/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0955 - accuracy: 0.5951 - val_loss: 1.4121 - val_accuracy: 0.4656\n",
            "Epoch 288/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.1120 - accuracy: 0.5773 - val_loss: 1.4411 - val_accuracy: 0.4392\n",
            "Epoch 289/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1077 - accuracy: 0.5937 - val_loss: 1.4606 - val_accuracy: 0.4339\n",
            "Epoch 290/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0914 - accuracy: 0.5923 - val_loss: 1.4233 - val_accuracy: 0.4339\n",
            "Epoch 291/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0993 - accuracy: 0.5759 - val_loss: 1.4055 - val_accuracy: 0.4762\n",
            "Epoch 292/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1162 - accuracy: 0.5882 - val_loss: 1.4681 - val_accuracy: 0.4550\n",
            "Epoch 293/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.1153 - accuracy: 0.6005 - val_loss: 1.4123 - val_accuracy: 0.4656\n",
            "Epoch 294/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0893 - accuracy: 0.5978 - val_loss: 1.4144 - val_accuracy: 0.4656\n",
            "Epoch 295/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0845 - accuracy: 0.6170 - val_loss: 1.4200 - val_accuracy: 0.4444\n",
            "Epoch 296/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0853 - accuracy: 0.6183 - val_loss: 1.4139 - val_accuracy: 0.4444\n",
            "Epoch 297/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.0873 - accuracy: 0.6074 - val_loss: 1.4147 - val_accuracy: 0.4550\n",
            "Epoch 298/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0897 - accuracy: 0.6088 - val_loss: 1.4244 - val_accuracy: 0.4656\n",
            "Epoch 299/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.1040 - accuracy: 0.5677 - val_loss: 1.4052 - val_accuracy: 0.4444\n",
            "Epoch 300/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0937 - accuracy: 0.5937 - val_loss: 1.4586 - val_accuracy: 0.4444\n",
            "Epoch 301/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0990 - accuracy: 0.6005 - val_loss: 1.4092 - val_accuracy: 0.4550\n",
            "Epoch 302/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0700 - accuracy: 0.6156 - val_loss: 1.4932 - val_accuracy: 0.4127\n",
            "Epoch 303/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0891 - accuracy: 0.5937 - val_loss: 1.4005 - val_accuracy: 0.4392\n",
            "Epoch 304/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0891 - accuracy: 0.5992 - val_loss: 1.4028 - val_accuracy: 0.4339\n",
            "Epoch 305/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0845 - accuracy: 0.5964 - val_loss: 1.4170 - val_accuracy: 0.4497\n",
            "Epoch 306/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0747 - accuracy: 0.6019 - val_loss: 1.4406 - val_accuracy: 0.4603\n",
            "Epoch 307/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.0838 - accuracy: 0.5978 - val_loss: 1.4232 - val_accuracy: 0.4286\n",
            "Epoch 308/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0570 - accuracy: 0.6252 - val_loss: 1.4106 - val_accuracy: 0.4444\n",
            "Epoch 309/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0812 - accuracy: 0.5951 - val_loss: 1.4139 - val_accuracy: 0.4603\n",
            "Epoch 310/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0482 - accuracy: 0.6197 - val_loss: 1.4295 - val_accuracy: 0.4497\n",
            "Epoch 311/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0434 - accuracy: 0.6183 - val_loss: 1.4478 - val_accuracy: 0.4286\n",
            "Epoch 312/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 1.0621 - accuracy: 0.6183 - val_loss: 1.4249 - val_accuracy: 0.4233\n",
            "Epoch 313/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0531 - accuracy: 0.6293 - val_loss: 1.4077 - val_accuracy: 0.4497\n",
            "Epoch 314/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.0509 - accuracy: 0.6005 - val_loss: 1.4046 - val_accuracy: 0.4339\n",
            "Epoch 315/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0421 - accuracy: 0.6183 - val_loss: 1.4065 - val_accuracy: 0.4392\n",
            "Epoch 316/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0747 - accuracy: 0.6074 - val_loss: 1.3968 - val_accuracy: 0.4339\n",
            "Epoch 317/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0511 - accuracy: 0.6129 - val_loss: 1.4026 - val_accuracy: 0.4286\n",
            "Epoch 318/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0636 - accuracy: 0.6088 - val_loss: 1.4032 - val_accuracy: 0.4286\n",
            "Epoch 319/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.0481 - accuracy: 0.6265 - val_loss: 1.4252 - val_accuracy: 0.4444\n",
            "Epoch 320/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0386 - accuracy: 0.6457 - val_loss: 1.4197 - val_accuracy: 0.4497\n",
            "Epoch 321/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0120 - accuracy: 0.6334 - val_loss: 1.4029 - val_accuracy: 0.4339\n",
            "Epoch 322/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0496 - accuracy: 0.6129 - val_loss: 1.3952 - val_accuracy: 0.4497\n",
            "Epoch 323/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.0490 - accuracy: 0.6170 - val_loss: 1.4261 - val_accuracy: 0.4392\n",
            "Epoch 324/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0486 - accuracy: 0.6170 - val_loss: 1.4300 - val_accuracy: 0.4286\n",
            "Epoch 325/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0471 - accuracy: 0.5951 - val_loss: 1.4502 - val_accuracy: 0.4392\n",
            "Epoch 326/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0380 - accuracy: 0.6252 - val_loss: 1.4146 - val_accuracy: 0.4392\n",
            "Epoch 327/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0525 - accuracy: 0.6088 - val_loss: 1.3884 - val_accuracy: 0.4550\n",
            "Epoch 328/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0251 - accuracy: 0.6293 - val_loss: 1.4138 - val_accuracy: 0.4180\n",
            "Epoch 329/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0518 - accuracy: 0.6156 - val_loss: 1.3904 - val_accuracy: 0.4550\n",
            "Epoch 330/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.0380 - accuracy: 0.6183 - val_loss: 1.4153 - val_accuracy: 0.4392\n",
            "Epoch 331/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0254 - accuracy: 0.6347 - val_loss: 1.4141 - val_accuracy: 0.4392\n",
            "Epoch 332/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0183 - accuracy: 0.6156 - val_loss: 1.3974 - val_accuracy: 0.4392\n",
            "Epoch 333/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0189 - accuracy: 0.6306 - val_loss: 1.3975 - val_accuracy: 0.4444\n",
            "Epoch 334/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.0293 - accuracy: 0.6361 - val_loss: 1.4164 - val_accuracy: 0.4127\n",
            "Epoch 335/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.0146 - accuracy: 0.6375 - val_loss: 1.3976 - val_accuracy: 0.4550\n",
            "Epoch 336/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0205 - accuracy: 0.6224 - val_loss: 1.3956 - val_accuracy: 0.4497\n",
            "Epoch 337/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0179 - accuracy: 0.6320 - val_loss: 1.3863 - val_accuracy: 0.4497\n",
            "Epoch 338/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 1.0343 - accuracy: 0.6306 - val_loss: 1.4118 - val_accuracy: 0.4497\n",
            "Epoch 339/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0252 - accuracy: 0.6183 - val_loss: 1.4117 - val_accuracy: 0.4286\n",
            "Epoch 340/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0117 - accuracy: 0.6334 - val_loss: 1.3862 - val_accuracy: 0.4392\n",
            "Epoch 341/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.0261 - accuracy: 0.6265 - val_loss: 1.3877 - val_accuracy: 0.4603\n",
            "Epoch 342/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0103 - accuracy: 0.6389 - val_loss: 1.4145 - val_accuracy: 0.4180\n",
            "Epoch 343/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0163 - accuracy: 0.6430 - val_loss: 1.3978 - val_accuracy: 0.4444\n",
            "Epoch 344/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0059 - accuracy: 0.6361 - val_loss: 1.4146 - val_accuracy: 0.4550\n",
            "Epoch 345/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0113 - accuracy: 0.6334 - val_loss: 1.3927 - val_accuracy: 0.4656\n",
            "Epoch 346/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0100 - accuracy: 0.6197 - val_loss: 1.4491 - val_accuracy: 0.4127\n",
            "Epoch 347/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 1.0042 - accuracy: 0.6416 - val_loss: 1.4196 - val_accuracy: 0.4392\n",
            "Epoch 348/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9948 - accuracy: 0.6347 - val_loss: 1.4151 - val_accuracy: 0.4392\n",
            "Epoch 349/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9959 - accuracy: 0.6389 - val_loss: 1.4099 - val_accuracy: 0.4392\n",
            "Epoch 350/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0001 - accuracy: 0.6443 - val_loss: 1.4028 - val_accuracy: 0.4603\n",
            "Epoch 351/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.0150 - accuracy: 0.6375 - val_loss: 1.3989 - val_accuracy: 0.4497\n",
            "Epoch 352/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 1.0064 - accuracy: 0.6361 - val_loss: 1.4188 - val_accuracy: 0.4180\n",
            "Epoch 353/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9987 - accuracy: 0.6361 - val_loss: 1.3977 - val_accuracy: 0.4497\n",
            "Epoch 354/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.9805 - accuracy: 0.6416 - val_loss: 1.3937 - val_accuracy: 0.4497\n",
            "Epoch 355/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.9879 - accuracy: 0.6361 - val_loss: 1.3900 - val_accuracy: 0.4444\n",
            "Epoch 356/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0089 - accuracy: 0.6265 - val_loss: 1.4018 - val_accuracy: 0.4497\n",
            "Epoch 357/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9987 - accuracy: 0.6238 - val_loss: 1.3959 - val_accuracy: 0.4550\n",
            "Epoch 358/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9887 - accuracy: 0.6457 - val_loss: 1.4147 - val_accuracy: 0.4286\n",
            "Epoch 359/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0129 - accuracy: 0.6224 - val_loss: 1.3967 - val_accuracy: 0.4497\n",
            "Epoch 360/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9667 - accuracy: 0.6580 - val_loss: 1.3983 - val_accuracy: 0.4709\n",
            "Epoch 361/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9875 - accuracy: 0.6471 - val_loss: 1.3962 - val_accuracy: 0.4603\n",
            "Epoch 362/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9845 - accuracy: 0.6621 - val_loss: 1.3843 - val_accuracy: 0.4709\n",
            "Epoch 363/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9984 - accuracy: 0.6142 - val_loss: 1.4057 - val_accuracy: 0.4444\n",
            "Epoch 364/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9967 - accuracy: 0.6293 - val_loss: 1.4023 - val_accuracy: 0.4286\n",
            "Epoch 365/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9675 - accuracy: 0.6375 - val_loss: 1.3859 - val_accuracy: 0.4762\n",
            "Epoch 366/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.9975 - accuracy: 0.6471 - val_loss: 1.3862 - val_accuracy: 0.4444\n",
            "Epoch 367/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9625 - accuracy: 0.6484 - val_loss: 1.3995 - val_accuracy: 0.4550\n",
            "Epoch 368/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9636 - accuracy: 0.6648 - val_loss: 1.4191 - val_accuracy: 0.4392\n",
            "Epoch 369/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9811 - accuracy: 0.6484 - val_loss: 1.3839 - val_accuracy: 0.4762\n",
            "Epoch 370/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9866 - accuracy: 0.6320 - val_loss: 1.4028 - val_accuracy: 0.4233\n",
            "Epoch 371/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9529 - accuracy: 0.6471 - val_loss: 1.4160 - val_accuracy: 0.4074\n",
            "Epoch 372/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9716 - accuracy: 0.6457 - val_loss: 1.3973 - val_accuracy: 0.4444\n",
            "Epoch 373/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9915 - accuracy: 0.6265 - val_loss: 1.3935 - val_accuracy: 0.4339\n",
            "Epoch 374/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9569 - accuracy: 0.6512 - val_loss: 1.3923 - val_accuracy: 0.4550\n",
            "Epoch 375/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.9664 - accuracy: 0.6402 - val_loss: 1.3972 - val_accuracy: 0.4392\n",
            "Epoch 376/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9862 - accuracy: 0.6375 - val_loss: 1.4204 - val_accuracy: 0.4286\n",
            "Epoch 377/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9395 - accuracy: 0.6662 - val_loss: 1.4213 - val_accuracy: 0.4286\n",
            "Epoch 378/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9609 - accuracy: 0.6443 - val_loss: 1.4163 - val_accuracy: 0.4180\n",
            "Epoch 379/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9617 - accuracy: 0.6539 - val_loss: 1.3850 - val_accuracy: 0.4497\n",
            "Epoch 380/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9627 - accuracy: 0.6553 - val_loss: 1.3837 - val_accuracy: 0.4762\n",
            "Epoch 381/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.9450 - accuracy: 0.6498 - val_loss: 1.3936 - val_accuracy: 0.4286\n",
            "Epoch 382/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.9651 - accuracy: 0.6484 - val_loss: 1.3851 - val_accuracy: 0.4550\n",
            "Epoch 383/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9843 - accuracy: 0.6402 - val_loss: 1.3970 - val_accuracy: 0.4497\n",
            "Epoch 384/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9420 - accuracy: 0.6772 - val_loss: 1.3910 - val_accuracy: 0.4762\n",
            "Epoch 385/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9656 - accuracy: 0.6443 - val_loss: 1.3951 - val_accuracy: 0.4550\n",
            "Epoch 386/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9129 - accuracy: 0.6731 - val_loss: 1.3815 - val_accuracy: 0.4656\n",
            "Epoch 387/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9296 - accuracy: 0.6566 - val_loss: 1.4179 - val_accuracy: 0.4444\n",
            "Epoch 388/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9215 - accuracy: 0.6635 - val_loss: 1.3922 - val_accuracy: 0.4603\n",
            "Epoch 389/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9608 - accuracy: 0.6430 - val_loss: 1.3861 - val_accuracy: 0.4709\n",
            "Epoch 390/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.9292 - accuracy: 0.6758 - val_loss: 1.3802 - val_accuracy: 0.4392\n",
            "Epoch 391/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9354 - accuracy: 0.6635 - val_loss: 1.3851 - val_accuracy: 0.4497\n",
            "Epoch 392/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9260 - accuracy: 0.6607 - val_loss: 1.3877 - val_accuracy: 0.4709\n",
            "Epoch 393/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.9192 - accuracy: 0.6703 - val_loss: 1.3809 - val_accuracy: 0.4497\n",
            "Epoch 394/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9560 - accuracy: 0.6566 - val_loss: 1.3815 - val_accuracy: 0.4603\n",
            "Epoch 395/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9376 - accuracy: 0.6676 - val_loss: 1.3736 - val_accuracy: 0.4550\n",
            "Epoch 396/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9292 - accuracy: 0.6744 - val_loss: 1.3888 - val_accuracy: 0.4497\n",
            "Epoch 397/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9305 - accuracy: 0.6471 - val_loss: 1.3790 - val_accuracy: 0.4709\n",
            "Epoch 398/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8982 - accuracy: 0.6731 - val_loss: 1.4176 - val_accuracy: 0.4233\n",
            "Epoch 399/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9132 - accuracy: 0.6799 - val_loss: 1.3861 - val_accuracy: 0.4815\n",
            "Epoch 400/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9371 - accuracy: 0.6553 - val_loss: 1.4015 - val_accuracy: 0.4762\n",
            "Epoch 401/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9270 - accuracy: 0.6443 - val_loss: 1.3895 - val_accuracy: 0.4339\n",
            "Epoch 402/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9037 - accuracy: 0.6717 - val_loss: 1.3860 - val_accuracy: 0.4550\n",
            "Epoch 403/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9218 - accuracy: 0.6662 - val_loss: 1.4476 - val_accuracy: 0.4550\n",
            "Epoch 404/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.9147 - accuracy: 0.6772 - val_loss: 1.4044 - val_accuracy: 0.4815\n",
            "Epoch 405/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9084 - accuracy: 0.6689 - val_loss: 1.3948 - val_accuracy: 0.4709\n",
            "Epoch 406/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9189 - accuracy: 0.6744 - val_loss: 1.3871 - val_accuracy: 0.4656\n",
            "Epoch 407/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.9145 - accuracy: 0.6881 - val_loss: 1.4063 - val_accuracy: 0.4550\n",
            "Epoch 408/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9193 - accuracy: 0.6895 - val_loss: 1.4250 - val_accuracy: 0.4444\n",
            "Epoch 409/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9317 - accuracy: 0.6635 - val_loss: 1.3778 - val_accuracy: 0.4762\n",
            "Epoch 410/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8973 - accuracy: 0.6813 - val_loss: 1.3753 - val_accuracy: 0.4815\n",
            "Epoch 411/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9124 - accuracy: 0.6539 - val_loss: 1.3920 - val_accuracy: 0.4762\n",
            "Epoch 412/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9180 - accuracy: 0.6758 - val_loss: 1.4349 - val_accuracy: 0.4233\n",
            "Epoch 413/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.9112 - accuracy: 0.6689 - val_loss: 1.3697 - val_accuracy: 0.4709\n",
            "Epoch 414/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8972 - accuracy: 0.6744 - val_loss: 1.3688 - val_accuracy: 0.4709\n",
            "Epoch 415/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8929 - accuracy: 0.6922 - val_loss: 1.3782 - val_accuracy: 0.4656\n",
            "Epoch 416/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8567 - accuracy: 0.7114 - val_loss: 1.3805 - val_accuracy: 0.4550\n",
            "Epoch 417/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8804 - accuracy: 0.6854 - val_loss: 1.4174 - val_accuracy: 0.4444\n",
            "Epoch 418/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8807 - accuracy: 0.6758 - val_loss: 1.3960 - val_accuracy: 0.4656\n",
            "Epoch 419/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8660 - accuracy: 0.6936 - val_loss: 1.4444 - val_accuracy: 0.4180\n",
            "Epoch 420/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.9076 - accuracy: 0.6689 - val_loss: 1.3861 - val_accuracy: 0.4603\n",
            "Epoch 421/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.8726 - accuracy: 0.6908 - val_loss: 1.4060 - val_accuracy: 0.4603\n",
            "Epoch 422/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8803 - accuracy: 0.7045 - val_loss: 1.4165 - val_accuracy: 0.4339\n",
            "Epoch 423/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8770 - accuracy: 0.6867 - val_loss: 1.4138 - val_accuracy: 0.4550\n",
            "Epoch 424/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8852 - accuracy: 0.6758 - val_loss: 1.3999 - val_accuracy: 0.4550\n",
            "Epoch 425/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8910 - accuracy: 0.6731 - val_loss: 1.4063 - val_accuracy: 0.4444\n",
            "Epoch 426/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8665 - accuracy: 0.6881 - val_loss: 1.3868 - val_accuracy: 0.4550\n",
            "Epoch 427/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8863 - accuracy: 0.6758 - val_loss: 1.4036 - val_accuracy: 0.4762\n",
            "Epoch 428/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8914 - accuracy: 0.6484 - val_loss: 1.3894 - val_accuracy: 0.4603\n",
            "Epoch 429/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8772 - accuracy: 0.6758 - val_loss: 1.4043 - val_accuracy: 0.4286\n",
            "Epoch 430/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.8720 - accuracy: 0.6867 - val_loss: 1.3755 - val_accuracy: 0.4762\n",
            "Epoch 431/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.8628 - accuracy: 0.7073 - val_loss: 1.4106 - val_accuracy: 0.4444\n",
            "Epoch 432/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.8632 - accuracy: 0.7059 - val_loss: 1.3862 - val_accuracy: 0.4656\n",
            "Epoch 433/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8563 - accuracy: 0.7100 - val_loss: 1.3781 - val_accuracy: 0.4603\n",
            "Epoch 434/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8651 - accuracy: 0.6772 - val_loss: 1.3900 - val_accuracy: 0.4656\n",
            "Epoch 435/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8492 - accuracy: 0.7114 - val_loss: 1.3944 - val_accuracy: 0.4550\n",
            "Epoch 436/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.8739 - accuracy: 0.6854 - val_loss: 1.3705 - val_accuracy: 0.4709\n",
            "Epoch 437/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8914 - accuracy: 0.6744 - val_loss: 1.3860 - val_accuracy: 0.4709\n",
            "Epoch 438/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8655 - accuracy: 0.7045 - val_loss: 1.4173 - val_accuracy: 0.4339\n",
            "Epoch 439/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8687 - accuracy: 0.7086 - val_loss: 1.3807 - val_accuracy: 0.4868\n",
            "Epoch 440/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.8548 - accuracy: 0.6990 - val_loss: 1.3934 - val_accuracy: 0.4709\n",
            "Epoch 441/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8529 - accuracy: 0.7004 - val_loss: 1.4236 - val_accuracy: 0.4286\n",
            "Epoch 442/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8711 - accuracy: 0.6813 - val_loss: 1.3743 - val_accuracy: 0.4815\n",
            "Epoch 443/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8381 - accuracy: 0.7018 - val_loss: 1.3892 - val_accuracy: 0.4603\n",
            "Epoch 444/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8491 - accuracy: 0.7045 - val_loss: 1.3699 - val_accuracy: 0.4815\n",
            "Epoch 445/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8146 - accuracy: 0.7141 - val_loss: 1.3976 - val_accuracy: 0.4497\n",
            "Epoch 446/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.8302 - accuracy: 0.7018 - val_loss: 1.3845 - val_accuracy: 0.4656\n",
            "Epoch 447/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8400 - accuracy: 0.6908 - val_loss: 1.3987 - val_accuracy: 0.4709\n",
            "Epoch 448/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8578 - accuracy: 0.6867 - val_loss: 1.4037 - val_accuracy: 0.4603\n",
            "Epoch 449/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8338 - accuracy: 0.7031 - val_loss: 1.3851 - val_accuracy: 0.4656\n",
            "Epoch 450/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8208 - accuracy: 0.7031 - val_loss: 1.3775 - val_accuracy: 0.4603\n",
            "Epoch 451/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8464 - accuracy: 0.6990 - val_loss: 1.3988 - val_accuracy: 0.4444\n",
            "Epoch 452/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8374 - accuracy: 0.7168 - val_loss: 1.3823 - val_accuracy: 0.4868\n",
            "Epoch 453/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.8199 - accuracy: 0.7073 - val_loss: 1.3893 - val_accuracy: 0.4709\n",
            "Epoch 454/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8566 - accuracy: 0.7018 - val_loss: 1.4037 - val_accuracy: 0.4444\n",
            "Epoch 455/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.8472 - accuracy: 0.6922 - val_loss: 1.3845 - val_accuracy: 0.4815\n",
            "Epoch 456/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8139 - accuracy: 0.7086 - val_loss: 1.3728 - val_accuracy: 0.4921\n",
            "Epoch 457/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8392 - accuracy: 0.7100 - val_loss: 1.3707 - val_accuracy: 0.4868\n",
            "Epoch 458/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8218 - accuracy: 0.7086 - val_loss: 1.4365 - val_accuracy: 0.4550\n",
            "Epoch 459/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8171 - accuracy: 0.7196 - val_loss: 1.3713 - val_accuracy: 0.4815\n",
            "Epoch 460/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8444 - accuracy: 0.6936 - val_loss: 1.3867 - val_accuracy: 0.4815\n",
            "Epoch 461/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8402 - accuracy: 0.6949 - val_loss: 1.3799 - val_accuracy: 0.4709\n",
            "Epoch 462/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8132 - accuracy: 0.7250 - val_loss: 1.3904 - val_accuracy: 0.4656\n",
            "Epoch 463/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.8022 - accuracy: 0.6881 - val_loss: 1.3877 - val_accuracy: 0.4603\n",
            "Epoch 464/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8034 - accuracy: 0.7196 - val_loss: 1.3936 - val_accuracy: 0.4603\n",
            "Epoch 465/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8363 - accuracy: 0.7237 - val_loss: 1.3949 - val_accuracy: 0.4709\n",
            "Epoch 466/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.8046 - accuracy: 0.7155 - val_loss: 1.3808 - val_accuracy: 0.4868\n",
            "Epoch 467/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8048 - accuracy: 0.7196 - val_loss: 1.4078 - val_accuracy: 0.4603\n",
            "Epoch 468/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8093 - accuracy: 0.7073 - val_loss: 1.3830 - val_accuracy: 0.4656\n",
            "Epoch 469/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8116 - accuracy: 0.7155 - val_loss: 1.3786 - val_accuracy: 0.4815\n",
            "Epoch 470/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8107 - accuracy: 0.7155 - val_loss: 1.3822 - val_accuracy: 0.4815\n",
            "Epoch 471/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8014 - accuracy: 0.7209 - val_loss: 1.3807 - val_accuracy: 0.4974\n",
            "Epoch 472/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8331 - accuracy: 0.7086 - val_loss: 1.3835 - val_accuracy: 0.4656\n",
            "Epoch 473/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8145 - accuracy: 0.6908 - val_loss: 1.3933 - val_accuracy: 0.4497\n",
            "Epoch 474/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7923 - accuracy: 0.7237 - val_loss: 1.4039 - val_accuracy: 0.4603\n",
            "Epoch 475/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7858 - accuracy: 0.7237 - val_loss: 1.3805 - val_accuracy: 0.4815\n",
            "Epoch 476/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7959 - accuracy: 0.7182 - val_loss: 1.4356 - val_accuracy: 0.4603\n",
            "Epoch 477/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7827 - accuracy: 0.7346 - val_loss: 1.3950 - val_accuracy: 0.4921\n",
            "Epoch 478/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8005 - accuracy: 0.7127 - val_loss: 1.3870 - val_accuracy: 0.4762\n",
            "Epoch 479/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.7805 - accuracy: 0.7182 - val_loss: 1.4038 - val_accuracy: 0.4603\n",
            "Epoch 480/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.8006 - accuracy: 0.7332 - val_loss: 1.3838 - val_accuracy: 0.4815\n",
            "Epoch 481/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7986 - accuracy: 0.7196 - val_loss: 1.3942 - val_accuracy: 0.4550\n",
            "Epoch 482/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7900 - accuracy: 0.7059 - val_loss: 1.3905 - val_accuracy: 0.4921\n",
            "Epoch 483/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7735 - accuracy: 0.7278 - val_loss: 1.3856 - val_accuracy: 0.4868\n",
            "Epoch 484/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7915 - accuracy: 0.7100 - val_loss: 1.3899 - val_accuracy: 0.4868\n",
            "Epoch 485/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.7737 - accuracy: 0.7346 - val_loss: 1.3836 - val_accuracy: 0.4921\n",
            "Epoch 486/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7829 - accuracy: 0.7127 - val_loss: 1.4024 - val_accuracy: 0.4603\n",
            "Epoch 487/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7727 - accuracy: 0.7168 - val_loss: 1.3831 - val_accuracy: 0.4762\n",
            "Epoch 488/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.7910 - accuracy: 0.7141 - val_loss: 1.4213 - val_accuracy: 0.4392\n",
            "Epoch 489/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7786 - accuracy: 0.6977 - val_loss: 1.3819 - val_accuracy: 0.4815\n",
            "Epoch 490/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.8060 - accuracy: 0.6922 - val_loss: 1.4011 - val_accuracy: 0.4497\n",
            "Epoch 491/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7791 - accuracy: 0.7250 - val_loss: 1.3675 - val_accuracy: 0.4868\n",
            "Epoch 492/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7641 - accuracy: 0.7428 - val_loss: 1.3930 - val_accuracy: 0.4709\n",
            "Epoch 493/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7548 - accuracy: 0.7401 - val_loss: 1.4355 - val_accuracy: 0.4550\n",
            "Epoch 494/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7673 - accuracy: 0.7428 - val_loss: 1.3916 - val_accuracy: 0.4815\n",
            "Epoch 495/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7559 - accuracy: 0.7305 - val_loss: 1.3787 - val_accuracy: 0.4868\n",
            "Epoch 496/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7608 - accuracy: 0.7291 - val_loss: 1.4162 - val_accuracy: 0.4497\n",
            "Epoch 497/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7590 - accuracy: 0.7250 - val_loss: 1.4453 - val_accuracy: 0.4497\n",
            "Epoch 498/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7736 - accuracy: 0.7196 - val_loss: 1.3986 - val_accuracy: 0.4709\n",
            "Epoch 499/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7413 - accuracy: 0.7524 - val_loss: 1.3826 - val_accuracy: 0.4709\n",
            "Epoch 500/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7574 - accuracy: 0.7332 - val_loss: 1.3928 - val_accuracy: 0.4656\n",
            "Epoch 501/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7651 - accuracy: 0.7332 - val_loss: 1.4058 - val_accuracy: 0.4709\n",
            "Epoch 502/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7444 - accuracy: 0.7415 - val_loss: 1.3960 - val_accuracy: 0.4709\n",
            "Epoch 503/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7264 - accuracy: 0.7524 - val_loss: 1.3842 - val_accuracy: 0.4550\n",
            "Epoch 504/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7356 - accuracy: 0.7579 - val_loss: 1.3830 - val_accuracy: 0.4974\n",
            "Epoch 505/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7522 - accuracy: 0.7291 - val_loss: 1.4168 - val_accuracy: 0.4762\n",
            "Epoch 506/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7450 - accuracy: 0.7373 - val_loss: 1.3794 - val_accuracy: 0.4921\n",
            "Epoch 507/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.7573 - accuracy: 0.7332 - val_loss: 1.3961 - val_accuracy: 0.4762\n",
            "Epoch 508/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7460 - accuracy: 0.7373 - val_loss: 1.3869 - val_accuracy: 0.4868\n",
            "Epoch 509/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7335 - accuracy: 0.7620 - val_loss: 1.3816 - val_accuracy: 0.4921\n",
            "Epoch 510/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7439 - accuracy: 0.7415 - val_loss: 1.3963 - val_accuracy: 0.4921\n",
            "Epoch 511/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7153 - accuracy: 0.7456 - val_loss: 1.4111 - val_accuracy: 0.4709\n",
            "Epoch 512/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7430 - accuracy: 0.7415 - val_loss: 1.4136 - val_accuracy: 0.4656\n",
            "Epoch 513/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7528 - accuracy: 0.7428 - val_loss: 1.4195 - val_accuracy: 0.4392\n",
            "Epoch 514/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7084 - accuracy: 0.7620 - val_loss: 1.4052 - val_accuracy: 0.4550\n",
            "Epoch 515/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7280 - accuracy: 0.7401 - val_loss: 1.3871 - val_accuracy: 0.4815\n",
            "Epoch 516/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7326 - accuracy: 0.7387 - val_loss: 1.4036 - val_accuracy: 0.4815\n",
            "Epoch 517/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.7491 - accuracy: 0.7237 - val_loss: 1.4035 - val_accuracy: 0.4656\n",
            "Epoch 518/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.7336 - accuracy: 0.7346 - val_loss: 1.4025 - val_accuracy: 0.4497\n",
            "Epoch 519/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7275 - accuracy: 0.7510 - val_loss: 1.4033 - val_accuracy: 0.4762\n",
            "Epoch 520/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7261 - accuracy: 0.7469 - val_loss: 1.3907 - val_accuracy: 0.4709\n",
            "Epoch 521/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.7181 - accuracy: 0.7469 - val_loss: 1.3975 - val_accuracy: 0.4762\n",
            "Epoch 522/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7153 - accuracy: 0.7702 - val_loss: 1.4206 - val_accuracy: 0.4550\n",
            "Epoch 523/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.7425 - accuracy: 0.7250 - val_loss: 1.4246 - val_accuracy: 0.4656\n",
            "Epoch 524/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.6945 - accuracy: 0.7551 - val_loss: 1.4105 - val_accuracy: 0.4709\n",
            "Epoch 525/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.7138 - accuracy: 0.7633 - val_loss: 1.3932 - val_accuracy: 0.4709\n",
            "Epoch 526/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6982 - accuracy: 0.7551 - val_loss: 1.4163 - val_accuracy: 0.4868\n",
            "Epoch 527/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7093 - accuracy: 0.7579 - val_loss: 1.4520 - val_accuracy: 0.4392\n",
            "Epoch 528/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7084 - accuracy: 0.7592 - val_loss: 1.4016 - val_accuracy: 0.4762\n",
            "Epoch 529/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7130 - accuracy: 0.7592 - val_loss: 1.4039 - val_accuracy: 0.4868\n",
            "Epoch 530/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.7205 - accuracy: 0.7524 - val_loss: 1.4087 - val_accuracy: 0.4709\n",
            "Epoch 531/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6920 - accuracy: 0.7688 - val_loss: 1.4005 - val_accuracy: 0.4762\n",
            "Epoch 532/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7026 - accuracy: 0.7456 - val_loss: 1.4034 - val_accuracy: 0.4709\n",
            "Epoch 533/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.7059 - accuracy: 0.7483 - val_loss: 1.4074 - val_accuracy: 0.4815\n",
            "Epoch 534/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.7046 - accuracy: 0.7606 - val_loss: 1.4118 - val_accuracy: 0.4815\n",
            "Epoch 535/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7031 - accuracy: 0.7483 - val_loss: 1.4125 - val_accuracy: 0.4815\n",
            "Epoch 536/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7054 - accuracy: 0.7620 - val_loss: 1.4137 - val_accuracy: 0.4550\n",
            "Epoch 537/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6916 - accuracy: 0.7606 - val_loss: 1.3945 - val_accuracy: 0.4868\n",
            "Epoch 538/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7201 - accuracy: 0.7538 - val_loss: 1.4021 - val_accuracy: 0.4868\n",
            "Epoch 539/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6925 - accuracy: 0.7729 - val_loss: 1.4107 - val_accuracy: 0.4656\n",
            "Epoch 540/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6992 - accuracy: 0.7661 - val_loss: 1.3998 - val_accuracy: 0.4868\n",
            "Epoch 541/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.7092 - accuracy: 0.7592 - val_loss: 1.4684 - val_accuracy: 0.4550\n",
            "Epoch 542/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6954 - accuracy: 0.7524 - val_loss: 1.4393 - val_accuracy: 0.4656\n",
            "Epoch 543/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.6815 - accuracy: 0.7839 - val_loss: 1.4374 - val_accuracy: 0.4603\n",
            "Epoch 544/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6934 - accuracy: 0.7743 - val_loss: 1.4363 - val_accuracy: 0.4392\n",
            "Epoch 545/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7130 - accuracy: 0.7456 - val_loss: 1.4108 - val_accuracy: 0.4656\n",
            "Epoch 546/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.6776 - accuracy: 0.7674 - val_loss: 1.4247 - val_accuracy: 0.4656\n",
            "Epoch 547/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.6984 - accuracy: 0.7688 - val_loss: 1.4281 - val_accuracy: 0.4603\n",
            "Epoch 548/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6839 - accuracy: 0.7606 - val_loss: 1.4173 - val_accuracy: 0.4815\n",
            "Epoch 549/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7117 - accuracy: 0.7346 - val_loss: 1.4419 - val_accuracy: 0.4762\n",
            "Epoch 550/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6937 - accuracy: 0.7620 - val_loss: 1.3967 - val_accuracy: 0.4709\n",
            "Epoch 551/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6814 - accuracy: 0.7647 - val_loss: 1.4110 - val_accuracy: 0.4709\n",
            "Epoch 552/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.6919 - accuracy: 0.7743 - val_loss: 1.4227 - val_accuracy: 0.4709\n",
            "Epoch 553/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.7041 - accuracy: 0.7551 - val_loss: 1.4226 - val_accuracy: 0.4709\n",
            "Epoch 554/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.6647 - accuracy: 0.7661 - val_loss: 1.3935 - val_accuracy: 0.4868\n",
            "Epoch 555/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.6827 - accuracy: 0.7647 - val_loss: 1.4075 - val_accuracy: 0.4815\n",
            "Epoch 556/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6735 - accuracy: 0.7839 - val_loss: 1.3989 - val_accuracy: 0.4868\n",
            "Epoch 557/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6709 - accuracy: 0.7688 - val_loss: 1.4333 - val_accuracy: 0.4709\n",
            "Epoch 558/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6680 - accuracy: 0.7606 - val_loss: 1.4362 - val_accuracy: 0.4815\n",
            "Epoch 559/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6634 - accuracy: 0.7743 - val_loss: 1.4269 - val_accuracy: 0.4709\n",
            "Epoch 560/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6704 - accuracy: 0.7729 - val_loss: 1.4009 - val_accuracy: 0.4762\n",
            "Epoch 561/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6426 - accuracy: 0.7784 - val_loss: 1.4077 - val_accuracy: 0.4815\n",
            "Epoch 562/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.6717 - accuracy: 0.7606 - val_loss: 1.4047 - val_accuracy: 0.4815\n",
            "Epoch 563/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6794 - accuracy: 0.7702 - val_loss: 1.4046 - val_accuracy: 0.4762\n",
            "Epoch 564/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6413 - accuracy: 0.7893 - val_loss: 1.4138 - val_accuracy: 0.4656\n",
            "Epoch 565/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6495 - accuracy: 0.7839 - val_loss: 1.4159 - val_accuracy: 0.4762\n",
            "Epoch 566/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6668 - accuracy: 0.7647 - val_loss: 1.4412 - val_accuracy: 0.4550\n",
            "Epoch 567/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6611 - accuracy: 0.7756 - val_loss: 1.4230 - val_accuracy: 0.4762\n",
            "Epoch 568/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6503 - accuracy: 0.7784 - val_loss: 1.4076 - val_accuracy: 0.4709\n",
            "Epoch 569/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6500 - accuracy: 0.7592 - val_loss: 1.4337 - val_accuracy: 0.4656\n",
            "Epoch 570/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6655 - accuracy: 0.7880 - val_loss: 1.4031 - val_accuracy: 0.4709\n",
            "Epoch 571/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6325 - accuracy: 0.7811 - val_loss: 1.4122 - val_accuracy: 0.4603\n",
            "Epoch 572/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6525 - accuracy: 0.7770 - val_loss: 1.4429 - val_accuracy: 0.4603\n",
            "Epoch 573/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6327 - accuracy: 0.7852 - val_loss: 1.4219 - val_accuracy: 0.4603\n",
            "Epoch 574/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6614 - accuracy: 0.7743 - val_loss: 1.4179 - val_accuracy: 0.4921\n",
            "Epoch 575/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6361 - accuracy: 0.7715 - val_loss: 1.4142 - val_accuracy: 0.4709\n",
            "Epoch 576/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6260 - accuracy: 0.7839 - val_loss: 1.4458 - val_accuracy: 0.4709\n",
            "Epoch 577/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6584 - accuracy: 0.7756 - val_loss: 1.4272 - val_accuracy: 0.4656\n",
            "Epoch 578/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6276 - accuracy: 0.7907 - val_loss: 1.4319 - val_accuracy: 0.4709\n",
            "Epoch 579/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6330 - accuracy: 0.7880 - val_loss: 1.4264 - val_accuracy: 0.4762\n",
            "Epoch 580/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6445 - accuracy: 0.7756 - val_loss: 1.4180 - val_accuracy: 0.4550\n",
            "Epoch 581/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.6279 - accuracy: 0.7743 - val_loss: 1.4125 - val_accuracy: 0.4868\n",
            "Epoch 582/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6448 - accuracy: 0.7770 - val_loss: 1.4251 - val_accuracy: 0.4762\n",
            "Epoch 583/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6324 - accuracy: 0.7866 - val_loss: 1.4154 - val_accuracy: 0.4709\n",
            "Epoch 584/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6316 - accuracy: 0.7798 - val_loss: 1.4560 - val_accuracy: 0.4339\n",
            "Epoch 585/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6296 - accuracy: 0.7893 - val_loss: 1.4146 - val_accuracy: 0.4762\n",
            "Epoch 586/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6022 - accuracy: 0.8153 - val_loss: 1.5258 - val_accuracy: 0.4921\n",
            "Epoch 587/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6238 - accuracy: 0.7921 - val_loss: 1.4411 - val_accuracy: 0.4550\n",
            "Epoch 588/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6285 - accuracy: 0.7934 - val_loss: 1.4653 - val_accuracy: 0.4762\n",
            "Epoch 589/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6216 - accuracy: 0.7866 - val_loss: 1.4684 - val_accuracy: 0.4709\n",
            "Epoch 590/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6298 - accuracy: 0.7839 - val_loss: 1.4082 - val_accuracy: 0.4815\n",
            "Epoch 591/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6276 - accuracy: 0.7852 - val_loss: 1.4314 - val_accuracy: 0.4709\n",
            "Epoch 592/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6123 - accuracy: 0.7907 - val_loss: 1.4610 - val_accuracy: 0.4815\n",
            "Epoch 593/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6418 - accuracy: 0.7839 - val_loss: 1.4712 - val_accuracy: 0.4868\n",
            "Epoch 594/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6042 - accuracy: 0.8126 - val_loss: 1.4451 - val_accuracy: 0.4603\n",
            "Epoch 595/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6151 - accuracy: 0.7839 - val_loss: 1.4265 - val_accuracy: 0.4656\n",
            "Epoch 596/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6046 - accuracy: 0.8071 - val_loss: 1.4730 - val_accuracy: 0.4656\n",
            "Epoch 597/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.6276 - accuracy: 0.7715 - val_loss: 1.4276 - val_accuracy: 0.4762\n",
            "Epoch 598/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5841 - accuracy: 0.8057 - val_loss: 1.4283 - val_accuracy: 0.4815\n",
            "Epoch 599/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5842 - accuracy: 0.8030 - val_loss: 1.4281 - val_accuracy: 0.4762\n",
            "Epoch 600/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5982 - accuracy: 0.7880 - val_loss: 1.4650 - val_accuracy: 0.4550\n",
            "Epoch 601/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6192 - accuracy: 0.8057 - val_loss: 1.4537 - val_accuracy: 0.4656\n",
            "Epoch 602/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5938 - accuracy: 0.7825 - val_loss: 1.4431 - val_accuracy: 0.4815\n",
            "Epoch 603/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.6024 - accuracy: 0.7839 - val_loss: 1.4179 - val_accuracy: 0.4709\n",
            "Epoch 604/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6120 - accuracy: 0.8057 - val_loss: 1.4267 - val_accuracy: 0.4709\n",
            "Epoch 605/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6198 - accuracy: 0.7934 - val_loss: 1.4419 - val_accuracy: 0.4815\n",
            "Epoch 606/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5745 - accuracy: 0.8057 - val_loss: 1.4377 - val_accuracy: 0.4709\n",
            "Epoch 607/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5712 - accuracy: 0.8112 - val_loss: 1.4463 - val_accuracy: 0.4709\n",
            "Epoch 608/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5892 - accuracy: 0.7975 - val_loss: 1.4400 - val_accuracy: 0.4709\n",
            "Epoch 609/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6013 - accuracy: 0.8003 - val_loss: 1.4912 - val_accuracy: 0.4709\n",
            "Epoch 610/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6046 - accuracy: 0.7962 - val_loss: 1.4446 - val_accuracy: 0.4868\n",
            "Epoch 611/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5694 - accuracy: 0.8235 - val_loss: 1.4365 - val_accuracy: 0.4762\n",
            "Epoch 612/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5694 - accuracy: 0.8085 - val_loss: 1.4446 - val_accuracy: 0.4709\n",
            "Epoch 613/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5945 - accuracy: 0.8071 - val_loss: 1.4381 - val_accuracy: 0.4868\n",
            "Epoch 614/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5908 - accuracy: 0.8030 - val_loss: 1.4401 - val_accuracy: 0.4762\n",
            "Epoch 615/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6097 - accuracy: 0.7852 - val_loss: 1.4327 - val_accuracy: 0.4709\n",
            "Epoch 616/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5708 - accuracy: 0.8085 - val_loss: 1.4607 - val_accuracy: 0.4815\n",
            "Epoch 617/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5836 - accuracy: 0.7989 - val_loss: 1.4355 - val_accuracy: 0.4815\n",
            "Epoch 618/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5726 - accuracy: 0.8085 - val_loss: 1.4383 - val_accuracy: 0.4762\n",
            "Epoch 619/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.5684 - accuracy: 0.8167 - val_loss: 1.4208 - val_accuracy: 0.4656\n",
            "Epoch 620/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.6125 - accuracy: 0.7839 - val_loss: 1.4551 - val_accuracy: 0.4762\n",
            "Epoch 621/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5855 - accuracy: 0.8044 - val_loss: 1.4348 - val_accuracy: 0.4921\n",
            "Epoch 622/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5867 - accuracy: 0.7989 - val_loss: 1.4398 - val_accuracy: 0.4815\n",
            "Epoch 623/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5737 - accuracy: 0.8085 - val_loss: 1.4329 - val_accuracy: 0.4815\n",
            "Epoch 624/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5815 - accuracy: 0.8003 - val_loss: 1.4798 - val_accuracy: 0.4762\n",
            "Epoch 625/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5793 - accuracy: 0.8016 - val_loss: 1.4450 - val_accuracy: 0.4709\n",
            "Epoch 626/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.5867 - accuracy: 0.8071 - val_loss: 1.4551 - val_accuracy: 0.4762\n",
            "Epoch 627/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5696 - accuracy: 0.8126 - val_loss: 1.5006 - val_accuracy: 0.4815\n",
            "Epoch 628/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5692 - accuracy: 0.8071 - val_loss: 1.4463 - val_accuracy: 0.4709\n",
            "Epoch 629/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5722 - accuracy: 0.8057 - val_loss: 1.4324 - val_accuracy: 0.4762\n",
            "Epoch 630/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5504 - accuracy: 0.8235 - val_loss: 1.4350 - val_accuracy: 0.4868\n",
            "Epoch 631/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5744 - accuracy: 0.7948 - val_loss: 1.4611 - val_accuracy: 0.4762\n",
            "Epoch 632/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5677 - accuracy: 0.8140 - val_loss: 1.4796 - val_accuracy: 0.4762\n",
            "Epoch 633/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5723 - accuracy: 0.8140 - val_loss: 1.4725 - val_accuracy: 0.4550\n",
            "Epoch 634/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5772 - accuracy: 0.8003 - val_loss: 1.4860 - val_accuracy: 0.4603\n",
            "Epoch 635/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5682 - accuracy: 0.8016 - val_loss: 1.4962 - val_accuracy: 0.4656\n",
            "Epoch 636/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5591 - accuracy: 0.8140 - val_loss: 1.4585 - val_accuracy: 0.4656\n",
            "Epoch 637/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5454 - accuracy: 0.8194 - val_loss: 1.4565 - val_accuracy: 0.4392\n",
            "Epoch 638/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5493 - accuracy: 0.8263 - val_loss: 1.4531 - val_accuracy: 0.4815\n",
            "Epoch 639/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5559 - accuracy: 0.8235 - val_loss: 1.4684 - val_accuracy: 0.4762\n",
            "Epoch 640/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5449 - accuracy: 0.8235 - val_loss: 1.4624 - val_accuracy: 0.4709\n",
            "Epoch 641/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5594 - accuracy: 0.8126 - val_loss: 1.4627 - val_accuracy: 0.4762\n",
            "Epoch 642/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5526 - accuracy: 0.8140 - val_loss: 1.4850 - val_accuracy: 0.4709\n",
            "Epoch 643/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5440 - accuracy: 0.8222 - val_loss: 1.4748 - val_accuracy: 0.4603\n",
            "Epoch 644/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5413 - accuracy: 0.8181 - val_loss: 1.4483 - val_accuracy: 0.4762\n",
            "Epoch 645/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5225 - accuracy: 0.8304 - val_loss: 1.4736 - val_accuracy: 0.4762\n",
            "Epoch 646/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5391 - accuracy: 0.8276 - val_loss: 1.5323 - val_accuracy: 0.4656\n",
            "Epoch 647/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5573 - accuracy: 0.8057 - val_loss: 1.4668 - val_accuracy: 0.4762\n",
            "Epoch 648/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5447 - accuracy: 0.7989 - val_loss: 1.4640 - val_accuracy: 0.4815\n",
            "Epoch 649/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5445 - accuracy: 0.8167 - val_loss: 1.4509 - val_accuracy: 0.4762\n",
            "Epoch 650/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.5211 - accuracy: 0.8358 - val_loss: 1.4494 - val_accuracy: 0.4921\n",
            "Epoch 651/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5489 - accuracy: 0.8098 - val_loss: 1.4613 - val_accuracy: 0.4709\n",
            "Epoch 652/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5495 - accuracy: 0.8126 - val_loss: 1.4905 - val_accuracy: 0.4656\n",
            "Epoch 653/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5491 - accuracy: 0.8030 - val_loss: 1.4867 - val_accuracy: 0.4550\n",
            "Epoch 654/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5327 - accuracy: 0.8345 - val_loss: 1.4931 - val_accuracy: 0.4974\n",
            "Epoch 655/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5318 - accuracy: 0.8413 - val_loss: 1.4678 - val_accuracy: 0.4815\n",
            "Epoch 656/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5213 - accuracy: 0.8167 - val_loss: 1.4646 - val_accuracy: 0.4815\n",
            "Epoch 657/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5058 - accuracy: 0.8399 - val_loss: 1.4721 - val_accuracy: 0.4709\n",
            "Epoch 658/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5261 - accuracy: 0.8372 - val_loss: 1.4520 - val_accuracy: 0.4868\n",
            "Epoch 659/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5114 - accuracy: 0.8331 - val_loss: 1.4772 - val_accuracy: 0.4656\n",
            "Epoch 660/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.5100 - accuracy: 0.8386 - val_loss: 1.4531 - val_accuracy: 0.4974\n",
            "Epoch 661/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5224 - accuracy: 0.8386 - val_loss: 1.4806 - val_accuracy: 0.4709\n",
            "Epoch 662/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5162 - accuracy: 0.8304 - val_loss: 1.4757 - val_accuracy: 0.4762\n",
            "Epoch 663/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5139 - accuracy: 0.8276 - val_loss: 1.4766 - val_accuracy: 0.4709\n",
            "Epoch 664/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.5204 - accuracy: 0.8290 - val_loss: 1.4763 - val_accuracy: 0.4603\n",
            "Epoch 665/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4922 - accuracy: 0.8427 - val_loss: 1.4812 - val_accuracy: 0.4709\n",
            "Epoch 666/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5224 - accuracy: 0.8249 - val_loss: 1.4818 - val_accuracy: 0.4603\n",
            "Epoch 667/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5070 - accuracy: 0.8290 - val_loss: 1.4791 - val_accuracy: 0.4762\n",
            "Epoch 668/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5018 - accuracy: 0.8331 - val_loss: 1.4723 - val_accuracy: 0.4815\n",
            "Epoch 669/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4983 - accuracy: 0.8304 - val_loss: 1.4757 - val_accuracy: 0.4868\n",
            "Epoch 670/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5161 - accuracy: 0.8153 - val_loss: 1.4855 - val_accuracy: 0.4709\n",
            "Epoch 671/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4973 - accuracy: 0.8454 - val_loss: 1.4947 - val_accuracy: 0.4656\n",
            "Epoch 672/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5132 - accuracy: 0.8263 - val_loss: 1.5118 - val_accuracy: 0.4868\n",
            "Epoch 673/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4891 - accuracy: 0.8399 - val_loss: 1.4796 - val_accuracy: 0.4762\n",
            "Epoch 674/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4881 - accuracy: 0.8358 - val_loss: 1.5075 - val_accuracy: 0.4709\n",
            "Epoch 675/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5044 - accuracy: 0.8386 - val_loss: 1.4695 - val_accuracy: 0.4815\n",
            "Epoch 676/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4908 - accuracy: 0.8454 - val_loss: 1.4662 - val_accuracy: 0.4815\n",
            "Epoch 677/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5066 - accuracy: 0.8304 - val_loss: 1.4718 - val_accuracy: 0.4762\n",
            "Epoch 678/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4674 - accuracy: 0.8577 - val_loss: 1.4664 - val_accuracy: 0.4921\n",
            "Epoch 679/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4926 - accuracy: 0.8454 - val_loss: 1.5018 - val_accuracy: 0.4921\n",
            "Epoch 680/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.5000 - accuracy: 0.8372 - val_loss: 1.5042 - val_accuracy: 0.4921\n",
            "Epoch 681/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4975 - accuracy: 0.8331 - val_loss: 1.4687 - val_accuracy: 0.4921\n",
            "Epoch 682/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4759 - accuracy: 0.8495 - val_loss: 1.4748 - val_accuracy: 0.4921\n",
            "Epoch 683/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.5012 - accuracy: 0.8345 - val_loss: 1.4786 - val_accuracy: 0.4921\n",
            "Epoch 684/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4939 - accuracy: 0.8386 - val_loss: 1.4832 - val_accuracy: 0.4815\n",
            "Epoch 685/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4759 - accuracy: 0.8482 - val_loss: 1.4832 - val_accuracy: 0.4868\n",
            "Epoch 686/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4793 - accuracy: 0.8468 - val_loss: 1.4996 - val_accuracy: 0.4550\n",
            "Epoch 687/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4771 - accuracy: 0.8468 - val_loss: 1.4961 - val_accuracy: 0.4868\n",
            "Epoch 688/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4977 - accuracy: 0.8290 - val_loss: 1.5526 - val_accuracy: 0.4497\n",
            "Epoch 689/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4666 - accuracy: 0.8509 - val_loss: 1.5179 - val_accuracy: 0.4762\n",
            "Epoch 690/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4909 - accuracy: 0.8386 - val_loss: 1.4772 - val_accuracy: 0.4762\n",
            "Epoch 691/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4869 - accuracy: 0.8427 - val_loss: 1.4736 - val_accuracy: 0.4868\n",
            "Epoch 692/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4800 - accuracy: 0.8331 - val_loss: 1.4994 - val_accuracy: 0.4709\n",
            "Epoch 693/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4737 - accuracy: 0.8564 - val_loss: 1.4984 - val_accuracy: 0.4921\n",
            "Epoch 694/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4743 - accuracy: 0.8427 - val_loss: 1.4711 - val_accuracy: 0.4868\n",
            "Epoch 695/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4793 - accuracy: 0.8495 - val_loss: 1.4888 - val_accuracy: 0.4709\n",
            "Epoch 696/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4811 - accuracy: 0.8290 - val_loss: 1.5175 - val_accuracy: 0.4497\n",
            "Epoch 697/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4653 - accuracy: 0.8550 - val_loss: 1.5105 - val_accuracy: 0.4868\n",
            "Epoch 698/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4827 - accuracy: 0.8399 - val_loss: 1.5233 - val_accuracy: 0.4497\n",
            "Epoch 699/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4656 - accuracy: 0.8427 - val_loss: 1.4997 - val_accuracy: 0.4921\n",
            "Epoch 700/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4457 - accuracy: 0.8618 - val_loss: 1.5020 - val_accuracy: 0.4709\n",
            "Epoch 701/1009\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.4665 - accuracy: 0.8386 - val_loss: 1.4984 - val_accuracy: 0.4762\n",
            "Epoch 702/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4739 - accuracy: 0.8427 - val_loss: 1.4897 - val_accuracy: 0.4815\n",
            "Epoch 703/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4486 - accuracy: 0.8564 - val_loss: 1.5769 - val_accuracy: 0.4497\n",
            "Epoch 704/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4803 - accuracy: 0.8249 - val_loss: 1.4998 - val_accuracy: 0.4974\n",
            "Epoch 705/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4729 - accuracy: 0.8413 - val_loss: 1.5144 - val_accuracy: 0.4656\n",
            "Epoch 706/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4655 - accuracy: 0.8440 - val_loss: 1.4959 - val_accuracy: 0.4762\n",
            "Epoch 707/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4611 - accuracy: 0.8495 - val_loss: 1.5001 - val_accuracy: 0.4762\n",
            "Epoch 708/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4537 - accuracy: 0.8495 - val_loss: 1.5505 - val_accuracy: 0.4656\n",
            "Epoch 709/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4643 - accuracy: 0.8454 - val_loss: 1.5259 - val_accuracy: 0.4656\n",
            "Epoch 710/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4460 - accuracy: 0.8495 - val_loss: 1.5165 - val_accuracy: 0.4762\n",
            "Epoch 711/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4621 - accuracy: 0.8550 - val_loss: 1.5024 - val_accuracy: 0.4868\n",
            "Epoch 712/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4410 - accuracy: 0.8728 - val_loss: 1.5407 - val_accuracy: 0.4868\n",
            "Epoch 713/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4478 - accuracy: 0.8550 - val_loss: 1.5124 - val_accuracy: 0.4974\n",
            "Epoch 714/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4493 - accuracy: 0.8536 - val_loss: 1.5484 - val_accuracy: 0.4815\n",
            "Epoch 715/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4375 - accuracy: 0.8591 - val_loss: 1.5218 - val_accuracy: 0.4550\n",
            "Epoch 716/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4112 - accuracy: 0.8782 - val_loss: 1.5439 - val_accuracy: 0.4762\n",
            "Epoch 717/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4441 - accuracy: 0.8523 - val_loss: 1.5825 - val_accuracy: 0.4868\n",
            "Epoch 718/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4469 - accuracy: 0.8523 - val_loss: 1.4936 - val_accuracy: 0.4815\n",
            "Epoch 719/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4430 - accuracy: 0.8577 - val_loss: 1.5708 - val_accuracy: 0.4709\n",
            "Epoch 720/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4367 - accuracy: 0.8659 - val_loss: 1.5328 - val_accuracy: 0.4974\n",
            "Epoch 721/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4492 - accuracy: 0.8550 - val_loss: 1.5038 - val_accuracy: 0.4762\n",
            "Epoch 722/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4335 - accuracy: 0.8687 - val_loss: 1.5148 - val_accuracy: 0.4656\n",
            "Epoch 723/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4416 - accuracy: 0.8673 - val_loss: 1.5117 - val_accuracy: 0.4921\n",
            "Epoch 724/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4311 - accuracy: 0.8659 - val_loss: 1.4997 - val_accuracy: 0.4709\n",
            "Epoch 725/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.4411 - accuracy: 0.8536 - val_loss: 1.5288 - val_accuracy: 0.4709\n",
            "Epoch 726/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4400 - accuracy: 0.8591 - val_loss: 1.5349 - val_accuracy: 0.4550\n",
            "Epoch 727/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4498 - accuracy: 0.8523 - val_loss: 1.5036 - val_accuracy: 0.4709\n",
            "Epoch 728/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4220 - accuracy: 0.8755 - val_loss: 1.5398 - val_accuracy: 0.4815\n",
            "Epoch 729/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4314 - accuracy: 0.8482 - val_loss: 1.5315 - val_accuracy: 0.4550\n",
            "Epoch 730/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4280 - accuracy: 0.8646 - val_loss: 1.5325 - val_accuracy: 0.4921\n",
            "Epoch 731/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4338 - accuracy: 0.8605 - val_loss: 1.5117 - val_accuracy: 0.4815\n",
            "Epoch 732/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4199 - accuracy: 0.8728 - val_loss: 1.5023 - val_accuracy: 0.4921\n",
            "Epoch 733/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4231 - accuracy: 0.8564 - val_loss: 1.5891 - val_accuracy: 0.4762\n",
            "Epoch 734/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4010 - accuracy: 0.8769 - val_loss: 1.5207 - val_accuracy: 0.4921\n",
            "Epoch 735/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4057 - accuracy: 0.8659 - val_loss: 1.5342 - val_accuracy: 0.5079\n",
            "Epoch 736/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4113 - accuracy: 0.8687 - val_loss: 1.5039 - val_accuracy: 0.4921\n",
            "Epoch 737/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4118 - accuracy: 0.8646 - val_loss: 1.5520 - val_accuracy: 0.4709\n",
            "Epoch 738/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4218 - accuracy: 0.8605 - val_loss: 1.5444 - val_accuracy: 0.4815\n",
            "Epoch 739/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.4141 - accuracy: 0.8536 - val_loss: 1.5540 - val_accuracy: 0.4762\n",
            "Epoch 740/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4142 - accuracy: 0.8700 - val_loss: 1.5342 - val_accuracy: 0.4709\n",
            "Epoch 741/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.4142 - accuracy: 0.8714 - val_loss: 1.5443 - val_accuracy: 0.4921\n",
            "Epoch 742/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3839 - accuracy: 0.8741 - val_loss: 1.5044 - val_accuracy: 0.4974\n",
            "Epoch 743/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4078 - accuracy: 0.8741 - val_loss: 1.5741 - val_accuracy: 0.5132\n",
            "Epoch 744/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4103 - accuracy: 0.8605 - val_loss: 1.5951 - val_accuracy: 0.4815\n",
            "Epoch 745/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4167 - accuracy: 0.8755 - val_loss: 1.5467 - val_accuracy: 0.4974\n",
            "Epoch 746/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4019 - accuracy: 0.8700 - val_loss: 1.5699 - val_accuracy: 0.4815\n",
            "Epoch 747/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3772 - accuracy: 0.8782 - val_loss: 1.5324 - val_accuracy: 0.4974\n",
            "Epoch 748/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3980 - accuracy: 0.8796 - val_loss: 1.5271 - val_accuracy: 0.4921\n",
            "Epoch 749/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4145 - accuracy: 0.8728 - val_loss: 1.5623 - val_accuracy: 0.4603\n",
            "Epoch 750/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3964 - accuracy: 0.8837 - val_loss: 1.5245 - val_accuracy: 0.4921\n",
            "Epoch 751/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4141 - accuracy: 0.8700 - val_loss: 1.5521 - val_accuracy: 0.4815\n",
            "Epoch 752/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3890 - accuracy: 0.8755 - val_loss: 1.5422 - val_accuracy: 0.4550\n",
            "Epoch 753/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4026 - accuracy: 0.8810 - val_loss: 1.6065 - val_accuracy: 0.4921\n",
            "Epoch 754/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4088 - accuracy: 0.8605 - val_loss: 1.5610 - val_accuracy: 0.4921\n",
            "Epoch 755/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.4069 - accuracy: 0.8741 - val_loss: 1.5363 - val_accuracy: 0.4815\n",
            "Epoch 756/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3912 - accuracy: 0.8728 - val_loss: 1.5362 - val_accuracy: 0.4868\n",
            "Epoch 757/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3731 - accuracy: 0.8810 - val_loss: 1.6068 - val_accuracy: 0.4603\n",
            "Epoch 758/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3965 - accuracy: 0.8714 - val_loss: 1.5932 - val_accuracy: 0.4868\n",
            "Epoch 759/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4090 - accuracy: 0.8687 - val_loss: 1.5637 - val_accuracy: 0.4656\n",
            "Epoch 760/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.4008 - accuracy: 0.8700 - val_loss: 1.5655 - val_accuracy: 0.4709\n",
            "Epoch 761/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3914 - accuracy: 0.8810 - val_loss: 1.5826 - val_accuracy: 0.4815\n",
            "Epoch 762/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4006 - accuracy: 0.8700 - val_loss: 1.5375 - val_accuracy: 0.5079\n",
            "Epoch 763/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3878 - accuracy: 0.8728 - val_loss: 1.5516 - val_accuracy: 0.4550\n",
            "Epoch 764/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3697 - accuracy: 0.8947 - val_loss: 1.5628 - val_accuracy: 0.4656\n",
            "Epoch 765/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4141 - accuracy: 0.8673 - val_loss: 1.5702 - val_accuracy: 0.4815\n",
            "Epoch 766/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3714 - accuracy: 0.8892 - val_loss: 1.5628 - val_accuracy: 0.4815\n",
            "Epoch 767/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3912 - accuracy: 0.8755 - val_loss: 1.5494 - val_accuracy: 0.4550\n",
            "Epoch 768/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3908 - accuracy: 0.8824 - val_loss: 1.5554 - val_accuracy: 0.5026\n",
            "Epoch 769/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3787 - accuracy: 0.8892 - val_loss: 1.5590 - val_accuracy: 0.4762\n",
            "Epoch 770/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3532 - accuracy: 0.8878 - val_loss: 1.6023 - val_accuracy: 0.4815\n",
            "Epoch 771/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3736 - accuracy: 0.8878 - val_loss: 1.5601 - val_accuracy: 0.4868\n",
            "Epoch 772/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3720 - accuracy: 0.8837 - val_loss: 1.5727 - val_accuracy: 0.4656\n",
            "Epoch 773/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3685 - accuracy: 0.8892 - val_loss: 1.5988 - val_accuracy: 0.4762\n",
            "Epoch 774/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3764 - accuracy: 0.8824 - val_loss: 1.5947 - val_accuracy: 0.4921\n",
            "Epoch 775/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3793 - accuracy: 0.8865 - val_loss: 1.5807 - val_accuracy: 0.4921\n",
            "Epoch 776/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3850 - accuracy: 0.8810 - val_loss: 1.5949 - val_accuracy: 0.4921\n",
            "Epoch 777/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3827 - accuracy: 0.8824 - val_loss: 1.6288 - val_accuracy: 0.4762\n",
            "Epoch 778/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3706 - accuracy: 0.8892 - val_loss: 1.5996 - val_accuracy: 0.5132\n",
            "Epoch 779/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3612 - accuracy: 0.8810 - val_loss: 1.5839 - val_accuracy: 0.4656\n",
            "Epoch 780/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3503 - accuracy: 0.9001 - val_loss: 1.5969 - val_accuracy: 0.5132\n",
            "Epoch 781/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3611 - accuracy: 0.8878 - val_loss: 1.5878 - val_accuracy: 0.4974\n",
            "Epoch 782/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3380 - accuracy: 0.8974 - val_loss: 1.5938 - val_accuracy: 0.4550\n",
            "Epoch 783/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3503 - accuracy: 0.8714 - val_loss: 1.5559 - val_accuracy: 0.4974\n",
            "Epoch 784/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3621 - accuracy: 0.8974 - val_loss: 1.5833 - val_accuracy: 0.4868\n",
            "Epoch 785/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3436 - accuracy: 0.8988 - val_loss: 1.5702 - val_accuracy: 0.4762\n",
            "Epoch 786/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3517 - accuracy: 0.8878 - val_loss: 1.5829 - val_accuracy: 0.4709\n",
            "Epoch 787/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3601 - accuracy: 0.8824 - val_loss: 1.5763 - val_accuracy: 0.4656\n",
            "Epoch 788/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3481 - accuracy: 0.8906 - val_loss: 1.5682 - val_accuracy: 0.4603\n",
            "Epoch 789/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3659 - accuracy: 0.8824 - val_loss: 1.6201 - val_accuracy: 0.4550\n",
            "Epoch 790/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3556 - accuracy: 0.8906 - val_loss: 1.6011 - val_accuracy: 0.4762\n",
            "Epoch 791/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3516 - accuracy: 0.9083 - val_loss: 1.6312 - val_accuracy: 0.4603\n",
            "Epoch 792/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3274 - accuracy: 0.9083 - val_loss: 1.5942 - val_accuracy: 0.4921\n",
            "Epoch 793/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3549 - accuracy: 0.8782 - val_loss: 1.5945 - val_accuracy: 0.4762\n",
            "Epoch 794/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3693 - accuracy: 0.8837 - val_loss: 1.5840 - val_accuracy: 0.4762\n",
            "Epoch 795/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3620 - accuracy: 0.8632 - val_loss: 1.5727 - val_accuracy: 0.4815\n",
            "Epoch 796/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3382 - accuracy: 0.9056 - val_loss: 1.5667 - val_accuracy: 0.4762\n",
            "Epoch 797/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3500 - accuracy: 0.8974 - val_loss: 1.6148 - val_accuracy: 0.4550\n",
            "Epoch 798/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3256 - accuracy: 0.8919 - val_loss: 1.5716 - val_accuracy: 0.4815\n",
            "Epoch 799/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3274 - accuracy: 0.8974 - val_loss: 1.5966 - val_accuracy: 0.4709\n",
            "Epoch 800/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3252 - accuracy: 0.9056 - val_loss: 1.6045 - val_accuracy: 0.4815\n",
            "Epoch 801/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3208 - accuracy: 0.9097 - val_loss: 1.6015 - val_accuracy: 0.5026\n",
            "Epoch 802/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3326 - accuracy: 0.8933 - val_loss: 1.6227 - val_accuracy: 0.4550\n",
            "Epoch 803/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3366 - accuracy: 0.8947 - val_loss: 1.6713 - val_accuracy: 0.4974\n",
            "Epoch 804/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3237 - accuracy: 0.9097 - val_loss: 1.5807 - val_accuracy: 0.4868\n",
            "Epoch 805/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3444 - accuracy: 0.8892 - val_loss: 1.5967 - val_accuracy: 0.4868\n",
            "Epoch 806/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3360 - accuracy: 0.8892 - val_loss: 1.5938 - val_accuracy: 0.4762\n",
            "Epoch 807/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3172 - accuracy: 0.9097 - val_loss: 1.5902 - val_accuracy: 0.4921\n",
            "Epoch 808/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3271 - accuracy: 0.8919 - val_loss: 1.5973 - val_accuracy: 0.4921\n",
            "Epoch 809/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3290 - accuracy: 0.9070 - val_loss: 1.6159 - val_accuracy: 0.4921\n",
            "Epoch 810/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3389 - accuracy: 0.8810 - val_loss: 1.5995 - val_accuracy: 0.4709\n",
            "Epoch 811/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.3180 - accuracy: 0.9001 - val_loss: 1.6496 - val_accuracy: 0.5238\n",
            "Epoch 812/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3202 - accuracy: 0.9070 - val_loss: 1.6147 - val_accuracy: 0.4656\n",
            "Epoch 813/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3328 - accuracy: 0.9001 - val_loss: 1.6424 - val_accuracy: 0.5132\n",
            "Epoch 814/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3508 - accuracy: 0.8878 - val_loss: 1.6032 - val_accuracy: 0.4656\n",
            "Epoch 815/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3477 - accuracy: 0.8810 - val_loss: 1.6462 - val_accuracy: 0.4603\n",
            "Epoch 816/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3102 - accuracy: 0.9124 - val_loss: 1.6125 - val_accuracy: 0.4815\n",
            "Epoch 817/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3241 - accuracy: 0.9083 - val_loss: 1.6036 - val_accuracy: 0.4656\n",
            "Epoch 818/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3284 - accuracy: 0.9097 - val_loss: 1.6053 - val_accuracy: 0.4550\n",
            "Epoch 819/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3107 - accuracy: 0.9179 - val_loss: 1.6134 - val_accuracy: 0.4974\n",
            "Epoch 820/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3176 - accuracy: 0.9015 - val_loss: 1.6176 - val_accuracy: 0.4921\n",
            "Epoch 821/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3052 - accuracy: 0.9001 - val_loss: 1.6859 - val_accuracy: 0.5026\n",
            "Epoch 822/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3053 - accuracy: 0.9042 - val_loss: 1.6141 - val_accuracy: 0.4815\n",
            "Epoch 823/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3066 - accuracy: 0.9124 - val_loss: 1.6765 - val_accuracy: 0.4762\n",
            "Epoch 824/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3225 - accuracy: 0.8974 - val_loss: 1.5982 - val_accuracy: 0.4921\n",
            "Epoch 825/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3127 - accuracy: 0.8988 - val_loss: 1.6313 - val_accuracy: 0.4656\n",
            "Epoch 826/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2988 - accuracy: 0.9001 - val_loss: 1.6254 - val_accuracy: 0.4603\n",
            "Epoch 827/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3112 - accuracy: 0.9124 - val_loss: 1.6245 - val_accuracy: 0.4762\n",
            "Epoch 828/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.3234 - accuracy: 0.8933 - val_loss: 1.6312 - val_accuracy: 0.5026\n",
            "Epoch 829/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3116 - accuracy: 0.9029 - val_loss: 1.6415 - val_accuracy: 0.4550\n",
            "Epoch 830/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2929 - accuracy: 0.9234 - val_loss: 1.6141 - val_accuracy: 0.4444\n",
            "Epoch 831/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3078 - accuracy: 0.9138 - val_loss: 1.6398 - val_accuracy: 0.4762\n",
            "Epoch 832/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.3096 - accuracy: 0.8919 - val_loss: 1.6520 - val_accuracy: 0.4974\n",
            "Epoch 833/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3087 - accuracy: 0.9015 - val_loss: 1.6332 - val_accuracy: 0.4762\n",
            "Epoch 834/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3111 - accuracy: 0.9124 - val_loss: 1.6591 - val_accuracy: 0.4974\n",
            "Epoch 835/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2914 - accuracy: 0.9138 - val_loss: 1.6481 - val_accuracy: 0.4709\n",
            "Epoch 836/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.3018 - accuracy: 0.9056 - val_loss: 1.6289 - val_accuracy: 0.4815\n",
            "Epoch 837/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3037 - accuracy: 0.9097 - val_loss: 1.6523 - val_accuracy: 0.4762\n",
            "Epoch 838/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.3010 - accuracy: 0.9056 - val_loss: 1.6273 - val_accuracy: 0.4762\n",
            "Epoch 839/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2975 - accuracy: 0.9111 - val_loss: 1.6469 - val_accuracy: 0.4762\n",
            "Epoch 840/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2801 - accuracy: 0.9248 - val_loss: 1.6508 - val_accuracy: 0.4921\n",
            "Epoch 841/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2949 - accuracy: 0.9083 - val_loss: 1.6661 - val_accuracy: 0.4815\n",
            "Epoch 842/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2998 - accuracy: 0.9056 - val_loss: 1.6361 - val_accuracy: 0.4603\n",
            "Epoch 843/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2922 - accuracy: 0.9083 - val_loss: 1.6908 - val_accuracy: 0.4656\n",
            "Epoch 844/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2929 - accuracy: 0.9083 - val_loss: 1.6654 - val_accuracy: 0.4762\n",
            "Epoch 845/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2832 - accuracy: 0.9179 - val_loss: 1.6549 - val_accuracy: 0.4656\n",
            "Epoch 846/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2868 - accuracy: 0.9097 - val_loss: 1.6431 - val_accuracy: 0.4974\n",
            "Epoch 847/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2841 - accuracy: 0.9207 - val_loss: 1.6672 - val_accuracy: 0.4815\n",
            "Epoch 848/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2783 - accuracy: 0.9193 - val_loss: 1.6881 - val_accuracy: 0.4868\n",
            "Epoch 849/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2859 - accuracy: 0.9248 - val_loss: 1.6707 - val_accuracy: 0.4603\n",
            "Epoch 850/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2761 - accuracy: 0.9179 - val_loss: 1.6573 - val_accuracy: 0.5079\n",
            "Epoch 851/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2754 - accuracy: 0.9234 - val_loss: 1.7111 - val_accuracy: 0.4868\n",
            "Epoch 852/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2617 - accuracy: 0.9207 - val_loss: 1.7358 - val_accuracy: 0.5079\n",
            "Epoch 853/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2726 - accuracy: 0.9220 - val_loss: 1.6854 - val_accuracy: 0.4815\n",
            "Epoch 854/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2729 - accuracy: 0.9083 - val_loss: 1.6688 - val_accuracy: 0.4815\n",
            "Epoch 855/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2823 - accuracy: 0.9207 - val_loss: 1.7111 - val_accuracy: 0.4762\n",
            "Epoch 856/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2858 - accuracy: 0.9124 - val_loss: 1.6961 - val_accuracy: 0.4762\n",
            "Epoch 857/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2826 - accuracy: 0.9083 - val_loss: 1.7148 - val_accuracy: 0.4868\n",
            "Epoch 858/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2838 - accuracy: 0.9179 - val_loss: 1.6738 - val_accuracy: 0.4709\n",
            "Epoch 859/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2720 - accuracy: 0.9220 - val_loss: 1.6701 - val_accuracy: 0.4868\n",
            "Epoch 860/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2655 - accuracy: 0.9261 - val_loss: 1.7417 - val_accuracy: 0.4762\n",
            "Epoch 861/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2721 - accuracy: 0.9275 - val_loss: 1.6882 - val_accuracy: 0.4656\n",
            "Epoch 862/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2741 - accuracy: 0.9193 - val_loss: 1.6831 - val_accuracy: 0.4815\n",
            "Epoch 863/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2773 - accuracy: 0.9179 - val_loss: 1.7151 - val_accuracy: 0.4709\n",
            "Epoch 864/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2655 - accuracy: 0.9166 - val_loss: 1.6568 - val_accuracy: 0.4656\n",
            "Epoch 865/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2697 - accuracy: 0.9179 - val_loss: 1.7498 - val_accuracy: 0.4868\n",
            "Epoch 866/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2425 - accuracy: 0.9302 - val_loss: 1.7534 - val_accuracy: 0.4709\n",
            "Epoch 867/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2762 - accuracy: 0.9152 - val_loss: 1.7200 - val_accuracy: 0.4815\n",
            "Epoch 868/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2494 - accuracy: 0.9234 - val_loss: 1.7425 - val_accuracy: 0.4868\n",
            "Epoch 869/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2693 - accuracy: 0.9275 - val_loss: 1.6940 - val_accuracy: 0.5079\n",
            "Epoch 870/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2522 - accuracy: 0.9289 - val_loss: 1.6845 - val_accuracy: 0.4868\n",
            "Epoch 871/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2725 - accuracy: 0.9138 - val_loss: 1.6669 - val_accuracy: 0.4921\n",
            "Epoch 872/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2631 - accuracy: 0.9248 - val_loss: 1.6830 - val_accuracy: 0.4762\n",
            "Epoch 873/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2707 - accuracy: 0.9248 - val_loss: 1.6797 - val_accuracy: 0.4709\n",
            "Epoch 874/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2531 - accuracy: 0.9261 - val_loss: 1.7644 - val_accuracy: 0.4974\n",
            "Epoch 875/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2823 - accuracy: 0.9124 - val_loss: 1.7314 - val_accuracy: 0.4868\n",
            "Epoch 876/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2641 - accuracy: 0.9289 - val_loss: 1.7148 - val_accuracy: 0.4974\n",
            "Epoch 877/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2689 - accuracy: 0.9220 - val_loss: 1.6867 - val_accuracy: 0.4762\n",
            "Epoch 878/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2413 - accuracy: 0.9439 - val_loss: 1.7009 - val_accuracy: 0.4868\n",
            "Epoch 879/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2636 - accuracy: 0.9234 - val_loss: 1.7072 - val_accuracy: 0.4762\n",
            "Epoch 880/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2643 - accuracy: 0.9289 - val_loss: 1.7109 - val_accuracy: 0.4974\n",
            "Epoch 881/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2574 - accuracy: 0.9234 - val_loss: 1.6790 - val_accuracy: 0.4815\n",
            "Epoch 882/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2642 - accuracy: 0.9275 - val_loss: 1.7209 - val_accuracy: 0.4815\n",
            "Epoch 883/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2372 - accuracy: 0.9316 - val_loss: 1.7928 - val_accuracy: 0.4709\n",
            "Epoch 884/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2612 - accuracy: 0.9179 - val_loss: 1.7153 - val_accuracy: 0.4815\n",
            "Epoch 885/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2556 - accuracy: 0.9275 - val_loss: 1.6972 - val_accuracy: 0.4497\n",
            "Epoch 886/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2413 - accuracy: 0.9302 - val_loss: 1.7271 - val_accuracy: 0.4656\n",
            "Epoch 887/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2524 - accuracy: 0.9234 - val_loss: 1.7082 - val_accuracy: 0.4921\n",
            "Epoch 888/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2473 - accuracy: 0.9343 - val_loss: 1.7302 - val_accuracy: 0.4656\n",
            "Epoch 889/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2320 - accuracy: 0.9357 - val_loss: 1.7063 - val_accuracy: 0.4974\n",
            "Epoch 890/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2723 - accuracy: 0.9124 - val_loss: 1.7541 - val_accuracy: 0.4709\n",
            "Epoch 891/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2412 - accuracy: 0.9248 - val_loss: 1.7413 - val_accuracy: 0.4815\n",
            "Epoch 892/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2302 - accuracy: 0.9371 - val_loss: 1.7644 - val_accuracy: 0.4815\n",
            "Epoch 893/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2339 - accuracy: 0.9261 - val_loss: 1.7129 - val_accuracy: 0.4603\n",
            "Epoch 894/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2497 - accuracy: 0.9261 - val_loss: 1.7202 - val_accuracy: 0.4656\n",
            "Epoch 895/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2261 - accuracy: 0.9398 - val_loss: 1.7196 - val_accuracy: 0.4974\n",
            "Epoch 896/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2305 - accuracy: 0.9330 - val_loss: 1.7700 - val_accuracy: 0.4921\n",
            "Epoch 897/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2154 - accuracy: 0.9508 - val_loss: 1.7924 - val_accuracy: 0.4921\n",
            "Epoch 898/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2501 - accuracy: 0.9234 - val_loss: 1.7289 - val_accuracy: 0.4709\n",
            "Epoch 899/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2467 - accuracy: 0.9234 - val_loss: 1.7487 - val_accuracy: 0.4868\n",
            "Epoch 900/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2555 - accuracy: 0.9261 - val_loss: 1.7551 - val_accuracy: 0.4656\n",
            "Epoch 901/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2264 - accuracy: 0.9398 - val_loss: 1.7348 - val_accuracy: 0.4868\n",
            "Epoch 902/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2233 - accuracy: 0.9453 - val_loss: 1.7410 - val_accuracy: 0.4868\n",
            "Epoch 903/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2698 - accuracy: 0.9220 - val_loss: 1.7272 - val_accuracy: 0.4815\n",
            "Epoch 904/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2169 - accuracy: 0.9453 - val_loss: 1.7201 - val_accuracy: 0.4709\n",
            "Epoch 905/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2094 - accuracy: 0.9508 - val_loss: 1.7315 - val_accuracy: 0.4762\n",
            "Epoch 906/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2524 - accuracy: 0.9234 - val_loss: 1.7605 - val_accuracy: 0.4762\n",
            "Epoch 907/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2424 - accuracy: 0.9302 - val_loss: 1.7478 - val_accuracy: 0.5132\n",
            "Epoch 908/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2241 - accuracy: 0.9302 - val_loss: 1.7893 - val_accuracy: 0.4815\n",
            "Epoch 909/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2239 - accuracy: 0.9439 - val_loss: 1.7224 - val_accuracy: 0.4868\n",
            "Epoch 910/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2343 - accuracy: 0.9234 - val_loss: 1.7912 - val_accuracy: 0.4868\n",
            "Epoch 911/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2266 - accuracy: 0.9357 - val_loss: 1.7532 - val_accuracy: 0.5026\n",
            "Epoch 912/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2417 - accuracy: 0.9330 - val_loss: 1.8038 - val_accuracy: 0.4921\n",
            "Epoch 913/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2370 - accuracy: 0.9234 - val_loss: 1.7238 - val_accuracy: 0.4762\n",
            "Epoch 914/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2198 - accuracy: 0.9425 - val_loss: 1.7268 - val_accuracy: 0.4868\n",
            "Epoch 915/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2385 - accuracy: 0.9330 - val_loss: 1.7472 - val_accuracy: 0.4762\n",
            "Epoch 916/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2154 - accuracy: 0.9480 - val_loss: 1.7722 - val_accuracy: 0.4815\n",
            "Epoch 917/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2097 - accuracy: 0.9466 - val_loss: 1.7896 - val_accuracy: 0.4921\n",
            "Epoch 918/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2085 - accuracy: 0.9466 - val_loss: 1.7697 - val_accuracy: 0.4974\n",
            "Epoch 919/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2116 - accuracy: 0.9316 - val_loss: 1.7241 - val_accuracy: 0.4974\n",
            "Epoch 920/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2419 - accuracy: 0.9261 - val_loss: 1.7227 - val_accuracy: 0.5026\n",
            "Epoch 921/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2127 - accuracy: 0.9439 - val_loss: 1.7625 - val_accuracy: 0.4868\n",
            "Epoch 922/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2298 - accuracy: 0.9316 - val_loss: 1.7390 - val_accuracy: 0.4815\n",
            "Epoch 923/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1979 - accuracy: 0.9521 - val_loss: 1.7782 - val_accuracy: 0.4497\n",
            "Epoch 924/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2195 - accuracy: 0.9275 - val_loss: 1.7599 - val_accuracy: 0.4921\n",
            "Epoch 925/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2062 - accuracy: 0.9330 - val_loss: 1.7456 - val_accuracy: 0.5026\n",
            "Epoch 926/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2156 - accuracy: 0.9412 - val_loss: 1.7516 - val_accuracy: 0.4868\n",
            "Epoch 927/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2222 - accuracy: 0.9425 - val_loss: 1.7759 - val_accuracy: 0.4974\n",
            "Epoch 928/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2154 - accuracy: 0.9439 - val_loss: 1.8547 - val_accuracy: 0.4656\n",
            "Epoch 929/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1973 - accuracy: 0.9371 - val_loss: 1.7705 - val_accuracy: 0.4974\n",
            "Epoch 930/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1910 - accuracy: 0.9576 - val_loss: 1.8020 - val_accuracy: 0.4921\n",
            "Epoch 931/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2144 - accuracy: 0.9412 - val_loss: 1.7683 - val_accuracy: 0.4815\n",
            "Epoch 932/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2030 - accuracy: 0.9439 - val_loss: 1.7995 - val_accuracy: 0.4762\n",
            "Epoch 933/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2370 - accuracy: 0.9275 - val_loss: 1.7622 - val_accuracy: 0.4656\n",
            "Epoch 934/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2021 - accuracy: 0.9508 - val_loss: 1.7648 - val_accuracy: 0.4868\n",
            "Epoch 935/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2100 - accuracy: 0.9439 - val_loss: 1.7769 - val_accuracy: 0.4921\n",
            "Epoch 936/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2059 - accuracy: 0.9439 - val_loss: 1.7973 - val_accuracy: 0.4921\n",
            "Epoch 937/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2230 - accuracy: 0.9384 - val_loss: 1.7519 - val_accuracy: 0.4868\n",
            "Epoch 938/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1996 - accuracy: 0.9398 - val_loss: 1.8980 - val_accuracy: 0.4762\n",
            "Epoch 939/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1975 - accuracy: 0.9549 - val_loss: 1.7557 - val_accuracy: 0.4762\n",
            "Epoch 940/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.2122 - accuracy: 0.9371 - val_loss: 1.7914 - val_accuracy: 0.4921\n",
            "Epoch 941/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2134 - accuracy: 0.9371 - val_loss: 1.7884 - val_accuracy: 0.4921\n",
            "Epoch 942/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1999 - accuracy: 0.9535 - val_loss: 1.8264 - val_accuracy: 0.4974\n",
            "Epoch 943/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1963 - accuracy: 0.9466 - val_loss: 1.7998 - val_accuracy: 0.5026\n",
            "Epoch 944/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2083 - accuracy: 0.9384 - val_loss: 1.7921 - val_accuracy: 0.4762\n",
            "Epoch 945/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1887 - accuracy: 0.9453 - val_loss: 1.7556 - val_accuracy: 0.4762\n",
            "Epoch 946/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1976 - accuracy: 0.9439 - val_loss: 1.8664 - val_accuracy: 0.4762\n",
            "Epoch 947/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1942 - accuracy: 0.9535 - val_loss: 1.7869 - val_accuracy: 0.4762\n",
            "Epoch 948/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1967 - accuracy: 0.9480 - val_loss: 1.8205 - val_accuracy: 0.5079\n",
            "Epoch 949/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2109 - accuracy: 0.9425 - val_loss: 1.8416 - val_accuracy: 0.4762\n",
            "Epoch 950/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1831 - accuracy: 0.9535 - val_loss: 1.8310 - val_accuracy: 0.4603\n",
            "Epoch 951/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1991 - accuracy: 0.9425 - val_loss: 1.7983 - val_accuracy: 0.4656\n",
            "Epoch 952/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1931 - accuracy: 0.9466 - val_loss: 1.9030 - val_accuracy: 0.4921\n",
            "Epoch 953/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1869 - accuracy: 0.9466 - val_loss: 1.8032 - val_accuracy: 0.4974\n",
            "Epoch 954/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1773 - accuracy: 0.9562 - val_loss: 1.7989 - val_accuracy: 0.4921\n",
            "Epoch 955/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1969 - accuracy: 0.9508 - val_loss: 1.8389 - val_accuracy: 0.4815\n",
            "Epoch 956/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1901 - accuracy: 0.9590 - val_loss: 1.8450 - val_accuracy: 0.4921\n",
            "Epoch 957/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1780 - accuracy: 0.9535 - val_loss: 1.8187 - val_accuracy: 0.4656\n",
            "Epoch 958/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1785 - accuracy: 0.9672 - val_loss: 1.8005 - val_accuracy: 0.4762\n",
            "Epoch 959/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1859 - accuracy: 0.9466 - val_loss: 1.8024 - val_accuracy: 0.4815\n",
            "Epoch 960/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1998 - accuracy: 0.9466 - val_loss: 1.8675 - val_accuracy: 0.4815\n",
            "Epoch 961/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1769 - accuracy: 0.9603 - val_loss: 1.8126 - val_accuracy: 0.4762\n",
            "Epoch 962/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1800 - accuracy: 0.9508 - val_loss: 1.8960 - val_accuracy: 0.4709\n",
            "Epoch 963/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1985 - accuracy: 0.9508 - val_loss: 1.8733 - val_accuracy: 0.4868\n",
            "Epoch 964/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.1881 - accuracy: 0.9480 - val_loss: 1.8342 - val_accuracy: 0.4815\n",
            "Epoch 965/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1718 - accuracy: 0.9535 - val_loss: 1.8188 - val_accuracy: 0.4921\n",
            "Epoch 966/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1794 - accuracy: 0.9590 - val_loss: 1.8687 - val_accuracy: 0.4762\n",
            "Epoch 967/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1772 - accuracy: 0.9466 - val_loss: 1.8296 - val_accuracy: 0.4868\n",
            "Epoch 968/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1694 - accuracy: 0.9631 - val_loss: 1.8636 - val_accuracy: 0.4974\n",
            "Epoch 969/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1709 - accuracy: 0.9576 - val_loss: 1.8499 - val_accuracy: 0.4974\n",
            "Epoch 970/1009\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.1684 - accuracy: 0.9590 - val_loss: 1.8074 - val_accuracy: 0.4762\n",
            "Epoch 971/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1672 - accuracy: 0.9576 - val_loss: 1.8287 - val_accuracy: 0.4921\n",
            "Epoch 972/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1676 - accuracy: 0.9508 - val_loss: 1.8329 - val_accuracy: 0.4815\n",
            "Epoch 973/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1890 - accuracy: 0.9494 - val_loss: 1.8090 - val_accuracy: 0.4815\n",
            "Epoch 974/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1712 - accuracy: 0.9603 - val_loss: 1.8264 - val_accuracy: 0.4868\n",
            "Epoch 975/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1748 - accuracy: 0.9617 - val_loss: 1.8454 - val_accuracy: 0.4815\n",
            "Epoch 976/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1541 - accuracy: 0.9685 - val_loss: 1.8244 - val_accuracy: 0.4921\n",
            "Epoch 977/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1532 - accuracy: 0.9685 - val_loss: 1.8788 - val_accuracy: 0.4762\n",
            "Epoch 978/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2002 - accuracy: 0.9330 - val_loss: 1.8473 - val_accuracy: 0.4868\n",
            "Epoch 979/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1966 - accuracy: 0.9439 - val_loss: 1.8469 - val_accuracy: 0.4868\n",
            "Epoch 980/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1573 - accuracy: 0.9631 - val_loss: 1.8456 - val_accuracy: 0.4868\n",
            "Epoch 981/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1819 - accuracy: 0.9535 - val_loss: 1.8318 - val_accuracy: 0.4921\n",
            "Epoch 982/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1571 - accuracy: 0.9562 - val_loss: 1.9226 - val_accuracy: 0.4762\n",
            "Epoch 983/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1653 - accuracy: 0.9535 - val_loss: 1.8449 - val_accuracy: 0.4815\n",
            "Epoch 984/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1526 - accuracy: 0.9590 - val_loss: 1.8786 - val_accuracy: 0.4974\n",
            "Epoch 985/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1549 - accuracy: 0.9672 - val_loss: 1.8535 - val_accuracy: 0.4974\n",
            "Epoch 986/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1547 - accuracy: 0.9603 - val_loss: 1.8887 - val_accuracy: 0.4815\n",
            "Epoch 987/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1721 - accuracy: 0.9535 - val_loss: 1.9944 - val_accuracy: 0.4868\n",
            "Epoch 988/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1598 - accuracy: 0.9658 - val_loss: 1.8576 - val_accuracy: 0.4709\n",
            "Epoch 989/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1741 - accuracy: 0.9494 - val_loss: 1.8737 - val_accuracy: 0.4868\n",
            "Epoch 990/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1697 - accuracy: 0.9562 - val_loss: 1.8591 - val_accuracy: 0.4868\n",
            "Epoch 991/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1558 - accuracy: 0.9549 - val_loss: 1.8515 - val_accuracy: 0.4815\n",
            "Epoch 992/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1382 - accuracy: 0.9699 - val_loss: 1.8625 - val_accuracy: 0.4921\n",
            "Epoch 993/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1584 - accuracy: 0.9603 - val_loss: 1.8447 - val_accuracy: 0.4974\n",
            "Epoch 994/1009\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1559 - accuracy: 0.9576 - val_loss: 1.8856 - val_accuracy: 0.4868\n",
            "Epoch 995/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1494 - accuracy: 0.9658 - val_loss: 1.8703 - val_accuracy: 0.4815\n",
            "Epoch 996/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1647 - accuracy: 0.9590 - val_loss: 1.8813 - val_accuracy: 0.4656\n",
            "Epoch 997/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1524 - accuracy: 0.9617 - val_loss: 1.8671 - val_accuracy: 0.4762\n",
            "Epoch 998/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1506 - accuracy: 0.9672 - val_loss: 1.8870 - val_accuracy: 0.4762\n",
            "Epoch 999/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1562 - accuracy: 0.9617 - val_loss: 1.9175 - val_accuracy: 0.5026\n",
            "Epoch 1000/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1451 - accuracy: 0.9699 - val_loss: 1.9113 - val_accuracy: 0.4921\n",
            "Epoch 1001/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1835 - accuracy: 0.9494 - val_loss: 1.9005 - val_accuracy: 0.4762\n",
            "Epoch 1002/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1501 - accuracy: 0.9713 - val_loss: 1.9585 - val_accuracy: 0.4656\n",
            "Epoch 1003/1009\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1451 - accuracy: 0.9699 - val_loss: 1.9186 - val_accuracy: 0.4762\n",
            "Epoch 1004/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1520 - accuracy: 0.9713 - val_loss: 1.9470 - val_accuracy: 0.4868\n",
            "Epoch 1005/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1350 - accuracy: 0.9658 - val_loss: 1.8801 - val_accuracy: 0.4762\n",
            "Epoch 1006/1009\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1566 - accuracy: 0.9549 - val_loss: 1.8622 - val_accuracy: 0.4815\n",
            "Epoch 1007/1009\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1610 - accuracy: 0.9549 - val_loss: 1.9054 - val_accuracy: 0.5079\n",
            "Epoch 1008/1009\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.1548 - accuracy: 0.9603 - val_loss: 1.8901 - val_accuracy: 0.4709\n",
            "Epoch 1009/1009\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1421 - accuracy: 0.9631 - val_loss: 1.9132 - val_accuracy: 0.4868\n"
          ]
        }
      ],
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=32, epochs=1009, validation_data=(x_testcnn, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uzzUPv_iQQD"
      },
      "source": [
        "## Plotting the accuracy and loss graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pzyYy5DqiSD6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "cd9c6b4e-e348-49e7-aa98-dc163e11855d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [2.5986135005950928, 2.348567247390747, 2.2906956672668457, 2.283384084701538, 2.257154941558838, 2.244178533554077, 2.2204771041870117, 2.213257312774658, 2.2226107120513916, 2.186615467071533, 2.1814730167388916, 2.1545889377593994, 2.1583163738250732, 2.119544267654419, 2.137927532196045, 2.1340012550354004, 2.103287696838379, 2.0659589767456055, 2.0925729274749756, 2.0738747119903564, 2.0591208934783936, 2.063876152038574, 2.0562548637390137, 2.0265145301818848, 2.036975860595703, 2.0172159671783447, 2.0142860412597656, 1.9968007802963257, 1.9797483682632446, 1.9750200510025024, 1.9470455646514893, 1.9512418508529663, 1.961345911026001, 1.9535645246505737, 1.9402765035629272, 1.9314173460006714, 1.9018863439559937, 1.9149599075317383, 1.9033920764923096, 1.8675003051757812, 1.8857513666152954, 1.8852214813232422, 1.8586188554763794, 1.8438329696655273, 1.8190034627914429, 1.8430770635604858, 1.8396234512329102, 1.821488618850708, 1.8304955959320068, 1.7900550365447998, 1.8106036186218262, 1.8191145658493042, 1.787270188331604, 1.7989628314971924, 1.767364740371704, 1.755934476852417, 1.7743664979934692, 1.7592090368270874, 1.7569692134857178, 1.7277865409851074, 1.7759534120559692, 1.743966817855835, 1.7237969636917114, 1.7249819040298462, 1.7001432180404663, 1.7095389366149902, 1.6871211528778076, 1.6663947105407715, 1.6914387941360474, 1.6876320838928223, 1.6663076877593994, 1.659529685974121, 1.6762183904647827, 1.6517215967178345, 1.6342464685440063, 1.6558036804199219, 1.6319297552108765, 1.6171692609786987, 1.6003923416137695, 1.619100570678711, 1.5939257144927979, 1.604854702949524, 1.5992077589035034, 1.6249291896820068, 1.6013845205307007, 1.5746798515319824, 1.5812139511108398, 1.5686531066894531, 1.5695046186447144, 1.5782321691513062, 1.5552711486816406, 1.548851728439331, 1.5378865003585815, 1.5329290628433228, 1.5262296199798584, 1.533695101737976, 1.5019253492355347, 1.522631049156189, 1.5096999406814575, 1.5030262470245361, 1.5181186199188232, 1.4957141876220703, 1.5024080276489258, 1.4760695695877075, 1.4760935306549072, 1.514772891998291, 1.4658243656158447, 1.4971250295639038, 1.4648691415786743, 1.4799975156784058, 1.472518801689148, 1.4608500003814697, 1.4426538944244385, 1.444527268409729, 1.464022159576416, 1.4527587890625, 1.4580795764923096, 1.4355984926223755, 1.4140092134475708, 1.4488390684127808, 1.4309165477752686, 1.4329893589019775, 1.4315760135650635, 1.4516167640686035, 1.421204686164856, 1.4130234718322754, 1.4062340259552002, 1.4055896997451782, 1.438664436340332, 1.3819140195846558, 1.4147616624832153, 1.4032315015792847, 1.4033986330032349, 1.3954441547393799, 1.3811661005020142, 1.3984732627868652, 1.4129642248153687, 1.3938374519348145, 1.383777379989624, 1.3503916263580322, 1.3767807483673096, 1.401555061340332, 1.3799264430999756, 1.3749346733093262, 1.369268774986267, 1.3694771528244019, 1.3769429922103882, 1.3894436359405518, 1.3536224365234375, 1.341862678527832, 1.3568916320800781, 1.3533769845962524, 1.370442509651184, 1.364393949508667, 1.3579936027526855, 1.3568280935287476, 1.3414833545684814, 1.315508246421814, 1.3571882247924805, 1.34819757938385, 1.3420580625534058, 1.3318649530410767, 1.326867938041687, 1.3188369274139404, 1.3031138181686401, 1.3060206174850464, 1.3320399522781372, 1.3485313653945923, 1.2779422998428345, 1.3313393592834473, 1.3144936561584473, 1.3023147583007812, 1.2997376918792725, 1.3204716444015503, 1.2957897186279297, 1.3148728609085083, 1.298417568206787, 1.3134745359420776, 1.291118860244751, 1.3276792764663696, 1.2871013879776, 1.2714898586273193, 1.2722560167312622, 1.3170548677444458, 1.3078151941299438, 1.300604224205017, 1.2860909700393677, 1.2605233192443848, 1.2497676610946655, 1.279226541519165, 1.2674992084503174, 1.2787328958511353, 1.2810202836990356, 1.254731297492981, 1.265800952911377, 1.294563889503479, 1.2793723344802856, 1.2927013635635376, 1.236891508102417, 1.244964599609375, 1.2757409811019897, 1.2356112003326416, 1.2513585090637207, 1.2829228639602661, 1.2515326738357544, 1.2665172815322876, 1.2502648830413818, 1.26278817653656, 1.2385637760162354, 1.2304612398147583, 1.2401643991470337, 1.2383440732955933, 1.2502225637435913, 1.2170263528823853, 1.2101233005523682, 1.2235780954360962, 1.2224568128585815, 1.2367284297943115, 1.2269614934921265, 1.2269247770309448, 1.2199559211730957, 1.2237868309020996, 1.2021536827087402, 1.2062770128250122, 1.1945304870605469, 1.2043002843856812, 1.2300188541412354, 1.189162015914917, 1.1837544441223145, 1.2025697231292725, 1.1966438293457031, 1.1960397958755493, 1.1978808641433716, 1.2083934545516968, 1.1981654167175293, 1.1992170810699463, 1.1986453533172607, 1.199725866317749, 1.1970219612121582, 1.176876425743103, 1.174681305885315, 1.2001736164093018, 1.1780046224594116, 1.1803042888641357, 1.165546178817749, 1.189194917678833, 1.1821495294570923, 1.1838572025299072, 1.1668102741241455, 1.1865310668945312, 1.1702739000320435, 1.181154727935791, 1.1585098505020142, 1.1569539308547974, 1.1499292850494385, 1.1494255065917969, 1.1617755889892578, 1.163076400756836, 1.1545288562774658, 1.1518642902374268, 1.1314777135849, 1.1467136144638062, 1.1442968845367432, 1.129243016242981, 1.1380791664123535, 1.1695443391799927, 1.1431747674942017, 1.1427078247070312, 1.1571934223175049, 1.1364388465881348, 1.1383072137832642, 1.1202553510665894, 1.1100012063980103, 1.1124404668807983, 1.1392401456832886, 1.1098926067352295, 1.1332775354385376, 1.1301614046096802, 1.1328438520431519, 1.1289774179458618, 1.1168251037597656, 1.1193060874938965, 1.1032688617706299, 1.0999932289123535, 1.1214916706085205, 1.1014245748519897, 1.0955321788787842, 1.1120439767837524, 1.1076997518539429, 1.0914196968078613, 1.0993436574935913, 1.1161926984786987, 1.1153382062911987, 1.0892733335494995, 1.084482192993164, 1.0853126049041748, 1.0872668027877808, 1.0896899700164795, 1.1039912700653076, 1.0937405824661255, 1.0990073680877686, 1.0700130462646484, 1.0890941619873047, 1.089106798171997, 1.0845280885696411, 1.07471764087677, 1.0838372707366943, 1.0570136308670044, 1.0812132358551025, 1.0481587648391724, 1.0433846712112427, 1.0620553493499756, 1.0531294345855713, 1.0508610010147095, 1.0420793294906616, 1.074660301208496, 1.051115870475769, 1.0636041164398193, 1.0481234788894653, 1.0385615825653076, 1.0119733810424805, 1.0496340990066528, 1.0489815473556519, 1.0485941171646118, 1.0471059083938599, 1.0380346775054932, 1.0525195598602295, 1.0251080989837646, 1.0517553091049194, 1.0379558801651, 1.0254451036453247, 1.0182842016220093, 1.0189090967178345, 1.0293238162994385, 1.014559268951416, 1.0205116271972656, 1.0179204940795898, 1.034289002418518, 1.0251878499984741, 1.011680006980896, 1.026080846786499, 1.0103089809417725, 1.0163321495056152, 1.005873441696167, 1.0112543106079102, 1.0099836587905884, 1.0042057037353516, 0.9948421716690063, 0.9959431290626526, 1.0000674724578857, 1.014967441558838, 1.006395697593689, 0.9987494945526123, 0.9805174469947815, 0.987949550151825, 1.008901834487915, 0.9986873865127563, 0.988685131072998, 1.0128729343414307, 0.9667364358901978, 0.9875423908233643, 0.9845303893089294, 0.9983835220336914, 0.9967270493507385, 0.9674807190895081, 0.9974619150161743, 0.962456226348877, 0.9636046886444092, 0.9810595512390137, 0.9865672588348389, 0.9529377818107605, 0.9716213941574097, 0.991454005241394, 0.9568662643432617, 0.966391921043396, 0.9862390160560608, 0.9394643902778625, 0.9608702063560486, 0.9616747498512268, 0.9626702070236206, 0.9449646472930908, 0.9650720953941345, 0.9842700362205505, 0.9420471787452698, 0.9655959606170654, 0.9129369258880615, 0.9295718669891357, 0.9214769005775452, 0.9608389735221863, 0.9291918873786926, 0.9353762865066528, 0.9259806275367737, 0.9192108511924744, 0.9559959769248962, 0.937554121017456, 0.9291867613792419, 0.9304972290992737, 0.8982320427894592, 0.9132264852523804, 0.9371288418769836, 0.9269838333129883, 0.9036518931388855, 0.9218029975891113, 0.914714515209198, 0.9084212779998779, 0.9188507199287415, 0.9145385026931763, 0.9192591905593872, 0.9317470192909241, 0.8972702622413635, 0.9124155640602112, 0.9179583191871643, 0.9112455248832703, 0.8972319960594177, 0.8929247260093689, 0.8567177653312683, 0.88044273853302, 0.880667507648468, 0.8660242557525635, 0.9075915813446045, 0.8725948929786682, 0.8802887797355652, 0.8769523501396179, 0.8851817846298218, 0.89103764295578, 0.8665466904640198, 0.8862818479537964, 0.8914217352867126, 0.8772293329238892, 0.871955931186676, 0.862797200679779, 0.8631770610809326, 0.8562643527984619, 0.86505526304245, 0.849222719669342, 0.8738692998886108, 0.8914136290550232, 0.8654668927192688, 0.868705689907074, 0.8548488020896912, 0.85285484790802, 0.8711452484130859, 0.8380931615829468, 0.8490663766860962, 0.8145962357521057, 0.8301807641983032, 0.8399653434753418, 0.8577790856361389, 0.8338489532470703, 0.8208258152008057, 0.8464111089706421, 0.8373641967773438, 0.8198639750480652, 0.8565990924835205, 0.8472000956535339, 0.8138696551322937, 0.839231550693512, 0.8218249082565308, 0.8170768022537231, 0.8443751931190491, 0.8402119874954224, 0.8132093548774719, 0.8021552562713623, 0.8034095764160156, 0.8362980484962463, 0.8046240210533142, 0.8048290014266968, 0.8093211054801941, 0.8116403222084045, 0.810712456703186, 0.8013527989387512, 0.8331279754638672, 0.8145107626914978, 0.7922584414482117, 0.7857999801635742, 0.7958788871765137, 0.7827350497245789, 0.8005436062812805, 0.7804684638977051, 0.8006041646003723, 0.7985764145851135, 0.7899880409240723, 0.7735496163368225, 0.7915497422218323, 0.7737297415733337, 0.7829236388206482, 0.7727445960044861, 0.7909591197967529, 0.7785931825637817, 0.8059631586074829, 0.7790981531143188, 0.7640970945358276, 0.7547504901885986, 0.767299234867096, 0.7559450268745422, 0.7608095407485962, 0.7589842677116394, 0.7736141681671143, 0.7413190603256226, 0.7574117183685303, 0.7651463747024536, 0.7444060444831848, 0.7263854742050171, 0.7355920076370239, 0.752238929271698, 0.7450390458106995, 0.7572762370109558, 0.746042788028717, 0.7335378527641296, 0.7439174652099609, 0.7152683138847351, 0.7429919242858887, 0.7528446316719055, 0.7084106802940369, 0.7279737591743469, 0.7325528860092163, 0.7491011619567871, 0.7335830330848694, 0.7274552583694458, 0.7261123061180115, 0.7181384563446045, 0.7153120040893555, 0.7424984574317932, 0.6945273280143738, 0.7137932181358337, 0.6981613039970398, 0.7092912197113037, 0.7084032297134399, 0.7130312919616699, 0.7205454707145691, 0.6919823884963989, 0.7025731205940247, 0.7059389352798462, 0.7045937776565552, 0.7031477093696594, 0.7053599953651428, 0.691577136516571, 0.720127284526825, 0.6924648284912109, 0.6991835236549377, 0.7092031240463257, 0.6954389810562134, 0.6815496683120728, 0.6933784484863281, 0.7129606008529663, 0.677649974822998, 0.6983513236045837, 0.6838852167129517, 0.7116591334342957, 0.6937016844749451, 0.6813756227493286, 0.6918541789054871, 0.7041174173355103, 0.6646901965141296, 0.6826640963554382, 0.6735434532165527, 0.6708863973617554, 0.6680354475975037, 0.6634454727172852, 0.6704375147819519, 0.6426290273666382, 0.671657383441925, 0.679431140422821, 0.6413010358810425, 0.6494953632354736, 0.6668252348899841, 0.6610796451568604, 0.6503016352653503, 0.6499720811843872, 0.6655094027519226, 0.6324587464332581, 0.6524521708488464, 0.6327422261238098, 0.6614444851875305, 0.6360753178596497, 0.6260031461715698, 0.658393383026123, 0.6275774240493774, 0.6329622268676758, 0.6444868445396423, 0.627885103225708, 0.6447845101356506, 0.6324300765991211, 0.6316443085670471, 0.6295587420463562, 0.602185845375061, 0.6237968802452087, 0.6284847855567932, 0.6215930581092834, 0.6298124194145203, 0.6276293396949768, 0.6123427748680115, 0.6417773365974426, 0.6042177677154541, 0.6151354908943176, 0.6046367883682251, 0.6276493668556213, 0.5840691924095154, 0.5842179656028748, 0.5982005596160889, 0.619170069694519, 0.5937690734863281, 0.6024199724197388, 0.612015426158905, 0.6198157072067261, 0.5744851231575012, 0.5711860656738281, 0.5891845226287842, 0.6012845635414124, 0.6045749187469482, 0.5693724751472473, 0.5694077610969543, 0.5944948792457581, 0.5907958745956421, 0.6096749901771545, 0.5707530975341797, 0.5835648775100708, 0.5725690722465515, 0.5684033632278442, 0.6124647855758667, 0.5855095982551575, 0.5867255926132202, 0.5736629366874695, 0.5815194249153137, 0.579329252243042, 0.5867149233818054, 0.5696015954017639, 0.5692291259765625, 0.5721659064292908, 0.5503668189048767, 0.5743587017059326, 0.5677424669265747, 0.5722631216049194, 0.5772191286087036, 0.568196713924408, 0.5590541362762451, 0.5454254746437073, 0.549288809299469, 0.5559172630310059, 0.5448811650276184, 0.5593883395195007, 0.5526130199432373, 0.5440478324890137, 0.5413051247596741, 0.5224848389625549, 0.5390849113464355, 0.557275116443634, 0.5447225570678711, 0.544532299041748, 0.5211127996444702, 0.5489131808280945, 0.5494595766067505, 0.5491250157356262, 0.5326811671257019, 0.5318470001220703, 0.5213174819946289, 0.505842924118042, 0.5260526537895203, 0.5113613605499268, 0.5100231170654297, 0.5223789215087891, 0.5161622762680054, 0.5139418244361877, 0.5203865766525269, 0.49221673607826233, 0.52239590883255, 0.5070016384124756, 0.501804769039154, 0.4983164370059967, 0.5160649418830872, 0.4973159432411194, 0.5132108926773071, 0.4890795350074768, 0.488141268491745, 0.5043690204620361, 0.49084901809692383, 0.5066105723381042, 0.46742671728134155, 0.4925974905490875, 0.4999541640281677, 0.49752023816108704, 0.47587206959724426, 0.5012482404708862, 0.4938731789588928, 0.4758685827255249, 0.4793378412723541, 0.47707754373550415, 0.49769824743270874, 0.4666183590888977, 0.49085891246795654, 0.4868695139884949, 0.479996919631958, 0.4737074673175812, 0.4743024408817291, 0.4793180227279663, 0.4810603857040405, 0.4653106927871704, 0.4826655089855194, 0.46558547019958496, 0.44572019577026367, 0.4664546251296997, 0.47388696670532227, 0.44862934947013855, 0.48034951090812683, 0.47292599081993103, 0.46546992659568787, 0.4611402153968811, 0.45370638370513916, 0.46425336599349976, 0.4460124969482422, 0.4621419608592987, 0.44098564982414246, 0.44776737689971924, 0.44934406876564026, 0.4374907314777374, 0.4112435281276703, 0.44405314326286316, 0.44694599509239197, 0.4430384635925293, 0.4367171823978424, 0.44920527935028076, 0.43354544043540955, 0.4416435956954956, 0.4310910701751709, 0.4410637617111206, 0.4399702250957489, 0.44978806376457214, 0.4219566583633423, 0.4314047396183014, 0.42801010608673096, 0.43382933735847473, 0.4198612868785858, 0.42312541604042053, 0.4010160565376282, 0.40570002794265747, 0.4113374650478363, 0.41184210777282715, 0.42184022068977356, 0.4140605926513672, 0.41417133808135986, 0.4142317771911621, 0.3838525712490082, 0.4077518880367279, 0.410323828458786, 0.4167129397392273, 0.4019128382205963, 0.37722817063331604, 0.3980066180229187, 0.41452476382255554, 0.39637303352355957, 0.4141009449958801, 0.3889859914779663, 0.40256547927856445, 0.4087829887866974, 0.406868577003479, 0.3911644518375397, 0.37306344509124756, 0.39648258686065674, 0.40903177857398987, 0.40079838037490845, 0.39144155383110046, 0.40056851506233215, 0.3878267705440521, 0.3697036802768707, 0.41405653953552246, 0.37135791778564453, 0.39115869998931885, 0.3907800018787384, 0.3786836564540863, 0.35316795110702515, 0.37356826663017273, 0.3719789683818817, 0.3684735894203186, 0.3764267563819885, 0.3793317973613739, 0.38502559065818787, 0.3827412724494934, 0.3705572485923767, 0.36119046807289124, 0.35029399394989014, 0.36110958456993103, 0.33796989917755127, 0.3503304123878479, 0.36208000779151917, 0.34355583786964417, 0.351723313331604, 0.3600996136665344, 0.34812629222869873, 0.3658891022205353, 0.3556253910064697, 0.3515608310699463, 0.32738348841667175, 0.35486799478530884, 0.36934447288513184, 0.36199697852134705, 0.33820483088493347, 0.35003289580345154, 0.3255555033683777, 0.32743531465530396, 0.32515937089920044, 0.3207724690437317, 0.3326127231121063, 0.33658069372177124, 0.32371801137924194, 0.3444368839263916, 0.33603635430336, 0.31715312600135803, 0.32711246609687805, 0.32895585894584656, 0.3388773500919342, 0.31804338097572327, 0.32019656896591187, 0.33284544944763184, 0.35076895356178284, 0.3477317988872528, 0.3102378845214844, 0.32411161065101624, 0.32838889956474304, 0.31072330474853516, 0.31760039925575256, 0.3051837682723999, 0.30527567863464355, 0.3065987229347229, 0.32249629497528076, 0.31270530819892883, 0.29881617426872253, 0.31124916672706604, 0.32343003153800964, 0.3115636110305786, 0.29286515712738037, 0.30782875418663025, 0.30957579612731934, 0.30871257185935974, 0.3110877573490143, 0.2914157211780548, 0.3017619550228119, 0.3037293553352356, 0.30098509788513184, 0.29749149084091187, 0.28007858991622925, 0.2948543429374695, 0.29976901412010193, 0.2922177016735077, 0.29286161065101624, 0.2831553816795349, 0.2868160903453827, 0.28413093090057373, 0.2783178985118866, 0.2859407067298889, 0.27611494064331055, 0.27542272210121155, 0.26172953844070435, 0.272621750831604, 0.272927463054657, 0.2823430001735687, 0.28581809997558594, 0.28263789415359497, 0.283809632062912, 0.27202481031417847, 0.26548904180526733, 0.2720561623573303, 0.2741275727748871, 0.27732163667678833, 0.26546400785446167, 0.26970812678337097, 0.24249599874019623, 0.27616631984710693, 0.24939562380313873, 0.26933494210243225, 0.25223681330680847, 0.2725182771682739, 0.2630552351474762, 0.2707059681415558, 0.2530507445335388, 0.28227588534355164, 0.2641333341598511, 0.26887181401252747, 0.241323322057724, 0.2636193633079529, 0.2643072009086609, 0.257365345954895, 0.2642415761947632, 0.23716320097446442, 0.26119717955589294, 0.25563016533851624, 0.24129915237426758, 0.2524428367614746, 0.24727842211723328, 0.23195171356201172, 0.2722722291946411, 0.2412140667438507, 0.23024266958236694, 0.233876571059227, 0.24972935020923615, 0.2260677069425583, 0.23046542704105377, 0.21535931527614594, 0.2501388490200043, 0.24669207632541656, 0.2555014193058014, 0.22638818621635437, 0.223282590508461, 0.26983973383903503, 0.21690228581428528, 0.20940755307674408, 0.25239109992980957, 0.24235327541828156, 0.22414571046829224, 0.22394205629825592, 0.23429670929908752, 0.2265680730342865, 0.2417498379945755, 0.23695559799671173, 0.2198280692100525, 0.2384902536869049, 0.21544265747070312, 0.2097322642803192, 0.20850910246372223, 0.2116393893957138, 0.24190455675125122, 0.212715744972229, 0.22983260452747345, 0.19794081151485443, 0.21950756013393402, 0.2062302827835083, 0.21563474833965302, 0.222220316529274, 0.2154114842414856, 0.1973426640033722, 0.190984845161438, 0.21440084278583527, 0.2030407339334488, 0.2370271533727646, 0.202087864279747, 0.21002255380153656, 0.20594923198223114, 0.2229548692703247, 0.1996341347694397, 0.19753333926200867, 0.2121589183807373, 0.2134139984846115, 0.19988436996936798, 0.19633260369300842, 0.20827174186706543, 0.18866905570030212, 0.19761411845684052, 0.1942284107208252, 0.19669410586357117, 0.2109086662530899, 0.18307384848594666, 0.19909261167049408, 0.1930536925792694, 0.18694381415843964, 0.17729216814041138, 0.19692057371139526, 0.19005924463272095, 0.17802385985851288, 0.17850695550441742, 0.18588750064373016, 0.19980290532112122, 0.17686279118061066, 0.1799662560224533, 0.19850902259349823, 0.18806448578834534, 0.1717836707830429, 0.17935924232006073, 0.1772293895483017, 0.1694277971982956, 0.17088699340820312, 0.16837036609649658, 0.16724351048469543, 0.167582705616951, 0.18898284435272217, 0.1711578518152237, 0.1748187094926834, 0.15414835512638092, 0.15318836271762848, 0.200180783867836, 0.1965707689523697, 0.15726229548454285, 0.1819448173046112, 0.15705560147762299, 0.16534355282783508, 0.15261222422122955, 0.1549200713634491, 0.15465012192726135, 0.17214138805866241, 0.15978612005710602, 0.17408454418182373, 0.16972950100898743, 0.155795156955719, 0.13818824291229248, 0.15843361616134644, 0.1559450775384903, 0.14937373995780945, 0.16472771763801575, 0.15238097310066223, 0.15057514607906342, 0.1562233418226242, 0.14509323239326477, 0.18349166214466095, 0.1500958502292633, 0.14510077238082886, 0.15199337899684906, 0.1350099742412567, 0.15657782554626465, 0.16097967326641083, 0.15477751195430756, 0.14212805032730103], 'accuracy': [0.1121750995516777, 0.14500683546066284, 0.15321476757526398, 0.1614227145910263, 0.150478795170784, 0.15731874108314514, 0.18331053853034973, 0.1737346053123474, 0.15595075488090515, 0.1874144971370697, 0.17783857882022858, 0.199726402759552, 0.20930232107639313, 0.20930232107639313, 0.201094388961792, 0.19151847064495087, 0.19425444304943085, 0.24076607823371887, 0.201094388961792, 0.25581395626068115, 0.20656634867191315, 0.23119014501571655, 0.22161422669887543, 0.23255814611911774, 0.23255814611911774, 0.2380300909280777, 0.25718194246292114, 0.2380300909280777, 0.2722298204898834, 0.24760602414608002, 0.2777017652988434, 0.2612859010696411, 0.23119014501571655, 0.2599179148674011, 0.28043776750564575, 0.26812586188316345, 0.28043776750564575, 0.2517099976539612, 0.28317373991012573, 0.27906978130340576, 0.300957590341568, 0.28180575370788574, 0.2900136709213257, 0.3269493877887726, 0.30506154894828796, 0.29274964332580566, 0.2900136709213257, 0.3091655373573303, 0.31874144077301025, 0.3351573050022125, 0.3255814015865326, 0.2900136709213257, 0.3214774429798126, 0.3283173739910126, 0.35567715764045715, 0.3406292796134949, 0.3255814015865326, 0.3242134153842926, 0.3502052128314972, 0.35841313004493713, 0.31874144077301025, 0.3488371968269348, 0.3515731990337372, 0.3474692106246948, 0.35567715764045715, 0.36935704946517944, 0.3761969804763794, 0.3597811162471771, 0.35704514384269714, 0.35841313004493713, 0.3734610080718994, 0.38303694128990173, 0.36662107706069946, 0.3912448585033417, 0.37893298268318176, 0.38166895508766174, 0.3844049274921417, 0.403556764125824, 0.3844049274921417, 0.38030096888542175, 0.399452805519104, 0.399452805519104, 0.4145006835460663, 0.3898768723011017, 0.40492475032806396, 0.398084819316864, 0.39534884691238403, 0.40629273653030396, 0.4131326973438263, 0.4281805753707886, 0.4254446029663086, 0.4268125891685486, 0.43091654777526855, 0.4227086305618286, 0.4281805753707886, 0.41860464215278625, 0.4281805753707886, 0.4281805753707886, 0.43228453397750854, 0.4404924809932709, 0.4268125891685486, 0.4391244947910309, 0.45417237281799316, 0.45690834522247314, 0.4596443176269531, 0.4254446029663086, 0.44596442580223083, 0.44596442580223083, 0.44459643959999084, 0.4514364004135132, 0.4404924809932709, 0.47879618406295776, 0.4528043866157532, 0.4487003982067108, 0.42134061455726624, 0.45417237281799316, 0.4651162922382355, 0.45417237281799316, 0.46785226464271545, 0.4487003982067108, 0.45690834522247314, 0.44459643959999084, 0.45417237281799316, 0.44322845339775085, 0.45690834522247314, 0.46922025084495544, 0.44186046719551086, 0.46785226464271545, 0.4391244947910309, 0.5034199953079224, 0.48290014266967773, 0.4965800344944, 0.46922025084495544, 0.47058823704719543, 0.5034199953079224, 0.4733242094516754, 0.4473324120044708, 0.49931600689888, 0.48290014266967773, 0.4911080598831177, 0.4842681288719177, 0.4842681288719177, 0.46922025084495544, 0.46922025084495544, 0.4842681288719177, 0.4842681288719177, 0.49247604608535767, 0.4897400736808777, 0.49794802069664, 0.5047879815101624, 0.49247604608535767, 0.49384406208992004, 0.49521204829216003, 0.4911080598831177, 0.5170998573303223, 0.49247604608535767, 0.48016417026519775, 0.5212038159370422, 0.48153215646743774, 0.4897400736808777, 0.48153215646743774, 0.4911080598831177, 0.5075239539146423, 0.5129958987236023, 0.5184678435325623, 0.49794802069664, 0.4965800344944, 0.4911080598831177, 0.529411792755127, 0.48290014266967773, 0.4965800344944, 0.529411792755127, 0.5225718021392822, 0.5088919401168823, 0.5212038159370422, 0.5184678435325623, 0.49521204829216003, 0.5417236685752869, 0.529411792755127, 0.4842681288719177, 0.5321477651596069, 0.5129958987236023, 0.5430916547775269, 0.5225718021392822, 0.5143638849258423, 0.5198358297348022, 0.5321477651596069, 0.5307797789573669, 0.5471956133842468, 0.5212038159370422, 0.5485635995864868, 0.5143638849258423, 0.5116279125213623, 0.5389876961708069, 0.5266757607460022, 0.5239397883415222, 0.5170998573303223, 0.5061559677124023, 0.5170998573303223, 0.5430916547775269, 0.5280437469482422, 0.5444596409797668, 0.5403556823730469, 0.5266757607460022, 0.5458276271820068, 0.5212038159370422, 0.5512995719909668, 0.5198358297348022, 0.5540355443954468, 0.5471956133842468, 0.5526675581932068, 0.5458276271820068, 0.5403556823730469, 0.5389876961708069, 0.5567715167999268, 0.5471956133842468, 0.5485635995864868, 0.5595075488090515, 0.5471956133842468, 0.5157318711280823, 0.5526675581932068, 0.5636115074157715, 0.5759233832359314, 0.5471956133842468, 0.5677154660224915, 0.5499315857887268, 0.5554035305976868, 0.5540355443954468, 0.5540355443954468, 0.5485635995864868, 0.5554035305976868, 0.5512995719909668, 0.5458276271820068, 0.5444596409797668, 0.5567715167999268, 0.5649794936180115, 0.5554035305976868, 0.5512995719909668, 0.5581395626068115, 0.5677154660224915, 0.5649794936180115, 0.5581395626068115, 0.5786593556404114, 0.5567715167999268, 0.5882353186607361, 0.5636115074157715, 0.5622435212135315, 0.5759233832359314, 0.5458276271820068, 0.5663474798202515, 0.5622435212135315, 0.5636115074157715, 0.5813953280448914, 0.5677154660224915, 0.5540355443954468, 0.5800273418426514, 0.5595075488090515, 0.5636115074157715, 0.5841313004493713, 0.5800273418426514, 0.593707263469696, 0.5786593556404114, 0.5827633142471313, 0.592339277267456, 0.5608755350112915, 0.5608755350112915, 0.5854992866516113, 0.5854992866516113, 0.5909712910652161, 0.5677154660224915, 0.5854992866516113, 0.5813953280448914, 0.595075249671936, 0.5909712910652161, 0.5841313004493713, 0.5854992866516113, 0.5882353186607361, 0.5786593556404114, 0.5772913694381714, 0.5896033048629761, 0.604651153087616, 0.5800273418426514, 0.603283166885376, 0.600547194480896, 0.595075249671936, 0.603283166885376, 0.595075249671936, 0.5772913694381714, 0.593707263469696, 0.592339277267456, 0.5759233832359314, 0.5882353186607361, 0.600547194480896, 0.597811222076416, 0.6169630885124207, 0.6183310747146606, 0.607387125492096, 0.6087551116943359, 0.5677154660224915, 0.593707263469696, 0.600547194480896, 0.6155951023101807, 0.593707263469696, 0.599179208278656, 0.596443235874176, 0.601915180683136, 0.597811222076416, 0.6251710057258606, 0.595075249671936, 0.6196990609169006, 0.6183310747146606, 0.6183310747146606, 0.6292749643325806, 0.600547194480896, 0.6183310747146606, 0.607387125492096, 0.6128590703010559, 0.6087551116943359, 0.6265389919281006, 0.6456908583641052, 0.6333789229393005, 0.6128590703010559, 0.6169630885124207, 0.6169630885124207, 0.595075249671936, 0.6251710057258606, 0.6087551116943359, 0.6292749643325806, 0.6155951023101807, 0.6183310747146606, 0.6347469091415405, 0.6155951023101807, 0.6306429505348206, 0.6361148953437805, 0.6374828815460205, 0.6224350333213806, 0.6320109367370605, 0.6306429505348206, 0.6183310747146606, 0.6333789229393005, 0.6265389919281006, 0.6388508677482605, 0.6429548859596252, 0.6361148953437805, 0.6333789229393005, 0.6196990609169006, 0.6415868401527405, 0.6347469091415405, 0.6388508677482605, 0.6443228721618652, 0.6374828815460205, 0.6361148953437805, 0.6361148953437805, 0.6415868401527405, 0.6361148953437805, 0.6265389919281006, 0.6238030195236206, 0.6456908583641052, 0.6224350333213806, 0.6580027341842651, 0.6470588445663452, 0.6621066927909851, 0.6142270565032959, 0.6292749643325806, 0.6374828815460205, 0.6470588445663452, 0.6484268307685852, 0.6648426651954651, 0.6484268307685852, 0.6320109367370605, 0.6470588445663452, 0.6456908583641052, 0.6265389919281006, 0.6511628031730652, 0.6402188539505005, 0.6374828815460205, 0.6662106513977051, 0.6443228721618652, 0.6538987755775452, 0.6552667617797852, 0.6497948169708252, 0.6484268307685852, 0.6402188539505005, 0.6771546006202698, 0.6443228721618652, 0.6730506420135498, 0.6566347479820251, 0.6634746789932251, 0.6429548859596252, 0.6757866144180298, 0.6634746789932251, 0.6607387065887451, 0.670314610004425, 0.6566347479820251, 0.6675786375999451, 0.6744186282157898, 0.6470588445663452, 0.6730506420135498, 0.6798905730247498, 0.6552667617797852, 0.6443228721618652, 0.6716826558113098, 0.6662106513977051, 0.6771546006202698, 0.6689466238021851, 0.6744186282157898, 0.6880984902381897, 0.6894664764404297, 0.6634746789932251, 0.6812585592269897, 0.6538987755775452, 0.6757866144180298, 0.6689466238021851, 0.6744186282157898, 0.6922024488449097, 0.7113543152809143, 0.6853625178337097, 0.6757866144180298, 0.6935704350471497, 0.6689466238021851, 0.6908344626426697, 0.7045143842697144, 0.6867305040359497, 0.6757866144180298, 0.6730506420135498, 0.6880984902381897, 0.6757866144180298, 0.6484268307685852, 0.6757866144180298, 0.6867305040359497, 0.7072503566741943, 0.7058823704719543, 0.7099863290786743, 0.6771546006202698, 0.7113543152809143, 0.6853625178337097, 0.6744186282157898, 0.7045143842697144, 0.7086183428764343, 0.6990423798561096, 0.7004104256629944, 0.6812585592269897, 0.7017784118652344, 0.7045143842697144, 0.7140902876853943, 0.7017784118652344, 0.6908344626426697, 0.6867305040359497, 0.7031463980674744, 0.7031463980674744, 0.6990423798561096, 0.7168262600898743, 0.7072503566741943, 0.7017784118652344, 0.6922024488449097, 0.7086183428764343, 0.7099863290786743, 0.7086183428764343, 0.7195622324943542, 0.6935704350471497, 0.6949384212493896, 0.7250341773033142, 0.6880984902381897, 0.7195622324943542, 0.7236661911010742, 0.7154582738876343, 0.7195622324943542, 0.7072503566741943, 0.7154582738876343, 0.7154582738876343, 0.7209302186965942, 0.7086183428764343, 0.6908344626426697, 0.7236661911010742, 0.7236661911010742, 0.7181942462921143, 0.7346101403236389, 0.7127223014831543, 0.7181942462921143, 0.7332421541213989, 0.7195622324943542, 0.7058823704719543, 0.7277701497077942, 0.7099863290786743, 0.7346101403236389, 0.7127223014831543, 0.7168262600898743, 0.7140902876853943, 0.6976743936538696, 0.6922024488449097, 0.7250341773033142, 0.7428180575370789, 0.7400820851325989, 0.7428180575370789, 0.730506181716919, 0.729138195514679, 0.7250341773033142, 0.7195622324943542, 0.7523939609527588, 0.7332421541213989, 0.7332421541213989, 0.7414500713348389, 0.7523939609527588, 0.7578659653663635, 0.729138195514679, 0.7373461127281189, 0.7332421541213989, 0.7373461127281189, 0.7619699239730835, 0.7414500713348389, 0.7455540299415588, 0.7414500713348389, 0.7428180575370789, 0.7619699239730835, 0.7400820851325989, 0.7387140989303589, 0.7236661911010742, 0.7346101403236389, 0.7510259747505188, 0.7469220161437988, 0.7469220161437988, 0.7701778411865234, 0.7250341773033142, 0.7551299333572388, 0.7633379101753235, 0.7551299333572388, 0.7578659653663635, 0.7592339515686035, 0.7592339515686035, 0.7523939609527588, 0.7688098549842834, 0.7455540299415588, 0.7482900023460388, 0.7606019377708435, 0.7482900023460388, 0.7619699239730835, 0.7606019377708435, 0.7537619471549988, 0.7729138135910034, 0.7660738825798035, 0.7592339515686035, 0.7523939609527588, 0.7838577032089233, 0.7742817997932434, 0.7455540299415588, 0.7674418687820435, 0.7688098549842834, 0.7606019377708435, 0.7346101403236389, 0.7619699239730835, 0.7647058963775635, 0.7742817997932434, 0.7551299333572388, 0.7660738825798035, 0.7647058963775635, 0.7838577032089233, 0.7688098549842834, 0.7606019377708435, 0.7742817997932434, 0.7729138135910034, 0.7783857583999634, 0.7606019377708435, 0.7701778411865234, 0.7893297076225281, 0.7838577032089233, 0.7647058963775635, 0.7756497859954834, 0.7783857583999634, 0.7592339515686035, 0.7879617214202881, 0.7811217308044434, 0.7770177721977234, 0.7852256894111633, 0.7742817997932434, 0.7715458273887634, 0.7838577032089233, 0.7756497859954834, 0.7906976938247681, 0.7879617214202881, 0.7756497859954834, 0.7742817997932434, 0.7770177721977234, 0.7865937352180481, 0.7797537446022034, 0.7893297076225281, 0.8153215050697327, 0.7920656800270081, 0.793433666229248, 0.7865937352180481, 0.7838577032089233, 0.7852256894111633, 0.7906976938247681, 0.7838577032089233, 0.8125854730606079, 0.7838577032089233, 0.807113528251648, 0.7715458273887634, 0.805745542049408, 0.803009569644928, 0.7879617214202881, 0.805745542049408, 0.7824897170066833, 0.7838577032089233, 0.805745542049408, 0.793433666229248, 0.805745542049408, 0.8112174868583679, 0.797537624835968, 0.800273597240448, 0.796169638633728, 0.8235294222831726, 0.8084815144538879, 0.807113528251648, 0.803009569644928, 0.7852256894111633, 0.8084815144538879, 0.798905611038208, 0.8084815144538879, 0.8166894912719727, 0.7838577032089233, 0.804377555847168, 0.798905611038208, 0.8084815144538879, 0.800273597240448, 0.801641583442688, 0.807113528251648, 0.8125854730606079, 0.807113528251648, 0.805745542049408, 0.8235294222831726, 0.794801652431488, 0.8139534592628479, 0.8139534592628479, 0.800273597240448, 0.801641583442688, 0.8139534592628479, 0.8194254636764526, 0.8262653946876526, 0.8235294222831726, 0.8235294222831726, 0.8125854730606079, 0.8139534592628479, 0.8221614360809326, 0.8180574774742126, 0.8303693532943726, 0.8276333808898926, 0.805745542049408, 0.798905611038208, 0.8166894912719727, 0.8358412981033325, 0.8098495006561279, 0.8125854730606079, 0.803009569644928, 0.8344733119010925, 0.8413132429122925, 0.8166894912719727, 0.8399452567100525, 0.8372092843055725, 0.8331053256988525, 0.8385772705078125, 0.8385772705078125, 0.8303693532943726, 0.8276333808898926, 0.8290013670921326, 0.8426812291145325, 0.8248974084854126, 0.8290013670921326, 0.8331053256988525, 0.8303693532943726, 0.8153215050697327, 0.8454172611236572, 0.8262653946876526, 0.8399452567100525, 0.8358412981033325, 0.8385772705078125, 0.8454172611236572, 0.8303693532943726, 0.8577291369438171, 0.8454172611236572, 0.8372092843055725, 0.8331053256988525, 0.8495212197303772, 0.8344733119010925, 0.8385772705078125, 0.8481532335281372, 0.8467852473258972, 0.8467852473258972, 0.8290013670921326, 0.8508892059326172, 0.8385772705078125, 0.8426812291145325, 0.8331053256988525, 0.8563611507415771, 0.8426812291145325, 0.8495212197303772, 0.8290013670921326, 0.8549931645393372, 0.8399452567100525, 0.8426812291145325, 0.8618330955505371, 0.8385772705078125, 0.8426812291145325, 0.8563611507415771, 0.8248974084854126, 0.8413132429122925, 0.8440492749214172, 0.8495212197303772, 0.8495212197303772, 0.8454172611236572, 0.8495212197303772, 0.8549931645393372, 0.8727770447731018, 0.8549931645393372, 0.8536251783370972, 0.8590971231460571, 0.8782489895820618, 0.8522571921348572, 0.8522571921348572, 0.8577291369438171, 0.8659370541572571, 0.8549931645393372, 0.8686730265617371, 0.8673050403594971, 0.8659370541572571, 0.8536251783370972, 0.8590971231460571, 0.8522571921348572, 0.8755130171775818, 0.8481532335281372, 0.8645690679550171, 0.8604651093482971, 0.8727770447731018, 0.8563611507415771, 0.8768810033798218, 0.8659370541572571, 0.8686730265617371, 0.8645690679550171, 0.8604651093482971, 0.8536251783370972, 0.870041012763977, 0.871408998966217, 0.8741450309753418, 0.8741450309753418, 0.8604651093482971, 0.8755130171775818, 0.870041012763977, 0.8782489895820618, 0.8796169757843018, 0.8727770447731018, 0.8837209343910217, 0.870041012763977, 0.8755130171775818, 0.8809849619865417, 0.8604651093482971, 0.8741450309753418, 0.8727770447731018, 0.8809849619865417, 0.871408998966217, 0.8686730265617371, 0.870041012763977, 0.8809849619865417, 0.870041012763977, 0.8727770447731018, 0.8946648240089417, 0.8673050403594971, 0.8891928791999817, 0.8755130171775818, 0.8823529481887817, 0.8891928791999817, 0.8878248929977417, 0.8878248929977417, 0.8837209343910217, 0.8891928791999817, 0.8823529481887817, 0.8864569067955017, 0.8809849619865417, 0.8823529481887817, 0.8891928791999817, 0.8809849619865417, 0.9001368284225464, 0.8878248929977417, 0.8974007964134216, 0.871408998966217, 0.8974007964134216, 0.8987687826156616, 0.8878248929977417, 0.8823529481887817, 0.8905608654022217, 0.8823529481887817, 0.8905608654022217, 0.9083447456359863, 0.9083447456359863, 0.8782489895820618, 0.8837209343910217, 0.8632010817527771, 0.9056087732315063, 0.8974007964134216, 0.8919288516044617, 0.8974007964134216, 0.9056087732315063, 0.9097127318382263, 0.8932968378067017, 0.8946648240089417, 0.9097127318382263, 0.8891928791999817, 0.8891928791999817, 0.9097127318382263, 0.8919288516044617, 0.9069767594337463, 0.8809849619865417, 0.9001368284225464, 0.9069767594337463, 0.9001368284225464, 0.8878248929977417, 0.8809849619865417, 0.9124487042427063, 0.9083447456359863, 0.9097127318382263, 0.9179206490516663, 0.9015048146247864, 0.9001368284225464, 0.9042407870292664, 0.9124487042427063, 0.8974007964134216, 0.8987687826156616, 0.9001368284225464, 0.9124487042427063, 0.8932968378067017, 0.9028728008270264, 0.9233925938606262, 0.9138166904449463, 0.8919288516044617, 0.9015048146247864, 0.9124487042427063, 0.9138166904449463, 0.9056087732315063, 0.9097127318382263, 0.9056087732315063, 0.9110807180404663, 0.9247605800628662, 0.9083447456359863, 0.9056087732315063, 0.9083447456359863, 0.9083447456359863, 0.9179206490516663, 0.9097127318382263, 0.9206566214561462, 0.9192886352539062, 0.9247605800628662, 0.9179206490516663, 0.9233925938606262, 0.9206566214561462, 0.9220246076583862, 0.9083447456359863, 0.9206566214561462, 0.9124487042427063, 0.9083447456359863, 0.9179206490516663, 0.9220246076583862, 0.9261285662651062, 0.9274965524673462, 0.9192886352539062, 0.9179206490516663, 0.9165526628494263, 0.9179206490516663, 0.930232584476471, 0.9151846766471863, 0.9233925938606262, 0.9274965524673462, 0.928864598274231, 0.9138166904449463, 0.9247605800628662, 0.9247605800628662, 0.9261285662651062, 0.9124487042427063, 0.928864598274231, 0.9220246076583862, 0.9439124464988708, 0.9233925938606262, 0.928864598274231, 0.9233925938606262, 0.9274965524673462, 0.9316005706787109, 0.9179206490516663, 0.9274965524673462, 0.930232584476471, 0.9233925938606262, 0.9343365430831909, 0.9357045292854309, 0.9124487042427063, 0.9247605800628662, 0.9370725154876709, 0.9261285662651062, 0.9261285662651062, 0.9398084878921509, 0.9329685568809509, 0.9507523775100708, 0.9233925938606262, 0.9233925938606262, 0.9261285662651062, 0.9398084878921509, 0.9452804327011108, 0.9220246076583862, 0.9452804327011108, 0.9507523775100708, 0.9233925938606262, 0.930232584476471, 0.930232584476471, 0.9439124464988708, 0.9233925938606262, 0.9357045292854309, 0.9329685568809509, 0.9233925938606262, 0.9425444602966309, 0.9329685568809509, 0.9480164051055908, 0.9466484189033508, 0.9466484189033508, 0.9316005706787109, 0.9261285662651062, 0.9439124464988708, 0.9316005706787109, 0.9521203637123108, 0.9274965524673462, 0.9329685568809509, 0.9411764740943909, 0.9425444602966309, 0.9439124464988708, 0.9370725154876709, 0.9575923681259155, 0.9411764740943909, 0.9439124464988708, 0.9274965524673462, 0.9507523775100708, 0.9439124464988708, 0.9439124464988708, 0.9384405016899109, 0.9398084878921509, 0.9548563361167908, 0.9370725154876709, 0.9370725154876709, 0.9534883499145508, 0.9466484189033508, 0.9384405016899109, 0.9452804327011108, 0.9439124464988708, 0.9534883499145508, 0.9480164051055908, 0.9425444602966309, 0.9534883499145508, 0.9425444602966309, 0.9466484189033508, 0.9466484189033508, 0.9562243223190308, 0.9507523775100708, 0.9589603543281555, 0.9534883499145508, 0.9671682715415955, 0.9466484189033508, 0.9466484189033508, 0.9603283405303955, 0.9507523775100708, 0.9507523775100708, 0.9480164051055908, 0.9534883499145508, 0.9589603543281555, 0.9466484189033508, 0.9630643129348755, 0.9575923681259155, 0.9589603543281555, 0.9575923681259155, 0.9507523775100708, 0.9493843913078308, 0.9603283405303955, 0.9616963267326355, 0.9685362577438354, 0.9685362577438354, 0.9329685568809509, 0.9439124464988708, 0.9630643129348755, 0.9534883499145508, 0.9562243223190308, 0.9534883499145508, 0.9589603543281555, 0.9671682715415955, 0.9603283405303955, 0.9534883499145508, 0.9658002853393555, 0.9493843913078308, 0.9562243223190308, 0.9548563361167908, 0.9699042439460754, 0.9603283405303955, 0.9575923681259155, 0.9658002853393555, 0.9589603543281555, 0.9616963267326355, 0.9671682715415955, 0.9616963267326355, 0.9699042439460754, 0.9493843913078308, 0.9712722301483154, 0.9699042439460754, 0.9712722301483154, 0.9658002853393555, 0.9548563361167908, 0.9548563361167908, 0.9603283405303955, 0.9630643129348755], 'val_loss': [2.3010220527648926, 2.253352403640747, 2.2356014251708984, 2.230043411254883, 2.225867509841919, 2.2047364711761475, 2.20107102394104, 2.182213544845581, 2.175731897354126, 2.171480417251587, 2.16184663772583, 2.1620750427246094, 2.150832176208496, 2.1398379802703857, 2.1262753009796143, 2.117682933807373, 2.1119117736816406, 2.0947837829589844, 2.0906760692596436, 2.0783071517944336, 2.0681040287017822, 2.0630905628204346, 2.0560660362243652, 2.049752950668335, 2.038863182067871, 2.042513132095337, 2.0178685188293457, 2.0177769660949707, 2.0152981281280518, 2.0139997005462646, 2.0066864490509033, 1.982287883758545, 1.9832038879394531, 1.9678683280944824, 1.9695371389389038, 1.9558860063552856, 1.962489366531372, 1.9439575672149658, 1.9470059871673584, 1.923529863357544, 1.913793921470642, 1.9121617078781128, 1.9131191968917847, 1.9110113382339478, 1.9154062271118164, 1.892512559890747, 1.8848334550857544, 1.8817744255065918, 1.8717983961105347, 1.87303626537323, 1.8567873239517212, 1.864769458770752, 1.850050687789917, 1.8468104600906372, 1.8405601978302002, 1.8288519382476807, 1.8185651302337646, 1.8131462335586548, 1.8165339231491089, 1.803457260131836, 1.795203685760498, 1.8091470003128052, 1.7900071144104004, 1.8085110187530518, 1.7859115600585938, 1.782876968383789, 1.7777856588363647, 1.760709524154663, 1.753834843635559, 1.7473901510238647, 1.7491705417633057, 1.7540875673294067, 1.746550440788269, 1.737803339958191, 1.7324365377426147, 1.7170213460922241, 1.703101634979248, 1.7120740413665771, 1.7189061641693115, 1.7051150798797607, 1.6836893558502197, 1.6842541694641113, 1.6880656480789185, 1.6814957857131958, 1.663436770439148, 1.6662766933441162, 1.651820421218872, 1.6518851518630981, 1.6703917980194092, 1.6539416313171387, 1.6337671279907227, 1.643113613128662, 1.6457138061523438, 1.6694555282592773, 1.6272974014282227, 1.6500873565673828, 1.623283863067627, 1.6201521158218384, 1.6415925025939941, 1.6159526109695435, 1.6119403839111328, 1.6146138906478882, 1.6049247980117798, 1.6277884244918823, 1.6013689041137695, 1.6096903085708618, 1.5915216207504272, 1.6035475730895996, 1.5911979675292969, 1.5935449600219727, 1.5784776210784912, 1.5782337188720703, 1.5877007246017456, 1.575483798980713, 1.5818098783493042, 1.5911668539047241, 1.5915549993515015, 1.5768691301345825, 1.5941294431686401, 1.587296724319458, 1.5908679962158203, 1.5718064308166504, 1.5657143592834473, 1.5828583240509033, 1.578531265258789, 1.5640923976898193, 1.5621137619018555, 1.6107532978057861, 1.5665483474731445, 1.5532965660095215, 1.551588773727417, 1.56723153591156, 1.5656344890594482, 1.5476831197738647, 1.5410537719726562, 1.551891565322876, 1.5944969654083252, 1.543513536453247, 1.5377833843231201, 1.5592237710952759, 1.5463862419128418, 1.5554840564727783, 1.5379793643951416, 1.536973237991333, 1.5541170835494995, 1.5441904067993164, 1.524877905845642, 1.5398310422897339, 1.533197045326233, 1.528635859489441, 1.5344994068145752, 1.54229736328125, 1.5381245613098145, 1.5349220037460327, 1.5313223600387573, 1.533666729927063, 1.5199625492095947, 1.5372834205627441, 1.5283790826797485, 1.5374927520751953, 1.5116472244262695, 1.5117652416229248, 1.5584266185760498, 1.5356813669204712, 1.5371540784835815, 1.5125231742858887, 1.507519245147705, 1.505677580833435, 1.5282715559005737, 1.5095083713531494, 1.5587589740753174, 1.5266304016113281, 1.4921764135360718, 1.4972156286239624, 1.502700686454773, 1.5035713911056519, 1.483521819114685, 1.5122774839401245, 1.4858165979385376, 1.5272561311721802, 1.5176995992660522, 1.497629165649414, 1.4871183633804321, 1.5040470361709595, 1.508093237876892, 1.4925806522369385, 1.4947398900985718, 1.4897493124008179, 1.4954317808151245, 1.494430661201477, 1.4790538549423218, 1.5174702405929565, 1.482758641242981, 1.4910777807235718, 1.4848741292953491, 1.4879777431488037, 1.4906953573226929, 1.4904694557189941, 1.489098310470581, 1.466626524925232, 1.5099682807922363, 1.464503526687622, 1.4974501132965088, 1.4993319511413574, 1.5249797105789185, 1.4687719345092773, 1.4875801801681519, 1.4971286058425903, 1.4704707860946655, 1.490057110786438, 1.4779963493347168, 1.4874039888381958, 1.4818781614303589, 1.4665111303329468, 1.4880720376968384, 1.472424030303955, 1.480706810951233, 1.4666041135787964, 1.4627898931503296, 1.4668364524841309, 1.464167833328247, 1.4676226377487183, 1.4604477882385254, 1.4539778232574463, 1.461748719215393, 1.455634593963623, 1.4740456342697144, 1.4631885290145874, 1.4451522827148438, 1.4615997076034546, 1.4422235488891602, 1.4519834518432617, 1.4825092554092407, 1.5067214965820312, 1.459444522857666, 1.446457028388977, 1.4424493312835693, 1.44838285446167, 1.4565520286560059, 1.4536484479904175, 1.4634276628494263, 1.454339861869812, 1.440773606300354, 1.4374425411224365, 1.4624614715576172, 1.4569555521011353, 1.4461452960968018, 1.440532922744751, 1.4359996318817139, 1.444661259651184, 1.480969786643982, 1.4647216796875, 1.443125605583191, 1.4354006052017212, 1.4333429336547852, 1.4418559074401855, 1.4284641742706299, 1.4345629215240479, 1.4225108623504639, 1.4390782117843628, 1.4385994672775269, 1.4364970922470093, 1.4263747930526733, 1.4315792322158813, 1.4442083835601807, 1.444402813911438, 1.4378206729888916, 1.4457447528839111, 1.4608503580093384, 1.4360206127166748, 1.433923363685608, 1.4343994855880737, 1.469143033027649, 1.4354321956634521, 1.4250504970550537, 1.4365367889404297, 1.4261069297790527, 1.448631763458252, 1.4339265823364258, 1.4165303707122803, 1.4238262176513672, 1.436608910560608, 1.427258014678955, 1.4521374702453613, 1.441135287284851, 1.4365828037261963, 1.412110686302185, 1.441083312034607, 1.4606436491012573, 1.4232581853866577, 1.4055066108703613, 1.4681296348571777, 1.4123283624649048, 1.4144350290298462, 1.4199546575546265, 1.4139472246170044, 1.4147239923477173, 1.4243547916412354, 1.4051613807678223, 1.4585834741592407, 1.4091535806655884, 1.493177890777588, 1.4005413055419922, 1.4028241634368896, 1.4169528484344482, 1.4405535459518433, 1.4231716394424438, 1.4106029272079468, 1.4139002561569214, 1.4295140504837036, 1.4478245973587036, 1.4248740673065186, 1.407700777053833, 1.4045674800872803, 1.4065186977386475, 1.3968195915222168, 1.4026225805282593, 1.4031598567962646, 1.4252175092697144, 1.419734239578247, 1.4028561115264893, 1.3952049016952515, 1.42607581615448, 1.4299607276916504, 1.4502309560775757, 1.4146314859390259, 1.388368010520935, 1.4137799739837646, 1.390407681465149, 1.4152796268463135, 1.4140548706054688, 1.3974289894104004, 1.3975179195404053, 1.41636323928833, 1.3975883722305298, 1.3955621719360352, 1.3863202333450317, 1.4118232727050781, 1.4116798639297485, 1.3862438201904297, 1.3877038955688477, 1.4145162105560303, 1.3977715969085693, 1.4145547151565552, 1.392703890800476, 1.4490762948989868, 1.4196052551269531, 1.415123701095581, 1.409915804862976, 1.4028433561325073, 1.3988707065582275, 1.418841004371643, 1.3976762294769287, 1.3937187194824219, 1.3899739980697632, 1.4018198251724243, 1.3958691358566284, 1.4146983623504639, 1.396687388420105, 1.3982948064804077, 1.3962050676345825, 1.3842906951904297, 1.4056780338287354, 1.4023429155349731, 1.3859363794326782, 1.3862413167953491, 1.3995217084884644, 1.4190987348556519, 1.3839212656021118, 1.4027968645095825, 1.4160373210906982, 1.3973188400268555, 1.3935470581054688, 1.3922655582427979, 1.3971993923187256, 1.4204378128051758, 1.4213420152664185, 1.4162954092025757, 1.3850326538085938, 1.3837486505508423, 1.3935577869415283, 1.3851428031921387, 1.396977186203003, 1.3910350799560547, 1.3950939178466797, 1.3815076351165771, 1.417896032333374, 1.3921877145767212, 1.3860877752304077, 1.3801521062850952, 1.3850579261779785, 1.387688159942627, 1.380915880203247, 1.3814985752105713, 1.3735569715499878, 1.3888015747070312, 1.3790228366851807, 1.4175796508789062, 1.3861171007156372, 1.401483416557312, 1.389516830444336, 1.385975956916809, 1.4476423263549805, 1.4043724536895752, 1.394773006439209, 1.3871158361434937, 1.4062731266021729, 1.4250149726867676, 1.3778412342071533, 1.375319242477417, 1.391953706741333, 1.434928297996521, 1.3696943521499634, 1.3688088655471802, 1.3781832456588745, 1.3805222511291504, 1.417382001876831, 1.3959839344024658, 1.4444125890731812, 1.386053442955017, 1.405975341796875, 1.4165425300598145, 1.4137694835662842, 1.3998777866363525, 1.4062505960464478, 1.3868467807769775, 1.403576135635376, 1.3894046545028687, 1.4043309688568115, 1.375484585762024, 1.410552978515625, 1.386181354522705, 1.378130555152893, 1.3899940252304077, 1.394420862197876, 1.3704839944839478, 1.3859764337539673, 1.4172554016113281, 1.3806685209274292, 1.393437147140503, 1.423585057258606, 1.374255895614624, 1.389176845550537, 1.369948148727417, 1.3975642919540405, 1.3844778537750244, 1.3987042903900146, 1.4036788940429688, 1.385076880455017, 1.3774998188018799, 1.3988237380981445, 1.3822832107543945, 1.389303207397461, 1.4036874771118164, 1.3844809532165527, 1.3727627992630005, 1.37065589427948, 1.436531662940979, 1.3713200092315674, 1.3867288827896118, 1.3798855543136597, 1.3903570175170898, 1.387681484222412, 1.3935613632202148, 1.3949331045150757, 1.3808317184448242, 1.4078477621078491, 1.3829611539840698, 1.3785836696624756, 1.3822258710861206, 1.3807495832443237, 1.383496642112732, 1.3932656049728394, 1.4038692712783813, 1.3804820775985718, 1.4356403350830078, 1.3950107097625732, 1.386986494064331, 1.4037675857543945, 1.3838016986846924, 1.3941705226898193, 1.390470027923584, 1.3855960369110107, 1.3899353742599487, 1.3836159706115723, 1.4023725986480713, 1.3830677270889282, 1.4212896823883057, 1.3819347620010376, 1.4010624885559082, 1.3674511909484863, 1.393012523651123, 1.4355045557022095, 1.3915807008743286, 1.3786635398864746, 1.416192889213562, 1.4453296661376953, 1.3985635042190552, 1.382617473602295, 1.3928371667861938, 1.4058327674865723, 1.39598548412323, 1.3842086791992188, 1.3830311298370361, 1.4167907238006592, 1.3794292211532593, 1.3961360454559326, 1.3868905305862427, 1.3815526962280273, 1.3963409662246704, 1.4110740423202515, 1.4135819673538208, 1.4194952249526978, 1.405195951461792, 1.387094497680664, 1.4035940170288086, 1.4034682512283325, 1.4024771451950073, 1.403292179107666, 1.3907132148742676, 1.3974509239196777, 1.4205503463745117, 1.4246042966842651, 1.410507321357727, 1.3931708335876465, 1.4162511825561523, 1.4520337581634521, 1.401615023612976, 1.403865933418274, 1.4087332487106323, 1.400471806526184, 1.4033823013305664, 1.407352328300476, 1.4117788076400757, 1.412503719329834, 1.4137259721755981, 1.3945153951644897, 1.4020724296569824, 1.4106528759002686, 1.3997520208358765, 1.4684454202651978, 1.4392532110214233, 1.4373501539230347, 1.4362876415252686, 1.4108121395111084, 1.4246574640274048, 1.4280682802200317, 1.4173169136047363, 1.4418812990188599, 1.39671790599823, 1.4109913110733032, 1.4227339029312134, 1.4225552082061768, 1.3935258388519287, 1.4074956178665161, 1.3988593816757202, 1.4332941770553589, 1.4361821413040161, 1.426930546760559, 1.4008865356445312, 1.4076616764068604, 1.404669165611267, 1.4045614004135132, 1.4137691259384155, 1.4159284830093384, 1.4412070512771606, 1.4229813814163208, 1.4075713157653809, 1.4336689710617065, 1.4030635356903076, 1.4121791124343872, 1.4428694248199463, 1.4219201803207397, 1.41787588596344, 1.4142284393310547, 1.4458107948303223, 1.427240252494812, 1.4318796396255493, 1.4263770580291748, 1.4180039167404175, 1.4125077724456787, 1.425089955329895, 1.415372610092163, 1.4559940099716187, 1.4145604372024536, 1.5258362293243408, 1.4410791397094727, 1.4652535915374756, 1.4683643579483032, 1.4082396030426025, 1.4314342737197876, 1.4610276222229004, 1.4712247848510742, 1.4451313018798828, 1.4265186786651611, 1.4729975461959839, 1.4275826215744019, 1.4282582998275757, 1.4280569553375244, 1.46499502658844, 1.4536687135696411, 1.4430800676345825, 1.417921543121338, 1.4267419576644897, 1.4419150352478027, 1.4377397298812866, 1.4462658166885376, 1.439978837966919, 1.4911627769470215, 1.444564938545227, 1.4365150928497314, 1.4445542097091675, 1.4380972385406494, 1.440148949623108, 1.4327363967895508, 1.4607367515563965, 1.4354513883590698, 1.4383491277694702, 1.4207731485366821, 1.4550718069076538, 1.4347772598266602, 1.439842939376831, 1.4329323768615723, 1.4798301458358765, 1.4449511766433716, 1.4551023244857788, 1.5006186962127686, 1.4462957382202148, 1.4323813915252686, 1.4350258111953735, 1.4611287117004395, 1.4795588254928589, 1.472466230392456, 1.4859670400619507, 1.4961515665054321, 1.4585293531417847, 1.4564900398254395, 1.453092098236084, 1.4683659076690674, 1.4623899459838867, 1.4627265930175781, 1.4849649667739868, 1.474764347076416, 1.4482908248901367, 1.4736199378967285, 1.5322753190994263, 1.4668233394622803, 1.4639942646026611, 1.4509186744689941, 1.449389100074768, 1.4612842798233032, 1.4904961585998535, 1.4867215156555176, 1.4931190013885498, 1.4678312540054321, 1.4646031856536865, 1.4720631837844849, 1.4520269632339478, 1.4772087335586548, 1.453126072883606, 1.4806296825408936, 1.4757013320922852, 1.4765675067901611, 1.4762667417526245, 1.4812395572662354, 1.4818038940429688, 1.4790832996368408, 1.4723187685012817, 1.4756793975830078, 1.4855031967163086, 1.4946627616882324, 1.5118424892425537, 1.4795506000518799, 1.5074782371520996, 1.4694738388061523, 1.4661909341812134, 1.4718245267868042, 1.4664093255996704, 1.5017616748809814, 1.5042117834091187, 1.4686813354492188, 1.4747616052627563, 1.4786463975906372, 1.4831737279891968, 1.4832261800765991, 1.499613881111145, 1.4961178302764893, 1.5526279211044312, 1.5178781747817993, 1.4772316217422485, 1.4735673666000366, 1.4993849992752075, 1.498350739479065, 1.4710719585418701, 1.488768458366394, 1.5175029039382935, 1.5104776620864868, 1.523282766342163, 1.4997049570083618, 1.5020008087158203, 1.4984480142593384, 1.4896783828735352, 1.5768636465072632, 1.4997586011886597, 1.514414668083191, 1.4958659410476685, 1.500145673751831, 1.5504629611968994, 1.5259068012237549, 1.5164657831192017, 1.5023959875106812, 1.54074227809906, 1.5124034881591797, 1.548393726348877, 1.5218263864517212, 1.5439091920852661, 1.5825237035751343, 1.4935859441757202, 1.5707542896270752, 1.5328482389450073, 1.503785490989685, 1.5148060321807861, 1.5116581916809082, 1.4996830224990845, 1.5288444757461548, 1.5348583459854126, 1.5036298036575317, 1.5397770404815674, 1.5315290689468384, 1.5325261354446411, 1.5116541385650635, 1.5023452043533325, 1.5891392230987549, 1.5207185745239258, 1.5341873168945312, 1.5038583278656006, 1.5519633293151855, 1.5443534851074219, 1.5539822578430176, 1.5341746807098389, 1.5443412065505981, 1.5043827295303345, 1.574108600616455, 1.595080852508545, 1.5467004776000977, 1.5699033737182617, 1.532356858253479, 1.5271251201629639, 1.562256932258606, 1.5245119333267212, 1.5521385669708252, 1.5421607494354248, 1.6064963340759277, 1.5610119104385376, 1.5362759828567505, 1.5361504554748535, 1.606844425201416, 1.59318208694458, 1.563660740852356, 1.5655312538146973, 1.5826212167739868, 1.5375415086746216, 1.5515702962875366, 1.5627533197402954, 1.570180892944336, 1.5627846717834473, 1.5493642091751099, 1.5553656816482544, 1.5590265989303589, 1.6022762060165405, 1.5601128339767456, 1.5727442502975464, 1.5988285541534424, 1.5947329998016357, 1.5806629657745361, 1.5949032306671143, 1.6288068294525146, 1.599552869796753, 1.58392333984375, 1.596875548362732, 1.58778715133667, 1.5938479900360107, 1.5558662414550781, 1.5833346843719482, 1.5701981782913208, 1.5828776359558105, 1.5762726068496704, 1.568200945854187, 1.6200600862503052, 1.6010863780975342, 1.6311732530593872, 1.5942074060440063, 1.5945444107055664, 1.5839884281158447, 1.5726819038391113, 1.566670298576355, 1.6148024797439575, 1.5715916156768799, 1.596582293510437, 1.6044588088989258, 1.601541519165039, 1.6226660013198853, 1.6713184118270874, 1.5807298421859741, 1.5967471599578857, 1.5938482284545898, 1.5901775360107422, 1.5973150730133057, 1.6159145832061768, 1.599508285522461, 1.6496086120605469, 1.6146646738052368, 1.6424278020858765, 1.6032235622406006, 1.6462366580963135, 1.6124709844589233, 1.603630542755127, 1.6053166389465332, 1.6133663654327393, 1.6175510883331299, 1.6859002113342285, 1.6141431331634521, 1.6765282154083252, 1.598190426826477, 1.6313308477401733, 1.6253749132156372, 1.6245090961456299, 1.6312432289123535, 1.6415166854858398, 1.6140661239624023, 1.6397819519042969, 1.6520476341247559, 1.6332095861434937, 1.659088134765625, 1.6480891704559326, 1.6289275884628296, 1.6522539854049683, 1.627293348312378, 1.6468921899795532, 1.6507967710494995, 1.6660757064819336, 1.6361465454101562, 1.6907694339752197, 1.6654388904571533, 1.6548844575881958, 1.643141269683838, 1.6672474145889282, 1.68805992603302, 1.6707241535186768, 1.657253384590149, 1.7110577821731567, 1.7358275651931763, 1.6853957176208496, 1.6687949895858765, 1.7110928297042847, 1.696095585823059, 1.714759111404419, 1.6737734079360962, 1.6701009273529053, 1.7416845560073853, 1.6882405281066895, 1.6830909252166748, 1.7151485681533813, 1.6567821502685547, 1.7498223781585693, 1.753359317779541, 1.7199780941009521, 1.742499589920044, 1.6940120458602905, 1.6844819784164429, 1.6668636798858643, 1.6829805374145508, 1.6796683073043823, 1.7644046545028687, 1.7313587665557861, 1.7148478031158447, 1.6867148876190186, 1.7009100914001465, 1.7071871757507324, 1.7108851671218872, 1.6790448427200317, 1.7208634614944458, 1.792815089225769, 1.7152581214904785, 1.6972486972808838, 1.727113127708435, 1.7081780433654785, 1.7302472591400146, 1.706342101097107, 1.7541426420211792, 1.741321086883545, 1.7644094228744507, 1.7129411697387695, 1.7201879024505615, 1.7196199893951416, 1.7700259685516357, 1.7923580408096313, 1.7288612127304077, 1.7487051486968994, 1.75513756275177, 1.7348395586013794, 1.7409712076187134, 1.7271728515625, 1.7201491594314575, 1.7315024137496948, 1.7605453729629517, 1.747763991355896, 1.7892807722091675, 1.7224454879760742, 1.791244626045227, 1.7532188892364502, 1.8037925958633423, 1.7238073348999023, 1.7268379926681519, 1.7471559047698975, 1.7721954584121704, 1.7895718812942505, 1.7697395086288452, 1.7240815162658691, 1.722723126411438, 1.7624579668045044, 1.7389942407608032, 1.7782455682754517, 1.7599468231201172, 1.745550513267517, 1.75161612033844, 1.7758783102035522, 1.854695200920105, 1.7704756259918213, 1.8019754886627197, 1.7683392763137817, 1.799453616142273, 1.762166976928711, 1.7647901773452759, 1.7768833637237549, 1.7972872257232666, 1.7518504858016968, 1.8980021476745605, 1.755743145942688, 1.7913792133331299, 1.7884025573730469, 1.8264387845993042, 1.7997864484786987, 1.792120099067688, 1.7555971145629883, 1.866389513015747, 1.7869216203689575, 1.8205479383468628, 1.8416131734848022, 1.831039547920227, 1.7983248233795166, 1.902980923652649, 1.8031589984893799, 1.798896312713623, 1.8388900756835938, 1.8449926376342773, 1.8187260627746582, 1.8004649877548218, 1.8023637533187866, 1.8675228357315063, 1.8126485347747803, 1.8959574699401855, 1.8733150959014893, 1.8342446088790894, 1.8187624216079712, 1.8686736822128296, 1.8295775651931763, 1.8635882139205933, 1.849873423576355, 1.8073993921279907, 1.8287103176116943, 1.8328505754470825, 1.8090291023254395, 1.8264238834381104, 1.8453563451766968, 1.8243767023086548, 1.8788001537322998, 1.8473429679870605, 1.8469393253326416, 1.8456171751022339, 1.8318498134613037, 1.9226270914077759, 1.844907283782959, 1.8785887956619263, 1.8534873723983765, 1.8887073993682861, 1.994413137435913, 1.8575983047485352, 1.8737393617630005, 1.8590999841690063, 1.8515421152114868, 1.8624745607376099, 1.8446847200393677, 1.8856269121170044, 1.8703480958938599, 1.8813250064849854, 1.8671470880508423, 1.8870000839233398, 1.9174973964691162, 1.9113317728042603, 1.9005472660064697, 1.95849609375, 1.9185810089111328, 1.9469654560089111, 1.8800941705703735, 1.8622463941574097, 1.9053698778152466, 1.890097975730896, 1.9131752252578735], 'val_accuracy': [0.16931216418743134, 0.16931216418743134, 0.18518517911434174, 0.2063492089509964, 0.17989417910575867, 0.2222222238779068, 0.190476194024086, 0.22751322388648987, 0.16402116417884827, 0.1746031790971756, 0.19576719403266907, 0.20105819404125214, 0.17989417910575867, 0.2063492089509964, 0.190476194024086, 0.23280423879623413, 0.2063492089509964, 0.21693122386932373, 0.20105819404125214, 0.24867725372314453, 0.23280423879623413, 0.20105819404125214, 0.22751322388648987, 0.2539682686328888, 0.2380952388048172, 0.20105819404125214, 0.2539682686328888, 0.2063492089509964, 0.22751322388648987, 0.2222222238779068, 0.21693122386932373, 0.23280423879623413, 0.25925925374031067, 0.24338623881340027, 0.23280423879623413, 0.2857142984867096, 0.18518517911434174, 0.26455026865005493, 0.26455026865005493, 0.27513226866722107, 0.29629629850387573, 0.26455026865005493, 0.21693122386932373, 0.2380952388048172, 0.22751322388648987, 0.29100528359413147, 0.26455026865005493, 0.2698412835597992, 0.25925925374031067, 0.27513226866722107, 0.32275131344795227, 0.2857142984867096, 0.2857142984867096, 0.31216931343078613, 0.2857142984867096, 0.3174603283405304, 0.3333333432674408, 0.31216931343078613, 0.3174603283405304, 0.31216931343078613, 0.30687829852104187, 0.31216931343078613, 0.29629629850387573, 0.31216931343078613, 0.30687829852104187, 0.30687829852104187, 0.3333333432674408, 0.3492063581943512, 0.35449734330177307, 0.3333333432674408, 0.34391534328460693, 0.33862432837486267, 0.32804232835769653, 0.3333333432674408, 0.33862432837486267, 0.33862432837486267, 0.38624337315559387, 0.3492063581943512, 0.37037035822868347, 0.37037035822868347, 0.34391534328460693, 0.380952388048172, 0.3174603283405304, 0.3650793731212616, 0.3968254029750824, 0.35449734330177307, 0.3650793731212616, 0.3650793731212616, 0.3650793731212616, 0.38624337315559387, 0.37566137313842773, 0.35449734330177307, 0.380952388048172, 0.37037035822868347, 0.37037035822868347, 0.3650793731212616, 0.42328041791915894, 0.380952388048172, 0.37566137313842773, 0.38624337315559387, 0.3968254029750824, 0.3492063581943512, 0.4021163880825043, 0.380952388048172, 0.42328041791915894, 0.37037035822868347, 0.3968254029750824, 0.37037035822868347, 0.40740740299224854, 0.380952388048172, 0.42328041791915894, 0.41798943281173706, 0.37566137313842773, 0.39153438806533813, 0.39153438806533813, 0.38624337315559387, 0.39153438806533813, 0.37037035822868347, 0.39153438806533813, 0.41798943281173706, 0.42328041791915894, 0.380952388048172, 0.4126984179019928, 0.3968254029750824, 0.3968254029750824, 0.37566137313842773, 0.39153438806533813, 0.39153438806533813, 0.37566137313842773, 0.4021163880825043, 0.39153438806533813, 0.41798943281173706, 0.37037035822868347, 0.4126984179019928, 0.4126984179019928, 0.3968254029750824, 0.35978835821151733, 0.4021163880825043, 0.4021163880825043, 0.4285714328289032, 0.38624337315559387, 0.37037035822868347, 0.42328041791915894, 0.4126984179019928, 0.40740740299224854, 0.40740740299224854, 0.42328041791915894, 0.38624337315559387, 0.41798943281173706, 0.41798943281173706, 0.38624337315559387, 0.41798943281173706, 0.3968254029750824, 0.40740740299224854, 0.3968254029750824, 0.37037035822868347, 0.43915343284606934, 0.39153438806533813, 0.4285714328289032, 0.4285714328289032, 0.460317462682724, 0.4285714328289032, 0.4021163880825043, 0.4126984179019928, 0.40740740299224854, 0.40740740299224854, 0.4285714328289032, 0.4285714328289032, 0.4021163880825043, 0.43386244773864746, 0.40740740299224854, 0.4444444477558136, 0.44973546266555786, 0.3968254029750824, 0.4126984179019928, 0.43386244773864746, 0.44973546266555786, 0.380952388048172, 0.4285714328289032, 0.3968254029750824, 0.4021163880825043, 0.42328041791915894, 0.43386244773864746, 0.43915343284606934, 0.43915343284606934, 0.4285714328289032, 0.3968254029750824, 0.44973546266555786, 0.42328041791915894, 0.4444444477558136, 0.42328041791915894, 0.43915343284606934, 0.41798943281173706, 0.40740740299224854, 0.41798943281173706, 0.4021163880825043, 0.4285714328289032, 0.41798943281173706, 0.42328041791915894, 0.460317462682724, 0.44973546266555786, 0.43915343284606934, 0.40740740299224854, 0.4126984179019928, 0.40740740299224854, 0.460317462682724, 0.43915343284606934, 0.41798943281173706, 0.43915343284606934, 0.4285714328289032, 0.4285714328289032, 0.43386244773864746, 0.43386244773864746, 0.46560847759246826, 0.43915343284606934, 0.42328041791915894, 0.41798943281173706, 0.41798943281173706, 0.4126984179019928, 0.4444444477558136, 0.4285714328289032, 0.43386244773864746, 0.43386244773864746, 0.4444444477558136, 0.41798943281173706, 0.42328041791915894, 0.4285714328289032, 0.42328041791915894, 0.460317462682724, 0.44973546266555786, 0.45502644777297974, 0.43386244773864746, 0.41798943281173706, 0.3968254029750824, 0.41798943281173706, 0.4444444477558136, 0.43386244773864746, 0.4285714328289032, 0.43915343284606934, 0.43915343284606934, 0.44973546266555786, 0.43915343284606934, 0.43915343284606934, 0.43386244773864746, 0.42328041791915894, 0.43915343284606934, 0.4285714328289032, 0.4285714328289032, 0.45502644777297974, 0.44973546266555786, 0.43386244773864746, 0.42328041791915894, 0.43386244773864746, 0.42328041791915894, 0.4444444477558136, 0.45502644777297974, 0.44973546266555786, 0.4444444477558136, 0.45502644777297974, 0.43386244773864746, 0.4285714328289032, 0.4444444477558136, 0.4444444477558136, 0.45502644777297974, 0.4444444477558136, 0.4444444477558136, 0.46560847759246826, 0.43915343284606934, 0.43386244773864746, 0.4444444477558136, 0.4285714328289032, 0.43386244773864746, 0.4285714328289032, 0.43386244773864746, 0.4285714328289032, 0.43915343284606934, 0.43386244773864746, 0.4126984179019928, 0.4444444477558136, 0.45502644777297974, 0.4444444477558136, 0.4285714328289032, 0.4444444477558136, 0.44973546266555786, 0.43386244773864746, 0.4285714328289032, 0.46560847759246826, 0.43915343284606934, 0.43386244773864746, 0.43386244773864746, 0.4761904776096344, 0.45502644777297974, 0.46560847759246826, 0.46560847759246826, 0.4444444477558136, 0.4444444477558136, 0.45502644777297974, 0.46560847759246826, 0.4444444477558136, 0.4444444477558136, 0.45502644777297974, 0.4126984179019928, 0.43915343284606934, 0.43386244773864746, 0.44973546266555786, 0.460317462682724, 0.4285714328289032, 0.4444444477558136, 0.460317462682724, 0.44973546266555786, 0.4285714328289032, 0.42328041791915894, 0.44973546266555786, 0.43386244773864746, 0.43915343284606934, 0.43386244773864746, 0.4285714328289032, 0.4285714328289032, 0.4444444477558136, 0.44973546266555786, 0.43386244773864746, 0.44973546266555786, 0.43915343284606934, 0.4285714328289032, 0.43915343284606934, 0.43915343284606934, 0.45502644777297974, 0.41798943281173706, 0.45502644777297974, 0.43915343284606934, 0.43915343284606934, 0.43915343284606934, 0.4444444477558136, 0.4126984179019928, 0.45502644777297974, 0.44973546266555786, 0.44973546266555786, 0.44973546266555786, 0.4285714328289032, 0.43915343284606934, 0.460317462682724, 0.41798943281173706, 0.4444444477558136, 0.45502644777297974, 0.46560847759246826, 0.4126984179019928, 0.43915343284606934, 0.43915343284606934, 0.43915343284606934, 0.460317462682724, 0.44973546266555786, 0.41798943281173706, 0.44973546266555786, 0.44973546266555786, 0.4444444477558136, 0.44973546266555786, 0.45502644777297974, 0.4285714328289032, 0.44973546266555786, 0.47089946269989014, 0.460317462682724, 0.47089946269989014, 0.4444444477558136, 0.4285714328289032, 0.4761904776096344, 0.4444444477558136, 0.45502644777297974, 0.43915343284606934, 0.4761904776096344, 0.42328041791915894, 0.40740740299224854, 0.4444444477558136, 0.43386244773864746, 0.45502644777297974, 0.43915343284606934, 0.4285714328289032, 0.4285714328289032, 0.41798943281173706, 0.44973546266555786, 0.4761904776096344, 0.4285714328289032, 0.45502644777297974, 0.44973546266555786, 0.4761904776096344, 0.45502644777297974, 0.46560847759246826, 0.4444444477558136, 0.460317462682724, 0.47089946269989014, 0.43915343284606934, 0.44973546266555786, 0.47089946269989014, 0.44973546266555786, 0.460317462682724, 0.45502644777297974, 0.44973546266555786, 0.47089946269989014, 0.42328041791915894, 0.48148149251937866, 0.4761904776096344, 0.43386244773864746, 0.45502644777297974, 0.45502644777297974, 0.48148149251937866, 0.47089946269989014, 0.46560847759246826, 0.45502644777297974, 0.4444444477558136, 0.4761904776096344, 0.48148149251937866, 0.4761904776096344, 0.42328041791915894, 0.47089946269989014, 0.47089946269989014, 0.46560847759246826, 0.45502644777297974, 0.4444444477558136, 0.46560847759246826, 0.41798943281173706, 0.460317462682724, 0.460317462682724, 0.43386244773864746, 0.45502644777297974, 0.45502644777297974, 0.4444444477558136, 0.45502644777297974, 0.4761904776096344, 0.460317462682724, 0.4285714328289032, 0.4761904776096344, 0.4444444477558136, 0.46560847759246826, 0.460317462682724, 0.46560847759246826, 0.45502644777297974, 0.47089946269989014, 0.47089946269989014, 0.43386244773864746, 0.48677247762680054, 0.47089946269989014, 0.4285714328289032, 0.48148149251937866, 0.460317462682724, 0.48148149251937866, 0.44973546266555786, 0.46560847759246826, 0.47089946269989014, 0.460317462682724, 0.46560847759246826, 0.460317462682724, 0.4444444477558136, 0.48677247762680054, 0.47089946269989014, 0.4444444477558136, 0.48148149251937866, 0.4920634925365448, 0.48677247762680054, 0.45502644777297974, 0.48148149251937866, 0.48148149251937866, 0.47089946269989014, 0.46560847759246826, 0.460317462682724, 0.460317462682724, 0.47089946269989014, 0.48677247762680054, 0.460317462682724, 0.46560847759246826, 0.48148149251937866, 0.48148149251937866, 0.49735450744628906, 0.46560847759246826, 0.44973546266555786, 0.460317462682724, 0.48148149251937866, 0.460317462682724, 0.4920634925365448, 0.4761904776096344, 0.460317462682724, 0.48148149251937866, 0.45502644777297974, 0.4920634925365448, 0.48677247762680054, 0.48677247762680054, 0.4920634925365448, 0.460317462682724, 0.4761904776096344, 0.43915343284606934, 0.48148149251937866, 0.44973546266555786, 0.48677247762680054, 0.47089946269989014, 0.45502644777297974, 0.48148149251937866, 0.48677247762680054, 0.44973546266555786, 0.44973546266555786, 0.47089946269989014, 0.47089946269989014, 0.46560847759246826, 0.47089946269989014, 0.47089946269989014, 0.45502644777297974, 0.49735450744628906, 0.4761904776096344, 0.4920634925365448, 0.4761904776096344, 0.48677247762680054, 0.4920634925365448, 0.4920634925365448, 0.47089946269989014, 0.46560847759246826, 0.43915343284606934, 0.45502644777297974, 0.48148149251937866, 0.48148149251937866, 0.46560847759246826, 0.44973546266555786, 0.4761904776096344, 0.47089946269989014, 0.4761904776096344, 0.45502644777297974, 0.46560847759246826, 0.47089946269989014, 0.47089946269989014, 0.48677247762680054, 0.43915343284606934, 0.4761904776096344, 0.48677247762680054, 0.47089946269989014, 0.4761904776096344, 0.47089946269989014, 0.48148149251937866, 0.48148149251937866, 0.48148149251937866, 0.45502644777297974, 0.48677247762680054, 0.48677247762680054, 0.46560847759246826, 0.48677247762680054, 0.45502644777297974, 0.46560847759246826, 0.460317462682724, 0.43915343284606934, 0.46560847759246826, 0.46560847759246826, 0.460317462682724, 0.48148149251937866, 0.4761904776096344, 0.47089946269989014, 0.47089946269989014, 0.47089946269989014, 0.47089946269989014, 0.48677247762680054, 0.48148149251937866, 0.48677247762680054, 0.47089946269989014, 0.48148149251937866, 0.47089946269989014, 0.4761904776096344, 0.48148149251937866, 0.48148149251937866, 0.4761904776096344, 0.46560847759246826, 0.4761904776096344, 0.45502644777297974, 0.4761904776096344, 0.47089946269989014, 0.46560847759246826, 0.47089946269989014, 0.460317462682724, 0.460317462682724, 0.460317462682724, 0.4920634925365448, 0.47089946269989014, 0.47089946269989014, 0.46560847759246826, 0.47089946269989014, 0.4761904776096344, 0.45502644777297974, 0.48677247762680054, 0.4761904776096344, 0.47089946269989014, 0.43386244773864746, 0.4761904776096344, 0.4920634925365448, 0.45502644777297974, 0.4761904776096344, 0.47089946269989014, 0.48148149251937866, 0.47089946269989014, 0.48148149251937866, 0.48677247762680054, 0.460317462682724, 0.46560847759246826, 0.46560847759246826, 0.4761904776096344, 0.48148149251937866, 0.4761904776096344, 0.45502644777297974, 0.46560847759246826, 0.48148149251937866, 0.47089946269989014, 0.47089946269989014, 0.48148149251937866, 0.47089946269989014, 0.47089946269989014, 0.47089946269989014, 0.47089946269989014, 0.48677247762680054, 0.4761904776096344, 0.47089946269989014, 0.48677247762680054, 0.4761904776096344, 0.47089946269989014, 0.48148149251937866, 0.48148149251937866, 0.4761904776096344, 0.46560847759246826, 0.4761904776096344, 0.4920634925365448, 0.48148149251937866, 0.48148149251937866, 0.4761904776096344, 0.47089946269989014, 0.4761904776096344, 0.48148149251937866, 0.47089946269989014, 0.4761904776096344, 0.48677247762680054, 0.4761904776096344, 0.4761904776096344, 0.45502644777297974, 0.460317462682724, 0.46560847759246826, 0.46560847759246826, 0.43915343284606934, 0.48148149251937866, 0.4761904776096344, 0.47089946269989014, 0.4761904776096344, 0.47089946269989014, 0.460317462682724, 0.4761904776096344, 0.4761904776096344, 0.46560847759246826, 0.4761904776096344, 0.48148149251937866, 0.4761904776096344, 0.4920634925365448, 0.47089946269989014, 0.46560847759246826, 0.45502644777297974, 0.49735450744628906, 0.48148149251937866, 0.48148149251937866, 0.47089946269989014, 0.48677247762680054, 0.46560847759246826, 0.49735450744628906, 0.47089946269989014, 0.4761904776096344, 0.47089946269989014, 0.460317462682724, 0.47089946269989014, 0.460317462682724, 0.4761904776096344, 0.48148149251937866, 0.48677247762680054, 0.47089946269989014, 0.46560847759246826, 0.48677247762680054, 0.4761904776096344, 0.47089946269989014, 0.48148149251937866, 0.48148149251937866, 0.4761904776096344, 0.4920634925365448, 0.4920634925365448, 0.4920634925365448, 0.4920634925365448, 0.4920634925365448, 0.4920634925365448, 0.48148149251937866, 0.48677247762680054, 0.45502644777297974, 0.48677247762680054, 0.44973546266555786, 0.4761904776096344, 0.4761904776096344, 0.48677247762680054, 0.47089946269989014, 0.4920634925365448, 0.48677247762680054, 0.47089946269989014, 0.44973546266555786, 0.48677247762680054, 0.44973546266555786, 0.4920634925365448, 0.47089946269989014, 0.4761904776096344, 0.48148149251937866, 0.44973546266555786, 0.49735450744628906, 0.46560847759246826, 0.4761904776096344, 0.4761904776096344, 0.46560847759246826, 0.46560847759246826, 0.4761904776096344, 0.48677247762680054, 0.48677247762680054, 0.49735450744628906, 0.48148149251937866, 0.45502644777297974, 0.4761904776096344, 0.48677247762680054, 0.48148149251937866, 0.47089946269989014, 0.49735450744628906, 0.4761904776096344, 0.46560847759246826, 0.4920634925365448, 0.47089946269989014, 0.47089946269989014, 0.45502644777297974, 0.47089946269989014, 0.48148149251937866, 0.45502644777297974, 0.4920634925365448, 0.48148149251937866, 0.4920634925365448, 0.4761904776096344, 0.4920634925365448, 0.5079365372657776, 0.4920634925365448, 0.47089946269989014, 0.48148149251937866, 0.4761904776096344, 0.47089946269989014, 0.4920634925365448, 0.49735450744628906, 0.5132275223731995, 0.48148149251937866, 0.49735450744628906, 0.48148149251937866, 0.49735450744628906, 0.4920634925365448, 0.460317462682724, 0.4920634925365448, 0.48148149251937866, 0.45502644777297974, 0.4920634925365448, 0.4920634925365448, 0.48148149251937866, 0.48677247762680054, 0.460317462682724, 0.48677247762680054, 0.46560847759246826, 0.47089946269989014, 0.48148149251937866, 0.5079365372657776, 0.45502644777297974, 0.46560847759246826, 0.48148149251937866, 0.48148149251937866, 0.45502644777297974, 0.5026454925537109, 0.4761904776096344, 0.48148149251937866, 0.48677247762680054, 0.46560847759246826, 0.4761904776096344, 0.4920634925365448, 0.4920634925365448, 0.4920634925365448, 0.4761904776096344, 0.5132275223731995, 0.46560847759246826, 0.5132275223731995, 0.49735450744628906, 0.45502644777297974, 0.49735450744628906, 0.48677247762680054, 0.4761904776096344, 0.47089946269989014, 0.46560847759246826, 0.460317462682724, 0.45502644777297974, 0.4761904776096344, 0.460317462682724, 0.4920634925365448, 0.4761904776096344, 0.4761904776096344, 0.48148149251937866, 0.4761904776096344, 0.45502644777297974, 0.48148149251937866, 0.47089946269989014, 0.48148149251937866, 0.5026454925537109, 0.45502644777297974, 0.49735450744628906, 0.48677247762680054, 0.48677247762680054, 0.4761904776096344, 0.4920634925365448, 0.4920634925365448, 0.4920634925365448, 0.47089946269989014, 0.523809552192688, 0.46560847759246826, 0.5132275223731995, 0.46560847759246826, 0.460317462682724, 0.48148149251937866, 0.46560847759246826, 0.45502644777297974, 0.49735450744628906, 0.4920634925365448, 0.5026454925537109, 0.48148149251937866, 0.4761904776096344, 0.4920634925365448, 0.46560847759246826, 0.460317462682724, 0.4761904776096344, 0.5026454925537109, 0.45502644777297974, 0.4444444477558136, 0.4761904776096344, 0.49735450744628906, 0.4761904776096344, 0.49735450744628906, 0.47089946269989014, 0.48148149251937866, 0.4761904776096344, 0.4761904776096344, 0.4761904776096344, 0.4920634925365448, 0.48148149251937866, 0.460317462682724, 0.46560847759246826, 0.4761904776096344, 0.46560847759246826, 0.49735450744628906, 0.48148149251937866, 0.48677247762680054, 0.460317462682724, 0.5079365372657776, 0.48677247762680054, 0.5079365372657776, 0.48148149251937866, 0.48148149251937866, 0.4761904776096344, 0.4761904776096344, 0.48677247762680054, 0.47089946269989014, 0.48677247762680054, 0.4761904776096344, 0.46560847759246826, 0.48148149251937866, 0.47089946269989014, 0.46560847759246826, 0.48677247762680054, 0.47089946269989014, 0.48148149251937866, 0.48677247762680054, 0.5079365372657776, 0.48677247762680054, 0.4920634925365448, 0.4761904776096344, 0.47089946269989014, 0.49735450744628906, 0.48677247762680054, 0.49735450744628906, 0.4761904776096344, 0.48677247762680054, 0.4761904776096344, 0.49735450744628906, 0.48148149251937866, 0.48148149251937866, 0.47089946269989014, 0.48148149251937866, 0.44973546266555786, 0.46560847759246826, 0.4920634925365448, 0.46560847759246826, 0.49735450744628906, 0.47089946269989014, 0.48148149251937866, 0.48148149251937866, 0.460317462682724, 0.46560847759246826, 0.49735450744628906, 0.4920634925365448, 0.4920634925365448, 0.47089946269989014, 0.48677247762680054, 0.46560847759246826, 0.48677247762680054, 0.48677247762680054, 0.48148149251937866, 0.47089946269989014, 0.4761904776096344, 0.4761904776096344, 0.5132275223731995, 0.48148149251937866, 0.48677247762680054, 0.48677247762680054, 0.5026454925537109, 0.4920634925365448, 0.4761904776096344, 0.48677247762680054, 0.4761904776096344, 0.48148149251937866, 0.4920634925365448, 0.49735450744628906, 0.49735450744628906, 0.5026454925537109, 0.48677247762680054, 0.48148149251937866, 0.44973546266555786, 0.4920634925365448, 0.5026454925537109, 0.48677247762680054, 0.49735450744628906, 0.46560847759246826, 0.49735450744628906, 0.4920634925365448, 0.48148149251937866, 0.4761904776096344, 0.46560847759246826, 0.48677247762680054, 0.4920634925365448, 0.4920634925365448, 0.48677247762680054, 0.4761904776096344, 0.4761904776096344, 0.4920634925365448, 0.4920634925365448, 0.49735450744628906, 0.5026454925537109, 0.4761904776096344, 0.4761904776096344, 0.4761904776096344, 0.4761904776096344, 0.5079365372657776, 0.4761904776096344, 0.460317462682724, 0.46560847759246826, 0.4920634925365448, 0.49735450744628906, 0.4920634925365448, 0.48148149251937866, 0.4920634925365448, 0.46560847759246826, 0.4761904776096344, 0.48148149251937866, 0.48148149251937866, 0.4761904776096344, 0.47089946269989014, 0.48677247762680054, 0.48148149251937866, 0.4920634925365448, 0.4761904776096344, 0.48677247762680054, 0.49735450744628906, 0.49735450744628906, 0.4761904776096344, 0.4920634925365448, 0.48148149251937866, 0.48148149251937866, 0.48677247762680054, 0.48148149251937866, 0.4920634925365448, 0.4761904776096344, 0.48677247762680054, 0.48677247762680054, 0.48677247762680054, 0.4920634925365448, 0.4761904776096344, 0.48148149251937866, 0.49735450744628906, 0.49735450744628906, 0.48148149251937866, 0.48677247762680054, 0.47089946269989014, 0.48677247762680054, 0.48677247762680054, 0.48148149251937866, 0.4920634925365448, 0.49735450744628906, 0.48677247762680054, 0.48148149251937866, 0.46560847759246826, 0.4761904776096344, 0.4761904776096344, 0.5026454925537109, 0.4920634925365448, 0.4761904776096344, 0.46560847759246826, 0.4761904776096344, 0.48677247762680054, 0.4761904776096344, 0.48148149251937866, 0.5079365372657776, 0.47089946269989014, 0.48677247762680054]}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mratio\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mratio\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlogical\u001b[0m \u001b[0mto\u001b[0m \u001b[0mphysical\u001b[0m \u001b[0mpixels\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(figure, print_method)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_without_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_non_interactive_terminal_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mMetadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlatin\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mencodable\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mAccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mspecification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mshorter\u001b[0m \u001b[0mthan\u001b[0m \u001b[0;36m79\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_png' from 'matplotlib' (/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#sigmoid\n",
        "print(cnnhistory.history)\n",
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1Hgr-xHsHGJ"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h0oVSxMcsOBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfd62e8-0829-4850-aafe-a2ff9f795f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved trained model at /content/drive/My Drive/audio-dataset/Audio-dataset/Rawdata/../saved_models/Emotion_Voice_Detection_Model.h5 \n"
          ]
        }
      ],
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = os.path.join(os.getcwd(), \"../saved_models\")\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Predicting emotions on the test data"
      ],
      "metadata": {
        "id": "rQgvfkq9s_-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_testcnn, \n",
        "                         batch_size=32, \n",
        "                         verbose=1)\n",
        "\n",
        "preds\n",
        "\n",
        "preds1 = preds.argmax(axis=1)\n",
        "\n",
        "preds1\n",
        "\n",
        "abc = preds.astype(int).flatten()\n",
        "\n",
        "predictions = (lb.inverse_transform((abc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KjULKwstJ0s",
        "outputId": "0a14f309-11b0-4e6a-b70a-953276009e33"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
        "print(preddf[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWCxMScrt9RG",
        "outputId": "b6c9fa55-4980-423b-8f13-82a9b47bd4c2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  predictedvalues\n",
            "0    female_angry\n",
            "1    female_angry\n",
            "2    female_angry\n",
            "3    female_angry\n",
            "4    female_angry\n",
            "5    female_angry\n",
            "6    female_angry\n",
            "7    female_angry\n",
            "8    female_angry\n",
            "9    female_angry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual=y_test.argmax(axis=1)\n",
        "abc123 = actual.astype(int).flatten()\n",
        "actualvalues = (lb.inverse_transform((abc123)))\n",
        "\n",
        "actualdf = pd.DataFrame({'actualvalues': actualvalues})\n",
        "print(actualdf[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejkmMC-SuTkf",
        "outputId": "501937d0-7a15-4bb4-c149-a2e1e375b7c8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     actualvalues\n",
            "0      male_angry\n",
            "1  female_fearful\n",
            "2      female_sad\n",
            "3    female_happy\n",
            "4    female_angry\n",
            "5        male_sad\n",
            "6  female_fearful\n",
            "7        male_sad\n",
            "8      male_happy\n",
            "9     female_calm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finaldf = actualdf.join(preddf)\n",
        "\n",
        "print(finaldf[170:180])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKVJ0gUKuqp7",
        "outputId": "397a12e0-4e21-42b3-9c04-212c29052580"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     actualvalues predictedvalues\n",
            "170    male_happy    female_angry\n",
            "171  male_fearful    female_angry\n",
            "172    male_angry    female_angry\n",
            "173     male_calm    female_angry\n",
            "174  female_happy    female_angry\n",
            "175    male_angry    female_angry\n",
            "176    female_sad    female_angry\n",
            "177  male_fearful    female_angry\n",
            "178    female_sad    female_angry\n",
            "179    male_happy    female_angry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finaldf.groupby('actualvalues').count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frhEG9IBu-kY",
        "outputId": "6bdc7188-a51f-4aef-de6c-dbd7efc13edb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                predictedvalues\n",
            "actualvalues                   \n",
            "female_angry                 16\n",
            "female_calm                  17\n",
            "female_fearful               20\n",
            "female_happy                 22\n",
            "female_sad                   25\n",
            "male_angry                   22\n",
            "male_calm                    11\n",
            "male_fearful                 21\n",
            "male_happy                   17\n",
            "male_sad                     18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finaldf.groupby('predictedvalues').count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76K2DFgWvJcA",
        "outputId": "4165ae5b-425c-41e2-8e30-184f23497def"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 actualvalues\n",
            "predictedvalues              \n",
            "female_angry              189\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "project-id2223.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlhrqesSXkCytqXpzdkZM7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}