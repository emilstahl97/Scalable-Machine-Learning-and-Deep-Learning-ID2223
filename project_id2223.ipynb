{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilstahl97/Scalable-Machine-Learning-and-Deep-Learning-ID2223/blob/notebooks/project_id2223.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2a0tOM7v0Em",
        "outputId": "ff4364d3-d638-4c46-afd3-2c0d65ffe88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset already exists\n",
            "Your configuration specifies to merge with the ref 'refs/heads/main'\n",
            "from the remote, but no such ref was fetched.\n",
            "03-01-01-01-01-01-01.wav  03-01-04-01-01-01-01.wav  03-01-06-02-01-01-01.wav\n",
            "03-01-01-01-01-01-02.wav  03-01-04-01-01-01-02.wav  03-01-06-02-01-01-02.wav\n",
            "03-01-01-01-01-01-03.wav  03-01-04-01-01-01-03.wav  03-01-06-02-01-01-03.wav\n",
            "03-01-01-01-01-01-04.wav  03-01-04-01-01-01-04.wav  03-01-06-02-01-01-04.wav\n",
            "03-01-01-01-01-01-06.wav  03-01-04-01-01-01-06.wav  03-01-06-02-01-01-06.wav\n",
            "03-01-01-01-01-01-07.wav  03-01-04-01-01-01-07.wav  03-01-06-02-01-01-07.wav\n",
            "03-01-01-01-01-01-08.wav  03-01-04-01-01-01-08.wav  03-01-06-02-01-01-08.wav\n",
            "03-01-01-01-01-01-09.wav  03-01-04-01-01-01-09.wav  03-01-06-02-01-01-09.wav\n",
            "03-01-01-01-01-01-10.wav  03-01-04-01-01-01-10.wav  03-01-06-02-01-01-10.wav\n",
            "03-01-01-01-01-01-11.wav  03-01-04-01-01-01-11.wav  03-01-06-02-01-01-11.wav\n",
            "03-01-01-01-01-01-12.wav  03-01-04-01-01-01-12.wav  03-01-06-02-01-01-12.wav\n",
            "03-01-01-01-01-01-13.wav  03-01-04-01-01-01-13.wav  03-01-06-02-01-01-13.wav\n",
            "03-01-01-01-01-01-14.wav  03-01-04-01-01-01-14.wav  03-01-06-02-01-01-14.wav\n",
            "03-01-01-01-01-01-15.wav  03-01-04-01-01-01-15.wav  03-01-06-02-01-01-15.wav\n",
            "03-01-01-01-01-01-16.wav  03-01-04-01-01-01-16.wav  03-01-06-02-01-01-16.wav\n",
            "03-01-01-01-01-01-17.wav  03-01-04-01-01-01-17.wav  03-01-06-02-01-01-17.wav\n",
            "03-01-01-01-01-01-18.wav  03-01-04-01-01-01-18.wav  03-01-06-02-01-01-18.wav\n",
            "03-01-01-01-01-01-19.wav  03-01-04-01-01-01-19.wav  03-01-06-02-01-01-19.wav\n",
            "03-01-01-01-01-01-20.wav  03-01-04-01-01-01-20.wav  03-01-06-02-01-01-20.wav\n",
            "03-01-01-01-01-01-21.wav  03-01-04-01-01-01-21.wav  03-01-06-02-01-01-21.wav\n",
            "03-01-01-01-01-01-22.wav  03-01-04-01-01-01-22.wav  03-01-06-02-01-01-22.wav\n",
            "03-01-01-01-01-01-23.wav  03-01-04-01-01-01-23.wav  03-01-06-02-01-01-23.wav\n",
            "03-01-01-01-01-01-24.wav  03-01-04-01-01-01-24.wav  03-01-06-02-01-01-24.wav\n",
            "03-01-01-01-01-02-01.wav  03-01-04-01-01-02-01.wav  03-01-06-02-01-02-01.wav\n",
            "03-01-01-01-01-02-02.wav  03-01-04-01-01-02-02.wav  03-01-06-02-01-02-02.wav\n",
            "03-01-01-01-01-02-03.wav  03-01-04-01-01-02-03.wav  03-01-06-02-01-02-03.wav\n",
            "03-01-01-01-01-02-04.wav  03-01-04-01-01-02-04.wav  03-01-06-02-01-02-04.wav\n",
            "03-01-01-01-01-02-06.wav  03-01-04-01-01-02-06.wav  03-01-06-02-01-02-06.wav\n",
            "03-01-01-01-01-02-07.wav  03-01-04-01-01-02-07.wav  03-01-06-02-01-02-07.wav\n",
            "03-01-01-01-01-02-08.wav  03-01-04-01-01-02-08.wav  03-01-06-02-01-02-08.wav\n",
            "03-01-01-01-01-02-09.wav  03-01-04-01-01-02-09.wav  03-01-06-02-01-02-09.wav\n",
            "03-01-01-01-01-02-10.wav  03-01-04-01-01-02-10.wav  03-01-06-02-01-02-10.wav\n",
            "03-01-01-01-01-02-11.wav  03-01-04-01-01-02-11.wav  03-01-06-02-01-02-11.wav\n",
            "03-01-01-01-01-02-12.wav  03-01-04-01-01-02-12.wav  03-01-06-02-01-02-12.wav\n",
            "03-01-01-01-01-02-13.wav  03-01-04-01-01-02-13.wav  03-01-06-02-01-02-13.wav\n",
            "03-01-01-01-01-02-14.wav  03-01-04-01-01-02-14.wav  03-01-06-02-01-02-14.wav\n",
            "03-01-01-01-01-02-15.wav  03-01-04-01-01-02-15.wav  03-01-06-02-01-02-15.wav\n",
            "03-01-01-01-01-02-16.wav  03-01-04-01-01-02-16.wav  03-01-06-02-01-02-16.wav\n",
            "03-01-01-01-01-02-17.wav  03-01-04-01-01-02-17.wav  03-01-06-02-01-02-17.wav\n",
            "03-01-01-01-01-02-18.wav  03-01-04-01-01-02-18.wav  03-01-06-02-01-02-18.wav\n",
            "03-01-01-01-01-02-19.wav  03-01-04-01-01-02-19.wav  03-01-06-02-01-02-19.wav\n",
            "03-01-01-01-01-02-20.wav  03-01-04-01-01-02-20.wav  03-01-06-02-01-02-20.wav\n",
            "03-01-01-01-01-02-21.wav  03-01-04-01-01-02-21.wav  03-01-06-02-01-02-21.wav\n",
            "03-01-01-01-01-02-22.wav  03-01-04-01-01-02-22.wav  03-01-06-02-01-02-22.wav\n",
            "03-01-01-01-01-02-23.wav  03-01-04-01-01-02-23.wav  03-01-06-02-01-02-23.wav\n",
            "03-01-01-01-01-02-24.wav  03-01-04-01-01-02-24.wav  03-01-06-02-01-02-24.wav\n",
            "03-01-01-01-02-01-01.wav  03-01-04-01-02-01-01.wav  03-01-06-02-02-01-01.wav\n",
            "03-01-01-01-02-01-02.wav  03-01-04-01-02-01-02.wav  03-01-06-02-02-01-02.wav\n",
            "03-01-01-01-02-01-03.wav  03-01-04-01-02-01-03.wav  03-01-06-02-02-01-03.wav\n",
            "03-01-01-01-02-01-04.wav  03-01-04-01-02-01-04.wav  03-01-06-02-02-01-04.wav\n",
            "03-01-01-01-02-01-06.wav  03-01-04-01-02-01-06.wav  03-01-06-02-02-01-06.wav\n",
            "03-01-01-01-02-01-07.wav  03-01-04-01-02-01-07.wav  03-01-06-02-02-01-07.wav\n",
            "03-01-01-01-02-01-08.wav  03-01-04-01-02-01-08.wav  03-01-06-02-02-01-08.wav\n",
            "03-01-01-01-02-01-09.wav  03-01-04-01-02-01-09.wav  03-01-06-02-02-01-09.wav\n",
            "03-01-01-01-02-01-10.wav  03-01-04-01-02-01-10.wav  03-01-06-02-02-01-10.wav\n",
            "03-01-01-01-02-01-11.wav  03-01-04-01-02-01-11.wav  03-01-06-02-02-01-11.wav\n",
            "03-01-01-01-02-01-12.wav  03-01-04-01-02-01-12.wav  03-01-06-02-02-01-12.wav\n",
            "03-01-01-01-02-01-13.wav  03-01-04-01-02-01-13.wav  03-01-06-02-02-01-13.wav\n",
            "03-01-01-01-02-01-14.wav  03-01-04-01-02-01-14.wav  03-01-06-02-02-01-14.wav\n",
            "03-01-01-01-02-01-15.wav  03-01-04-01-02-01-15.wav  03-01-06-02-02-01-15.wav\n",
            "03-01-01-01-02-01-16.wav  03-01-04-01-02-01-16.wav  03-01-06-02-02-01-16.wav\n",
            "03-01-01-01-02-01-17.wav  03-01-04-01-02-01-17.wav  03-01-06-02-02-01-17.wav\n",
            "03-01-01-01-02-01-18.wav  03-01-04-01-02-01-18.wav  03-01-06-02-02-01-18.wav\n",
            "03-01-01-01-02-01-19.wav  03-01-04-01-02-01-19.wav  03-01-06-02-02-01-19.wav\n",
            "03-01-01-01-02-01-20.wav  03-01-04-01-02-01-20.wav  03-01-06-02-02-01-20.wav\n",
            "03-01-01-01-02-01-21.wav  03-01-04-01-02-01-21.wav  03-01-06-02-02-01-21.wav\n",
            "03-01-01-01-02-01-22.wav  03-01-04-01-02-01-22.wav  03-01-06-02-02-01-22.wav\n",
            "03-01-01-01-02-01-23.wav  03-01-04-01-02-01-23.wav  03-01-06-02-02-01-23.wav\n",
            "03-01-01-01-02-01-24.wav  03-01-04-01-02-01-24.wav  03-01-06-02-02-01-24.wav\n",
            "03-01-01-01-02-02-01.wav  03-01-04-01-02-02-01.wav  03-01-06-02-02-02-01.wav\n",
            "03-01-01-01-02-02-02.wav  03-01-04-01-02-02-02.wav  03-01-06-02-02-02-02.wav\n",
            "03-01-01-01-02-02-03.wav  03-01-04-01-02-02-03.wav  03-01-06-02-02-02-03.wav\n",
            "03-01-01-01-02-02-04.wav  03-01-04-01-02-02-04.wav  03-01-06-02-02-02-04.wav\n",
            "03-01-01-01-02-02-06.wav  03-01-04-01-02-02-06.wav  03-01-06-02-02-02-06.wav\n",
            "03-01-01-01-02-02-07.wav  03-01-04-01-02-02-07.wav  03-01-06-02-02-02-07.wav\n",
            "03-01-01-01-02-02-08.wav  03-01-04-01-02-02-08.wav  03-01-06-02-02-02-08.wav\n",
            "03-01-01-01-02-02-09.wav  03-01-04-01-02-02-09.wav  03-01-06-02-02-02-09.wav\n",
            "03-01-01-01-02-02-10.wav  03-01-04-01-02-02-10.wav  03-01-06-02-02-02-10.wav\n",
            "03-01-01-01-02-02-11.wav  03-01-04-01-02-02-11.wav  03-01-06-02-02-02-11.wav\n",
            "03-01-01-01-02-02-12.wav  03-01-04-01-02-02-12.wav  03-01-06-02-02-02-12.wav\n",
            "03-01-01-01-02-02-13.wav  03-01-04-01-02-02-13.wav  03-01-06-02-02-02-13.wav\n",
            "03-01-01-01-02-02-14.wav  03-01-04-01-02-02-14.wav  03-01-06-02-02-02-14.wav\n",
            "03-01-01-01-02-02-15.wav  03-01-04-01-02-02-15.wav  03-01-06-02-02-02-15.wav\n",
            "03-01-01-01-02-02-16.wav  03-01-04-01-02-02-16.wav  03-01-06-02-02-02-16.wav\n",
            "03-01-01-01-02-02-17.wav  03-01-04-01-02-02-17.wav  03-01-06-02-02-02-17.wav\n",
            "03-01-01-01-02-02-18.wav  03-01-04-01-02-02-18.wav  03-01-06-02-02-02-18.wav\n",
            "03-01-01-01-02-02-19.wav  03-01-04-01-02-02-19.wav  03-01-06-02-02-02-19.wav\n",
            "03-01-01-01-02-02-20.wav  03-01-04-01-02-02-20.wav  03-01-06-02-02-02-20.wav\n",
            "03-01-01-01-02-02-21.wav  03-01-04-01-02-02-21.wav  03-01-06-02-02-02-21.wav\n",
            "03-01-01-01-02-02-22.wav  03-01-04-01-02-02-22.wav  03-01-06-02-02-02-22.wav\n",
            "03-01-01-01-02-02-23.wav  03-01-04-01-02-02-23.wav  03-01-06-02-02-02-23.wav\n",
            "03-01-01-01-02-02-24.wav  03-01-04-01-02-02-24.wav  03-01-06-02-02-02-24.wav\n",
            "03-01-02-01-01-01-01.wav  03-01-04-02-01-01-01.wav  03-01-07-01-01-01-01.wav\n",
            "03-01-02-01-01-01-02.wav  03-01-04-02-01-01-02.wav  03-01-07-01-01-01-02.wav\n",
            "03-01-02-01-01-01-03.wav  03-01-04-02-01-01-03.wav  03-01-07-01-01-01-03.wav\n",
            "03-01-02-01-01-01-04.wav  03-01-04-02-01-01-04.wav  03-01-07-01-01-01-04.wav\n",
            "03-01-02-01-01-01-06.wav  03-01-04-02-01-01-06.wav  03-01-07-01-01-01-06.wav\n",
            "03-01-02-01-01-01-07.wav  03-01-04-02-01-01-07.wav  03-01-07-01-01-01-07.wav\n",
            "03-01-02-01-01-01-08.wav  03-01-04-02-01-01-08.wav  03-01-07-01-01-01-08.wav\n",
            "03-01-02-01-01-01-09.wav  03-01-04-02-01-01-09.wav  03-01-07-01-01-01-09.wav\n",
            "03-01-02-01-01-01-10.wav  03-01-04-02-01-01-10.wav  03-01-07-01-01-01-10.wav\n",
            "03-01-02-01-01-01-11.wav  03-01-04-02-01-01-11.wav  03-01-07-01-01-01-11.wav\n",
            "03-01-02-01-01-01-12.wav  03-01-04-02-01-01-12.wav  03-01-07-01-01-01-12.wav\n",
            "03-01-02-01-01-01-13.wav  03-01-04-02-01-01-13.wav  03-01-07-01-01-01-13.wav\n",
            "03-01-02-01-01-01-14.wav  03-01-04-02-01-01-14.wav  03-01-07-01-01-01-14.wav\n",
            "03-01-02-01-01-01-15.wav  03-01-04-02-01-01-15.wav  03-01-07-01-01-01-15.wav\n",
            "03-01-02-01-01-01-16.wav  03-01-04-02-01-01-16.wav  03-01-07-01-01-01-16.wav\n",
            "03-01-02-01-01-01-17.wav  03-01-04-02-01-01-17.wav  03-01-07-01-01-01-17.wav\n",
            "03-01-02-01-01-01-18.wav  03-01-04-02-01-01-18.wav  03-01-07-01-01-01-18.wav\n",
            "03-01-02-01-01-01-19.wav  03-01-04-02-01-01-19.wav  03-01-07-01-01-01-19.wav\n",
            "03-01-02-01-01-01-20.wav  03-01-04-02-01-01-20.wav  03-01-07-01-01-01-20.wav\n",
            "03-01-02-01-01-01-21.wav  03-01-04-02-01-01-21.wav  03-01-07-01-01-01-21.wav\n",
            "03-01-02-01-01-01-22.wav  03-01-04-02-01-01-22.wav  03-01-07-01-01-01-22.wav\n",
            "03-01-02-01-01-01-23.wav  03-01-04-02-01-01-23.wav  03-01-07-01-01-01-23.wav\n",
            "03-01-02-01-01-01-24.wav  03-01-04-02-01-01-24.wav  03-01-07-01-01-01-24.wav\n",
            "03-01-02-01-01-02-01.wav  03-01-04-02-01-02-01.wav  03-01-07-01-01-02-01.wav\n",
            "03-01-02-01-01-02-02.wav  03-01-04-02-01-02-02.wav  03-01-07-01-01-02-02.wav\n",
            "03-01-02-01-01-02-03.wav  03-01-04-02-01-02-03.wav  03-01-07-01-01-02-03.wav\n",
            "03-01-02-01-01-02-04.wav  03-01-04-02-01-02-04.wav  03-01-07-01-01-02-04.wav\n",
            "03-01-02-01-01-02-06.wav  03-01-04-02-01-02-06.wav  03-01-07-01-01-02-06.wav\n",
            "03-01-02-01-01-02-07.wav  03-01-04-02-01-02-07.wav  03-01-07-01-01-02-07.wav\n",
            "03-01-02-01-01-02-08.wav  03-01-04-02-01-02-08.wav  03-01-07-01-01-02-08.wav\n",
            "03-01-02-01-01-02-09.wav  03-01-04-02-01-02-09.wav  03-01-07-01-01-02-09.wav\n",
            "03-01-02-01-01-02-10.wav  03-01-04-02-01-02-10.wav  03-01-07-01-01-02-10.wav\n",
            "03-01-02-01-01-02-11.wav  03-01-04-02-01-02-11.wav  03-01-07-01-01-02-11.wav\n",
            "03-01-02-01-01-02-12.wav  03-01-04-02-01-02-12.wav  03-01-07-01-01-02-12.wav\n",
            "03-01-02-01-01-02-13.wav  03-01-04-02-01-02-13.wav  03-01-07-01-01-02-13.wav\n",
            "03-01-02-01-01-02-14.wav  03-01-04-02-01-02-14.wav  03-01-07-01-01-02-14.wav\n",
            "03-01-02-01-01-02-15.wav  03-01-04-02-01-02-15.wav  03-01-07-01-01-02-15.wav\n",
            "03-01-02-01-01-02-16.wav  03-01-04-02-01-02-16.wav  03-01-07-01-01-02-16.wav\n",
            "03-01-02-01-01-02-17.wav  03-01-04-02-01-02-17.wav  03-01-07-01-01-02-17.wav\n",
            "03-01-02-01-01-02-18.wav  03-01-04-02-01-02-18.wav  03-01-07-01-01-02-18.wav\n",
            "03-01-02-01-01-02-19.wav  03-01-04-02-01-02-19.wav  03-01-07-01-01-02-19.wav\n",
            "03-01-02-01-01-02-20.wav  03-01-04-02-01-02-20.wav  03-01-07-01-01-02-20.wav\n",
            "03-01-02-01-01-02-21.wav  03-01-04-02-01-02-21.wav  03-01-07-01-01-02-21.wav\n",
            "03-01-02-01-01-02-22.wav  03-01-04-02-01-02-22.wav  03-01-07-01-01-02-22.wav\n",
            "03-01-02-01-01-02-23.wav  03-01-04-02-01-02-23.wav  03-01-07-01-01-02-23.wav\n",
            "03-01-02-01-01-02-24.wav  03-01-04-02-01-02-24.wav  03-01-07-01-01-02-24.wav\n",
            "03-01-02-01-02-01-01.wav  03-01-04-02-02-01-01.wav  03-01-07-01-02-01-01.wav\n",
            "03-01-02-01-02-01-02.wav  03-01-04-02-02-01-02.wav  03-01-07-01-02-01-02.wav\n",
            "03-01-02-01-02-01-03.wav  03-01-04-02-02-01-03.wav  03-01-07-01-02-01-03.wav\n",
            "03-01-02-01-02-01-04.wav  03-01-04-02-02-01-04.wav  03-01-07-01-02-01-04.wav\n",
            "03-01-02-01-02-01-06.wav  03-01-04-02-02-01-06.wav  03-01-07-01-02-01-06.wav\n",
            "03-01-02-01-02-01-07.wav  03-01-04-02-02-01-07.wav  03-01-07-01-02-01-07.wav\n",
            "03-01-02-01-02-01-08.wav  03-01-04-02-02-01-08.wav  03-01-07-01-02-01-08.wav\n",
            "03-01-02-01-02-01-09.wav  03-01-04-02-02-01-09.wav  03-01-07-01-02-01-09.wav\n",
            "03-01-02-01-02-01-10.wav  03-01-04-02-02-01-10.wav  03-01-07-01-02-01-10.wav\n",
            "03-01-02-01-02-01-11.wav  03-01-04-02-02-01-11.wav  03-01-07-01-02-01-11.wav\n",
            "03-01-02-01-02-01-12.wav  03-01-04-02-02-01-12.wav  03-01-07-01-02-01-12.wav\n",
            "03-01-02-01-02-01-13.wav  03-01-04-02-02-01-13.wav  03-01-07-01-02-01-13.wav\n",
            "03-01-02-01-02-01-14.wav  03-01-04-02-02-01-14.wav  03-01-07-01-02-01-14.wav\n",
            "03-01-02-01-02-01-15.wav  03-01-04-02-02-01-15.wav  03-01-07-01-02-01-15.wav\n",
            "03-01-02-01-02-01-16.wav  03-01-04-02-02-01-16.wav  03-01-07-01-02-01-16.wav\n",
            "03-01-02-01-02-01-17.wav  03-01-04-02-02-01-17.wav  03-01-07-01-02-01-17.wav\n",
            "03-01-02-01-02-01-18.wav  03-01-04-02-02-01-18.wav  03-01-07-01-02-01-18.wav\n",
            "03-01-02-01-02-01-19.wav  03-01-04-02-02-01-19.wav  03-01-07-01-02-01-19.wav\n",
            "03-01-02-01-02-01-20.wav  03-01-04-02-02-01-20.wav  03-01-07-01-02-01-20.wav\n",
            "03-01-02-01-02-01-21.wav  03-01-04-02-02-01-21.wav  03-01-07-01-02-01-21.wav\n",
            "03-01-02-01-02-01-22.wav  03-01-04-02-02-01-22.wav  03-01-07-01-02-01-22.wav\n",
            "03-01-02-01-02-01-23.wav  03-01-04-02-02-01-23.wav  03-01-07-01-02-01-23.wav\n",
            "03-01-02-01-02-01-24.wav  03-01-04-02-02-01-24.wav  03-01-07-01-02-01-24.wav\n",
            "03-01-02-01-02-02-01.wav  03-01-04-02-02-02-01.wav  03-01-07-01-02-02-01.wav\n",
            "03-01-02-01-02-02-02.wav  03-01-04-02-02-02-02.wav  03-01-07-01-02-02-02.wav\n",
            "03-01-02-01-02-02-03.wav  03-01-04-02-02-02-03.wav  03-01-07-01-02-02-03.wav\n",
            "03-01-02-01-02-02-04.wav  03-01-04-02-02-02-04.wav  03-01-07-01-02-02-04.wav\n",
            "03-01-02-01-02-02-06.wav  03-01-04-02-02-02-06.wav  03-01-07-01-02-02-06.wav\n",
            "03-01-02-01-02-02-07.wav  03-01-04-02-02-02-07.wav  03-01-07-01-02-02-07.wav\n",
            "03-01-02-01-02-02-08.wav  03-01-04-02-02-02-08.wav  03-01-07-01-02-02-08.wav\n",
            "03-01-02-01-02-02-09.wav  03-01-04-02-02-02-09.wav  03-01-07-01-02-02-09.wav\n",
            "03-01-02-01-02-02-10.wav  03-01-04-02-02-02-10.wav  03-01-07-01-02-02-10.wav\n",
            "03-01-02-01-02-02-11.wav  03-01-04-02-02-02-11.wav  03-01-07-01-02-02-11.wav\n",
            "03-01-02-01-02-02-12.wav  03-01-04-02-02-02-12.wav  03-01-07-01-02-02-12.wav\n",
            "03-01-02-01-02-02-13.wav  03-01-04-02-02-02-13.wav  03-01-07-01-02-02-13.wav\n",
            "03-01-02-01-02-02-14.wav  03-01-04-02-02-02-14.wav  03-01-07-01-02-02-14.wav\n",
            "03-01-02-01-02-02-15.wav  03-01-04-02-02-02-15.wav  03-01-07-01-02-02-15.wav\n",
            "03-01-02-01-02-02-16.wav  03-01-04-02-02-02-16.wav  03-01-07-01-02-02-16.wav\n",
            "03-01-02-01-02-02-17.wav  03-01-04-02-02-02-17.wav  03-01-07-01-02-02-17.wav\n",
            "03-01-02-01-02-02-18.wav  03-01-04-02-02-02-18.wav  03-01-07-01-02-02-18.wav\n",
            "03-01-02-01-02-02-19.wav  03-01-04-02-02-02-19.wav  03-01-07-01-02-02-19.wav\n",
            "03-01-02-01-02-02-20.wav  03-01-04-02-02-02-20.wav  03-01-07-01-02-02-20.wav\n",
            "03-01-02-01-02-02-21.wav  03-01-04-02-02-02-21.wav  03-01-07-01-02-02-21.wav\n",
            "03-01-02-01-02-02-22.wav  03-01-04-02-02-02-22.wav  03-01-07-01-02-02-22.wav\n",
            "03-01-02-01-02-02-23.wav  03-01-04-02-02-02-23.wav  03-01-07-01-02-02-23.wav\n",
            "03-01-02-01-02-02-24.wav  03-01-04-02-02-02-24.wav  03-01-07-01-02-02-24.wav\n",
            "03-01-02-02-01-01-01.wav  03-01-05-01-01-01-01.wav  03-01-07-02-01-01-01.wav\n",
            "03-01-02-02-01-01-02.wav  03-01-05-01-01-01-02.wav  03-01-07-02-01-01-02.wav\n",
            "03-01-02-02-01-01-03.wav  03-01-05-01-01-01-03.wav  03-01-07-02-01-01-03.wav\n",
            "03-01-02-02-01-01-04.wav  03-01-05-01-01-01-04.wav  03-01-07-02-01-01-04.wav\n",
            "03-01-02-02-01-01-06.wav  03-01-05-01-01-01-06.wav  03-01-07-02-01-01-06.wav\n",
            "03-01-02-02-01-01-07.wav  03-01-05-01-01-01-07.wav  03-01-07-02-01-01-07.wav\n",
            "03-01-02-02-01-01-08.wav  03-01-05-01-01-01-08.wav  03-01-07-02-01-01-08.wav\n",
            "03-01-02-02-01-01-09.wav  03-01-05-01-01-01-09.wav  03-01-07-02-01-01-09.wav\n",
            "03-01-02-02-01-01-10.wav  03-01-05-01-01-01-10.wav  03-01-07-02-01-01-10.wav\n",
            "03-01-02-02-01-01-11.wav  03-01-05-01-01-01-11.wav  03-01-07-02-01-01-11.wav\n",
            "03-01-02-02-01-01-12.wav  03-01-05-01-01-01-12.wav  03-01-07-02-01-01-12.wav\n",
            "03-01-02-02-01-01-13.wav  03-01-05-01-01-01-13.wav  03-01-07-02-01-01-13.wav\n",
            "03-01-02-02-01-01-14.wav  03-01-05-01-01-01-14.wav  03-01-07-02-01-01-14.wav\n",
            "03-01-02-02-01-01-15.wav  03-01-05-01-01-01-15.wav  03-01-07-02-01-01-15.wav\n",
            "03-01-02-02-01-01-16.wav  03-01-05-01-01-01-16.wav  03-01-07-02-01-01-16.wav\n",
            "03-01-02-02-01-01-17.wav  03-01-05-01-01-01-17.wav  03-01-07-02-01-01-17.wav\n",
            "03-01-02-02-01-01-18.wav  03-01-05-01-01-01-18.wav  03-01-07-02-01-01-18.wav\n",
            "03-01-02-02-01-01-19.wav  03-01-05-01-01-01-19.wav  03-01-07-02-01-01-19.wav\n",
            "03-01-02-02-01-01-20.wav  03-01-05-01-01-01-20.wav  03-01-07-02-01-01-20.wav\n",
            "03-01-02-02-01-01-21.wav  03-01-05-01-01-01-21.wav  03-01-07-02-01-01-21.wav\n",
            "03-01-02-02-01-01-22.wav  03-01-05-01-01-01-22.wav  03-01-07-02-01-01-22.wav\n",
            "03-01-02-02-01-01-23.wav  03-01-05-01-01-01-23.wav  03-01-07-02-01-01-23.wav\n",
            "03-01-02-02-01-01-24.wav  03-01-05-01-01-01-24.wav  03-01-07-02-01-01-24.wav\n",
            "03-01-02-02-01-02-01.wav  03-01-05-01-01-02-01.wav  03-01-07-02-01-02-01.wav\n",
            "03-01-02-02-01-02-02.wav  03-01-05-01-01-02-02.wav  03-01-07-02-01-02-02.wav\n",
            "03-01-02-02-01-02-03.wav  03-01-05-01-01-02-03.wav  03-01-07-02-01-02-03.wav\n",
            "03-01-02-02-01-02-04.wav  03-01-05-01-01-02-04.wav  03-01-07-02-01-02-04.wav\n",
            "03-01-02-02-01-02-06.wav  03-01-05-01-01-02-06.wav  03-01-07-02-01-02-06.wav\n",
            "03-01-02-02-01-02-07.wav  03-01-05-01-01-02-07.wav  03-01-07-02-01-02-07.wav\n",
            "03-01-02-02-01-02-08.wav  03-01-05-01-01-02-08.wav  03-01-07-02-01-02-08.wav\n",
            "03-01-02-02-01-02-09.wav  03-01-05-01-01-02-09.wav  03-01-07-02-01-02-09.wav\n",
            "03-01-02-02-01-02-10.wav  03-01-05-01-01-02-10.wav  03-01-07-02-01-02-10.wav\n",
            "03-01-02-02-01-02-11.wav  03-01-05-01-01-02-11.wav  03-01-07-02-01-02-11.wav\n",
            "03-01-02-02-01-02-12.wav  03-01-05-01-01-02-12.wav  03-01-07-02-01-02-12.wav\n",
            "03-01-02-02-01-02-13.wav  03-01-05-01-01-02-13.wav  03-01-07-02-01-02-13.wav\n",
            "03-01-02-02-01-02-14.wav  03-01-05-01-01-02-14.wav  03-01-07-02-01-02-14.wav\n",
            "03-01-02-02-01-02-15.wav  03-01-05-01-01-02-15.wav  03-01-07-02-01-02-15.wav\n",
            "03-01-02-02-01-02-16.wav  03-01-05-01-01-02-16.wav  03-01-07-02-01-02-16.wav\n",
            "03-01-02-02-01-02-17.wav  03-01-05-01-01-02-17.wav  03-01-07-02-01-02-17.wav\n",
            "03-01-02-02-01-02-18.wav  03-01-05-01-01-02-18.wav  03-01-07-02-01-02-18.wav\n",
            "03-01-02-02-01-02-19.wav  03-01-05-01-01-02-19.wav  03-01-07-02-01-02-19.wav\n",
            "03-01-02-02-01-02-20.wav  03-01-05-01-01-02-20.wav  03-01-07-02-01-02-20.wav\n",
            "03-01-02-02-01-02-21.wav  03-01-05-01-01-02-21.wav  03-01-07-02-01-02-21.wav\n",
            "03-01-02-02-01-02-22.wav  03-01-05-01-01-02-22.wav  03-01-07-02-01-02-22.wav\n",
            "03-01-02-02-01-02-23.wav  03-01-05-01-01-02-23.wav  03-01-07-02-01-02-23.wav\n",
            "03-01-02-02-01-02-24.wav  03-01-05-01-01-02-24.wav  03-01-07-02-01-02-24.wav\n",
            "03-01-02-02-02-01-01.wav  03-01-05-01-02-01-01.wav  03-01-07-02-02-01-01.wav\n",
            "03-01-02-02-02-01-02.wav  03-01-05-01-02-01-02.wav  03-01-07-02-02-01-02.wav\n",
            "03-01-02-02-02-01-03.wav  03-01-05-01-02-01-03.wav  03-01-07-02-02-01-03.wav\n",
            "03-01-02-02-02-01-04.wav  03-01-05-01-02-01-04.wav  03-01-07-02-02-01-04.wav\n",
            "03-01-02-02-02-01-06.wav  03-01-05-01-02-01-06.wav  03-01-07-02-02-01-06.wav\n",
            "03-01-02-02-02-01-07.wav  03-01-05-01-02-01-07.wav  03-01-07-02-02-01-07.wav\n",
            "03-01-02-02-02-01-08.wav  03-01-05-01-02-01-08.wav  03-01-07-02-02-01-08.wav\n",
            "03-01-02-02-02-01-09.wav  03-01-05-01-02-01-09.wav  03-01-07-02-02-01-09.wav\n",
            "03-01-02-02-02-01-10.wav  03-01-05-01-02-01-10.wav  03-01-07-02-02-01-10.wav\n",
            "03-01-02-02-02-01-11.wav  03-01-05-01-02-01-11.wav  03-01-07-02-02-01-11.wav\n",
            "03-01-02-02-02-01-12.wav  03-01-05-01-02-01-12.wav  03-01-07-02-02-01-12.wav\n",
            "03-01-02-02-02-01-13.wav  03-01-05-01-02-01-13.wav  03-01-07-02-02-01-13.wav\n",
            "03-01-02-02-02-01-14.wav  03-01-05-01-02-01-14.wav  03-01-07-02-02-01-14.wav\n",
            "03-01-02-02-02-01-15.wav  03-01-05-01-02-01-15.wav  03-01-07-02-02-01-15.wav\n",
            "03-01-02-02-02-01-16.wav  03-01-05-01-02-01-16.wav  03-01-07-02-02-01-16.wav\n",
            "03-01-02-02-02-01-17.wav  03-01-05-01-02-01-17.wav  03-01-07-02-02-01-17.wav\n",
            "03-01-02-02-02-01-18.wav  03-01-05-01-02-01-18.wav  03-01-07-02-02-01-18.wav\n",
            "03-01-02-02-02-01-19.wav  03-01-05-01-02-01-19.wav  03-01-07-02-02-01-19.wav\n",
            "03-01-02-02-02-01-20.wav  03-01-05-01-02-01-20.wav  03-01-07-02-02-01-20.wav\n",
            "03-01-02-02-02-01-21.wav  03-01-05-01-02-01-21.wav  03-01-07-02-02-01-21.wav\n",
            "03-01-02-02-02-01-22.wav  03-01-05-01-02-01-22.wav  03-01-07-02-02-01-22.wav\n",
            "03-01-02-02-02-01-23.wav  03-01-05-01-02-01-23.wav  03-01-07-02-02-01-23.wav\n",
            "03-01-02-02-02-01-24.wav  03-01-05-01-02-01-24.wav  03-01-07-02-02-01-24.wav\n",
            "03-01-02-02-02-02-01.wav  03-01-05-01-02-02-01.wav  03-01-07-02-02-02-01.wav\n",
            "03-01-02-02-02-02-02.wav  03-01-05-01-02-02-02.wav  03-01-07-02-02-02-02.wav\n",
            "03-01-02-02-02-02-03.wav  03-01-05-01-02-02-03.wav  03-01-07-02-02-02-03.wav\n",
            "03-01-02-02-02-02-04.wav  03-01-05-01-02-02-04.wav  03-01-07-02-02-02-04.wav\n",
            "03-01-02-02-02-02-06.wav  03-01-05-01-02-02-06.wav  03-01-07-02-02-02-06.wav\n",
            "03-01-02-02-02-02-07.wav  03-01-05-01-02-02-07.wav  03-01-07-02-02-02-07.wav\n",
            "03-01-02-02-02-02-08.wav  03-01-05-01-02-02-08.wav  03-01-07-02-02-02-08.wav\n",
            "03-01-02-02-02-02-09.wav  03-01-05-01-02-02-09.wav  03-01-07-02-02-02-09.wav\n",
            "03-01-02-02-02-02-10.wav  03-01-05-01-02-02-10.wav  03-01-07-02-02-02-10.wav\n",
            "03-01-02-02-02-02-11.wav  03-01-05-01-02-02-11.wav  03-01-07-02-02-02-11.wav\n",
            "03-01-02-02-02-02-12.wav  03-01-05-01-02-02-12.wav  03-01-07-02-02-02-12.wav\n",
            "03-01-02-02-02-02-13.wav  03-01-05-01-02-02-13.wav  03-01-07-02-02-02-13.wav\n",
            "03-01-02-02-02-02-14.wav  03-01-05-01-02-02-14.wav  03-01-07-02-02-02-14.wav\n",
            "03-01-02-02-02-02-15.wav  03-01-05-01-02-02-15.wav  03-01-07-02-02-02-15.wav\n",
            "03-01-02-02-02-02-16.wav  03-01-05-01-02-02-16.wav  03-01-07-02-02-02-16.wav\n",
            "03-01-02-02-02-02-17.wav  03-01-05-01-02-02-17.wav  03-01-07-02-02-02-17.wav\n",
            "03-01-02-02-02-02-18.wav  03-01-05-01-02-02-18.wav  03-01-07-02-02-02-18.wav\n",
            "03-01-02-02-02-02-19.wav  03-01-05-01-02-02-19.wav  03-01-07-02-02-02-19.wav\n",
            "03-01-02-02-02-02-20.wav  03-01-05-01-02-02-20.wav  03-01-07-02-02-02-20.wav\n",
            "03-01-02-02-02-02-21.wav  03-01-05-01-02-02-21.wav  03-01-07-02-02-02-21.wav\n",
            "03-01-02-02-02-02-22.wav  03-01-05-01-02-02-22.wav  03-01-07-02-02-02-22.wav\n",
            "03-01-02-02-02-02-23.wav  03-01-05-01-02-02-23.wav  03-01-07-02-02-02-23.wav\n",
            "03-01-02-02-02-02-24.wav  03-01-05-01-02-02-24.wav  03-01-07-02-02-02-24.wav\n",
            "03-01-03-01-01-01-01.wav  03-01-05-02-01-01-01.wav  03-01-08-01-01-01-01.wav\n",
            "03-01-03-01-01-01-02.wav  03-01-05-02-01-01-02.wav  03-01-08-01-01-01-02.wav\n",
            "03-01-03-01-01-01-03.wav  03-01-05-02-01-01-03.wav  03-01-08-01-01-01-03.wav\n",
            "03-01-03-01-01-01-04.wav  03-01-05-02-01-01-04.wav  03-01-08-01-01-01-04.wav\n",
            "03-01-03-01-01-01-06.wav  03-01-05-02-01-01-06.wav  03-01-08-01-01-01-06.wav\n",
            "03-01-03-01-01-01-07.wav  03-01-05-02-01-01-07.wav  03-01-08-01-01-01-07.wav\n",
            "03-01-03-01-01-01-08.wav  03-01-05-02-01-01-08.wav  03-01-08-01-01-01-08.wav\n",
            "03-01-03-01-01-01-09.wav  03-01-05-02-01-01-09.wav  03-01-08-01-01-01-09.wav\n",
            "03-01-03-01-01-01-10.wav  03-01-05-02-01-01-10.wav  03-01-08-01-01-01-10.wav\n",
            "03-01-03-01-01-01-11.wav  03-01-05-02-01-01-11.wav  03-01-08-01-01-01-11.wav\n",
            "03-01-03-01-01-01-12.wav  03-01-05-02-01-01-12.wav  03-01-08-01-01-01-12.wav\n",
            "03-01-03-01-01-01-13.wav  03-01-05-02-01-01-13.wav  03-01-08-01-01-01-13.wav\n",
            "03-01-03-01-01-01-14.wav  03-01-05-02-01-01-14.wav  03-01-08-01-01-01-14.wav\n",
            "03-01-03-01-01-01-15.wav  03-01-05-02-01-01-15.wav  03-01-08-01-01-01-15.wav\n",
            "03-01-03-01-01-01-16.wav  03-01-05-02-01-01-16.wav  03-01-08-01-01-01-16.wav\n",
            "03-01-03-01-01-01-17.wav  03-01-05-02-01-01-17.wav  03-01-08-01-01-01-17.wav\n",
            "03-01-03-01-01-01-18.wav  03-01-05-02-01-01-18.wav  03-01-08-01-01-01-18.wav\n",
            "03-01-03-01-01-01-19.wav  03-01-05-02-01-01-19.wav  03-01-08-01-01-01-19.wav\n",
            "03-01-03-01-01-01-20.wav  03-01-05-02-01-01-20.wav  03-01-08-01-01-01-20.wav\n",
            "03-01-03-01-01-01-21.wav  03-01-05-02-01-01-21.wav  03-01-08-01-01-01-21.wav\n",
            "03-01-03-01-01-01-22.wav  03-01-05-02-01-01-22.wav  03-01-08-01-01-01-22.wav\n",
            "03-01-03-01-01-01-23.wav  03-01-05-02-01-01-23.wav  03-01-08-01-01-01-23.wav\n",
            "03-01-03-01-01-01-24.wav  03-01-05-02-01-01-24.wav  03-01-08-01-01-01-24.wav\n",
            "03-01-03-01-01-02-01.wav  03-01-05-02-01-02-01.wav  03-01-08-01-01-02-01.wav\n",
            "03-01-03-01-01-02-02.wav  03-01-05-02-01-02-02.wav  03-01-08-01-01-02-02.wav\n",
            "03-01-03-01-01-02-03.wav  03-01-05-02-01-02-03.wav  03-01-08-01-01-02-03.wav\n",
            "03-01-03-01-01-02-04.wav  03-01-05-02-01-02-04.wav  03-01-08-01-01-02-04.wav\n",
            "03-01-03-01-01-02-06.wav  03-01-05-02-01-02-06.wav  03-01-08-01-01-02-06.wav\n",
            "03-01-03-01-01-02-07.wav  03-01-05-02-01-02-07.wav  03-01-08-01-01-02-07.wav\n",
            "03-01-03-01-01-02-08.wav  03-01-05-02-01-02-08.wav  03-01-08-01-01-02-08.wav\n",
            "03-01-03-01-01-02-09.wav  03-01-05-02-01-02-09.wav  03-01-08-01-01-02-09.wav\n",
            "03-01-03-01-01-02-10.wav  03-01-05-02-01-02-10.wav  03-01-08-01-01-02-10.wav\n",
            "03-01-03-01-01-02-11.wav  03-01-05-02-01-02-11.wav  03-01-08-01-01-02-11.wav\n",
            "03-01-03-01-01-02-12.wav  03-01-05-02-01-02-12.wav  03-01-08-01-01-02-12.wav\n",
            "03-01-03-01-01-02-13.wav  03-01-05-02-01-02-13.wav  03-01-08-01-01-02-13.wav\n",
            "03-01-03-01-01-02-14.wav  03-01-05-02-01-02-14.wav  03-01-08-01-01-02-14.wav\n",
            "03-01-03-01-01-02-15.wav  03-01-05-02-01-02-15.wav  03-01-08-01-01-02-15.wav\n",
            "03-01-03-01-01-02-16.wav  03-01-05-02-01-02-16.wav  03-01-08-01-01-02-16.wav\n",
            "03-01-03-01-01-02-17.wav  03-01-05-02-01-02-17.wav  03-01-08-01-01-02-17.wav\n",
            "03-01-03-01-01-02-18.wav  03-01-05-02-01-02-18.wav  03-01-08-01-01-02-18.wav\n",
            "03-01-03-01-01-02-19.wav  03-01-05-02-01-02-19.wav  03-01-08-01-01-02-19.wav\n",
            "03-01-03-01-01-02-20.wav  03-01-05-02-01-02-20.wav  03-01-08-01-01-02-20.wav\n",
            "03-01-03-01-01-02-21.wav  03-01-05-02-01-02-21.wav  03-01-08-01-01-02-21.wav\n",
            "03-01-03-01-01-02-22.wav  03-01-05-02-01-02-22.wav  03-01-08-01-01-02-22.wav\n",
            "03-01-03-01-01-02-23.wav  03-01-05-02-01-02-23.wav  03-01-08-01-01-02-23.wav\n",
            "03-01-03-01-01-02-24.wav  03-01-05-02-01-02-24.wav  03-01-08-01-01-02-24.wav\n",
            "03-01-03-01-02-01-01.wav  03-01-05-02-02-01-01.wav  03-01-08-01-02-01-01.wav\n",
            "03-01-03-01-02-01-02.wav  03-01-05-02-02-01-02.wav  03-01-08-01-02-01-02.wav\n",
            "03-01-03-01-02-01-03.wav  03-01-05-02-02-01-03.wav  03-01-08-01-02-01-03.wav\n",
            "03-01-03-01-02-01-04.wav  03-01-05-02-02-01-04.wav  03-01-08-01-02-01-04.wav\n",
            "03-01-03-01-02-01-06.wav  03-01-05-02-02-01-06.wav  03-01-08-01-02-01-06.wav\n",
            "03-01-03-01-02-01-07.wav  03-01-05-02-02-01-07.wav  03-01-08-01-02-01-07.wav\n",
            "03-01-03-01-02-01-08.wav  03-01-05-02-02-01-08.wav  03-01-08-01-02-01-08.wav\n",
            "03-01-03-01-02-01-09.wav  03-01-05-02-02-01-09.wav  03-01-08-01-02-01-09.wav\n",
            "03-01-03-01-02-01-10.wav  03-01-05-02-02-01-10.wav  03-01-08-01-02-01-10.wav\n",
            "03-01-03-01-02-01-11.wav  03-01-05-02-02-01-11.wav  03-01-08-01-02-01-11.wav\n",
            "03-01-03-01-02-01-12.wav  03-01-05-02-02-01-12.wav  03-01-08-01-02-01-12.wav\n",
            "03-01-03-01-02-01-13.wav  03-01-05-02-02-01-13.wav  03-01-08-01-02-01-13.wav\n",
            "03-01-03-01-02-01-14.wav  03-01-05-02-02-01-14.wav  03-01-08-01-02-01-14.wav\n",
            "03-01-03-01-02-01-15.wav  03-01-05-02-02-01-15.wav  03-01-08-01-02-01-15.wav\n",
            "03-01-03-01-02-01-16.wav  03-01-05-02-02-01-16.wav  03-01-08-01-02-01-16.wav\n",
            "03-01-03-01-02-01-17.wav  03-01-05-02-02-01-17.wav  03-01-08-01-02-01-17.wav\n",
            "03-01-03-01-02-01-18.wav  03-01-05-02-02-01-18.wav  03-01-08-01-02-01-18.wav\n",
            "03-01-03-01-02-01-19.wav  03-01-05-02-02-01-19.wav  03-01-08-01-02-01-19.wav\n",
            "03-01-03-01-02-01-20.wav  03-01-05-02-02-01-20.wav  03-01-08-01-02-01-20.wav\n",
            "03-01-03-01-02-01-21.wav  03-01-05-02-02-01-21.wav  03-01-08-01-02-01-21.wav\n",
            "03-01-03-01-02-01-22.wav  03-01-05-02-02-01-22.wav  03-01-08-01-02-01-22.wav\n",
            "03-01-03-01-02-01-23.wav  03-01-05-02-02-01-23.wav  03-01-08-01-02-01-23.wav\n",
            "03-01-03-01-02-01-24.wav  03-01-05-02-02-01-24.wav  03-01-08-01-02-01-24.wav\n",
            "03-01-03-01-02-02-01.wav  03-01-05-02-02-02-01.wav  03-01-08-01-02-02-01.wav\n",
            "03-01-03-01-02-02-02.wav  03-01-05-02-02-02-02.wav  03-01-08-01-02-02-02.wav\n",
            "03-01-03-01-02-02-03.wav  03-01-05-02-02-02-03.wav  03-01-08-01-02-02-03.wav\n",
            "03-01-03-01-02-02-04.wav  03-01-05-02-02-02-04.wav  03-01-08-01-02-02-04.wav\n",
            "03-01-03-01-02-02-06.wav  03-01-05-02-02-02-06.wav  03-01-08-01-02-02-06.wav\n",
            "03-01-03-01-02-02-07.wav  03-01-05-02-02-02-07.wav  03-01-08-01-02-02-07.wav\n",
            "03-01-03-01-02-02-08.wav  03-01-05-02-02-02-08.wav  03-01-08-01-02-02-08.wav\n",
            "03-01-03-01-02-02-09.wav  03-01-05-02-02-02-09.wav  03-01-08-01-02-02-09.wav\n",
            "03-01-03-01-02-02-10.wav  03-01-05-02-02-02-10.wav  03-01-08-01-02-02-10.wav\n",
            "03-01-03-01-02-02-11.wav  03-01-05-02-02-02-11.wav  03-01-08-01-02-02-11.wav\n",
            "03-01-03-01-02-02-12.wav  03-01-05-02-02-02-12.wav  03-01-08-01-02-02-12.wav\n",
            "03-01-03-01-02-02-13.wav  03-01-05-02-02-02-13.wav  03-01-08-01-02-02-13.wav\n",
            "03-01-03-01-02-02-14.wav  03-01-05-02-02-02-14.wav  03-01-08-01-02-02-14.wav\n",
            "03-01-03-01-02-02-15.wav  03-01-05-02-02-02-15.wav  03-01-08-01-02-02-15.wav\n",
            "03-01-03-01-02-02-16.wav  03-01-05-02-02-02-16.wav  03-01-08-01-02-02-16.wav\n",
            "03-01-03-01-02-02-17.wav  03-01-05-02-02-02-17.wav  03-01-08-01-02-02-17.wav\n",
            "03-01-03-01-02-02-18.wav  03-01-05-02-02-02-18.wav  03-01-08-01-02-02-18.wav\n",
            "03-01-03-01-02-02-19.wav  03-01-05-02-02-02-19.wav  03-01-08-01-02-02-19.wav\n",
            "03-01-03-01-02-02-20.wav  03-01-05-02-02-02-20.wav  03-01-08-01-02-02-20.wav\n",
            "03-01-03-01-02-02-21.wav  03-01-05-02-02-02-21.wav  03-01-08-01-02-02-21.wav\n",
            "03-01-03-01-02-02-22.wav  03-01-05-02-02-02-22.wav  03-01-08-01-02-02-22.wav\n",
            "03-01-03-01-02-02-23.wav  03-01-05-02-02-02-23.wav  03-01-08-01-02-02-23.wav\n",
            "03-01-03-01-02-02-24.wav  03-01-05-02-02-02-24.wav  03-01-08-01-02-02-24.wav\n",
            "03-01-03-02-01-01-01.wav  03-01-06-01-01-01-01.wav  03-01-08-02-01-01-01.wav\n",
            "03-01-03-02-01-01-02.wav  03-01-06-01-01-01-02.wav  03-01-08-02-01-01-02.wav\n",
            "03-01-03-02-01-01-03.wav  03-01-06-01-01-01-03.wav  03-01-08-02-01-01-03.wav\n",
            "03-01-03-02-01-01-04.wav  03-01-06-01-01-01-04.wav  03-01-08-02-01-01-04.wav\n",
            "03-01-03-02-01-01-06.wav  03-01-06-01-01-01-06.wav  03-01-08-02-01-01-06.wav\n",
            "03-01-03-02-01-01-07.wav  03-01-06-01-01-01-07.wav  03-01-08-02-01-01-07.wav\n",
            "03-01-03-02-01-01-08.wav  03-01-06-01-01-01-08.wav  03-01-08-02-01-01-08.wav\n",
            "03-01-03-02-01-01-09.wav  03-01-06-01-01-01-09.wav  03-01-08-02-01-01-09.wav\n",
            "03-01-03-02-01-01-10.wav  03-01-06-01-01-01-10.wav  03-01-08-02-01-01-10.wav\n",
            "03-01-03-02-01-01-11.wav  03-01-06-01-01-01-11.wav  03-01-08-02-01-01-11.wav\n",
            "03-01-03-02-01-01-12.wav  03-01-06-01-01-01-12.wav  03-01-08-02-01-01-12.wav\n",
            "03-01-03-02-01-01-13.wav  03-01-06-01-01-01-13.wav  03-01-08-02-01-01-13.wav\n",
            "03-01-03-02-01-01-14.wav  03-01-06-01-01-01-14.wav  03-01-08-02-01-01-14.wav\n",
            "03-01-03-02-01-01-15.wav  03-01-06-01-01-01-15.wav  03-01-08-02-01-01-15.wav\n",
            "03-01-03-02-01-01-16.wav  03-01-06-01-01-01-16.wav  03-01-08-02-01-01-16.wav\n",
            "03-01-03-02-01-01-17.wav  03-01-06-01-01-01-17.wav  03-01-08-02-01-01-17.wav\n",
            "03-01-03-02-01-01-18.wav  03-01-06-01-01-01-18.wav  03-01-08-02-01-01-18.wav\n",
            "03-01-03-02-01-01-19.wav  03-01-06-01-01-01-19.wav  03-01-08-02-01-01-19.wav\n",
            "03-01-03-02-01-01-20.wav  03-01-06-01-01-01-20.wav  03-01-08-02-01-01-20.wav\n",
            "03-01-03-02-01-01-21.wav  03-01-06-01-01-01-21.wav  03-01-08-02-01-01-21.wav\n",
            "03-01-03-02-01-01-22.wav  03-01-06-01-01-01-22.wav  03-01-08-02-01-01-22.wav\n",
            "03-01-03-02-01-01-23.wav  03-01-06-01-01-01-23.wav  03-01-08-02-01-01-23.wav\n",
            "03-01-03-02-01-01-24.wav  03-01-06-01-01-01-24.wav  03-01-08-02-01-01-24.wav\n",
            "03-01-03-02-01-02-01.wav  03-01-06-01-01-02-01.wav  03-01-08-02-01-02-01.wav\n",
            "03-01-03-02-01-02-02.wav  03-01-06-01-01-02-02.wav  03-01-08-02-01-02-02.wav\n",
            "03-01-03-02-01-02-03.wav  03-01-06-01-01-02-03.wav  03-01-08-02-01-02-03.wav\n",
            "03-01-03-02-01-02-04.wav  03-01-06-01-01-02-04.wav  03-01-08-02-01-02-04.wav\n",
            "03-01-03-02-01-02-06.wav  03-01-06-01-01-02-06.wav  03-01-08-02-01-02-06.wav\n",
            "03-01-03-02-01-02-07.wav  03-01-06-01-01-02-07.wav  03-01-08-02-01-02-07.wav\n",
            "03-01-03-02-01-02-08.wav  03-01-06-01-01-02-08.wav  03-01-08-02-01-02-08.wav\n",
            "03-01-03-02-01-02-09.wav  03-01-06-01-01-02-09.wav  03-01-08-02-01-02-09.wav\n",
            "03-01-03-02-01-02-10.wav  03-01-06-01-01-02-10.wav  03-01-08-02-01-02-10.wav\n",
            "03-01-03-02-01-02-11.wav  03-01-06-01-01-02-11.wav  03-01-08-02-01-02-11.wav\n",
            "03-01-03-02-01-02-12.wav  03-01-06-01-01-02-12.wav  03-01-08-02-01-02-12.wav\n",
            "03-01-03-02-01-02-13.wav  03-01-06-01-01-02-13.wav  03-01-08-02-01-02-13.wav\n",
            "03-01-03-02-01-02-14.wav  03-01-06-01-01-02-14.wav  03-01-08-02-01-02-14.wav\n",
            "03-01-03-02-01-02-15.wav  03-01-06-01-01-02-15.wav  03-01-08-02-01-02-15.wav\n",
            "03-01-03-02-01-02-16.wav  03-01-06-01-01-02-16.wav  03-01-08-02-01-02-16.wav\n",
            "03-01-03-02-01-02-17.wav  03-01-06-01-01-02-17.wav  03-01-08-02-01-02-17.wav\n",
            "03-01-03-02-01-02-18.wav  03-01-06-01-01-02-18.wav  03-01-08-02-01-02-18.wav\n",
            "03-01-03-02-01-02-19.wav  03-01-06-01-01-02-19.wav  03-01-08-02-01-02-19.wav\n",
            "03-01-03-02-01-02-20.wav  03-01-06-01-01-02-20.wav  03-01-08-02-01-02-20.wav\n",
            "03-01-03-02-01-02-21.wav  03-01-06-01-01-02-21.wav  03-01-08-02-01-02-21.wav\n",
            "03-01-03-02-01-02-22.wav  03-01-06-01-01-02-22.wav  03-01-08-02-01-02-22.wav\n",
            "03-01-03-02-01-02-23.wav  03-01-06-01-01-02-23.wav  03-01-08-02-01-02-23.wav\n",
            "03-01-03-02-01-02-24.wav  03-01-06-01-01-02-24.wav  03-01-08-02-01-02-24.wav\n",
            "03-01-03-02-02-01-01.wav  03-01-06-01-02-01-01.wav  03-01-08-02-02-01-01.wav\n",
            "03-01-03-02-02-01-02.wav  03-01-06-01-02-01-02.wav  03-01-08-02-02-01-02.wav\n",
            "03-01-03-02-02-01-03.wav  03-01-06-01-02-01-03.wav  03-01-08-02-02-01-03.wav\n",
            "03-01-03-02-02-01-04.wav  03-01-06-01-02-01-04.wav  03-01-08-02-02-01-04.wav\n",
            "03-01-03-02-02-01-06.wav  03-01-06-01-02-01-06.wav  03-01-08-02-02-01-06.wav\n",
            "03-01-03-02-02-01-07.wav  03-01-06-01-02-01-07.wav  03-01-08-02-02-01-07.wav\n",
            "03-01-03-02-02-01-08.wav  03-01-06-01-02-01-08.wav  03-01-08-02-02-01-08.wav\n",
            "03-01-03-02-02-01-09.wav  03-01-06-01-02-01-09.wav  03-01-08-02-02-01-09.wav\n",
            "03-01-03-02-02-01-10.wav  03-01-06-01-02-01-10.wav  03-01-08-02-02-01-10.wav\n",
            "03-01-03-02-02-01-11.wav  03-01-06-01-02-01-11.wav  03-01-08-02-02-01-11.wav\n",
            "03-01-03-02-02-01-12.wav  03-01-06-01-02-01-12.wav  03-01-08-02-02-01-12.wav\n",
            "03-01-03-02-02-01-13.wav  03-01-06-01-02-01-13.wav  03-01-08-02-02-01-13.wav\n",
            "03-01-03-02-02-01-14.wav  03-01-06-01-02-01-14.wav  03-01-08-02-02-01-14.wav\n",
            "03-01-03-02-02-01-15.wav  03-01-06-01-02-01-15.wav  03-01-08-02-02-01-15.wav\n",
            "03-01-03-02-02-01-16.wav  03-01-06-01-02-01-16.wav  03-01-08-02-02-01-16.wav\n",
            "03-01-03-02-02-01-17.wav  03-01-06-01-02-01-17.wav  03-01-08-02-02-01-17.wav\n",
            "03-01-03-02-02-01-18.wav  03-01-06-01-02-01-18.wav  03-01-08-02-02-01-18.wav\n",
            "03-01-03-02-02-01-19.wav  03-01-06-01-02-01-19.wav  03-01-08-02-02-01-19.wav\n",
            "03-01-03-02-02-01-20.wav  03-01-06-01-02-01-20.wav  03-01-08-02-02-01-20.wav\n",
            "03-01-03-02-02-01-21.wav  03-01-06-01-02-01-21.wav  03-01-08-02-02-01-21.wav\n",
            "03-01-03-02-02-01-22.wav  03-01-06-01-02-01-22.wav  03-01-08-02-02-01-22.wav\n",
            "03-01-03-02-02-01-23.wav  03-01-06-01-02-01-23.wav  03-01-08-02-02-01-23.wav\n",
            "03-01-03-02-02-01-24.wav  03-01-06-01-02-01-24.wav  03-01-08-02-02-01-24.wav\n",
            "03-01-03-02-02-02-01.wav  03-01-06-01-02-02-01.wav  03-01-08-02-02-02-01.wav\n",
            "03-01-03-02-02-02-02.wav  03-01-06-01-02-02-02.wav  03-01-08-02-02-02-02.wav\n",
            "03-01-03-02-02-02-03.wav  03-01-06-01-02-02-03.wav  03-01-08-02-02-02-03.wav\n",
            "03-01-03-02-02-02-04.wav  03-01-06-01-02-02-04.wav  03-01-08-02-02-02-04.wav\n",
            "03-01-03-02-02-02-06.wav  03-01-06-01-02-02-06.wav  03-01-08-02-02-02-06.wav\n",
            "03-01-03-02-02-02-07.wav  03-01-06-01-02-02-07.wav  03-01-08-02-02-02-07.wav\n",
            "03-01-03-02-02-02-08.wav  03-01-06-01-02-02-08.wav  03-01-08-02-02-02-08.wav\n",
            "03-01-03-02-02-02-09.wav  03-01-06-01-02-02-09.wav  03-01-08-02-02-02-09.wav\n",
            "03-01-03-02-02-02-10.wav  03-01-06-01-02-02-10.wav  03-01-08-02-02-02-10.wav\n",
            "03-01-03-02-02-02-11.wav  03-01-06-01-02-02-11.wav  03-01-08-02-02-02-11.wav\n",
            "03-01-03-02-02-02-12.wav  03-01-06-01-02-02-12.wav  03-01-08-02-02-02-12.wav\n",
            "03-01-03-02-02-02-13.wav  03-01-06-01-02-02-13.wav  03-01-08-02-02-02-13.wav\n",
            "03-01-03-02-02-02-14.wav  03-01-06-01-02-02-14.wav  03-01-08-02-02-02-14.wav\n",
            "03-01-03-02-02-02-15.wav  03-01-06-01-02-02-15.wav  03-01-08-02-02-02-15.wav\n",
            "03-01-03-02-02-02-16.wav  03-01-06-01-02-02-16.wav  03-01-08-02-02-02-16.wav\n",
            "03-01-03-02-02-02-17.wav  03-01-06-01-02-02-17.wav  03-01-08-02-02-02-17.wav\n",
            "03-01-03-02-02-02-18.wav  03-01-06-01-02-02-18.wav  03-01-08-02-02-02-18.wav\n",
            "03-01-03-02-02-02-19.wav  03-01-06-01-02-02-19.wav  03-01-08-02-02-02-19.wav\n",
            "03-01-03-02-02-02-20.wav  03-01-06-01-02-02-20.wav  03-01-08-02-02-02-20.wav\n",
            "03-01-03-02-02-02-21.wav  03-01-06-01-02-02-21.wav  03-01-08-02-02-02-21.wav\n",
            "03-01-03-02-02-02-22.wav  03-01-06-01-02-02-22.wav  03-01-08-02-02-02-22.wav\n",
            "03-01-03-02-02-02-23.wav  03-01-06-01-02-02-23.wav  03-01-08-02-02-02-23.wav\n",
            "03-01-03-02-02-02-24.wav  03-01-06-01-02-02-24.wav  03-01-08-02-02-02-24.wav\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        " import os\n",
        "\n",
        "# README - Execute this cell to mount the notebook in your google drive. \n",
        "# Execute the cell and follow the link to sign and, paste the given key in the little text box. The credentials are only available for you. \n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/audio-dataset\"):\n",
        "  print(\"Pulling dataset\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/audio-dataset\")\n",
        "  !git clone https://github.com/emilstahl97/Audio-dataset.git\n",
        "else:\n",
        "  print(\"Dataset already exists\")\n",
        "\n",
        "#os.chdir(\"/content/drive/MyDrive/audio-dataset/Audio-dataset/Audio-dataset\")\n",
        "os.chdir(\"/content/drive/MyDrive/audio-dataset/Audio-dataset/Rawdata\")\n",
        "\n",
        "!git pull\n",
        "!ls\n",
        "\n",
        "RAVDESS_PATH = \"./RAVDESS\"\n",
        "SAVEE_PATH = \"./SAVEE\"\n",
        "SAVED_MODELS_PATH = \"../saved_models\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbZiFaPP38Hx"
      },
      "source": [
        "##**PIP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97_tuuAh3_Cc",
        "outputId": "466c0574-05cd-42e3-92cc-2f94f191bc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.21.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.4)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.28.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.4)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.6)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade keras\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade matplotlib\n",
        "!pip install --upgrade librosa\n",
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRGucpTn3YnC"
      },
      "source": [
        "##**IMPORTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOis6L1x3dWz",
        "outputId": "bf1a1eb6-2a1b-44ef-df7d-e811c0acfc31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything imported\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io.wavfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "print(\"Everything imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGjXYjveBEXU"
      },
      "source": [
        "## **Make list**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLGlHDznBIP3",
        "outputId": "0dc51f8a-8e58-4912-ac7b-5e2d0a6438d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of audio files: 1380\n",
            "03\n"
          ]
        }
      ],
      "source": [
        "mylist= os.listdir('./')\n",
        "type(mylist)\n",
        "print(\"Number of audio files: {}\".format(len(mylist)))\n",
        "\n",
        "print(mylist[1][6:-16])\n",
        "\n",
        "#print(mylist)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozz9uxHjCZTP"
      },
      "source": [
        "##**Get labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "wo7TnHteCcjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0d7c9f-2f68-4d93-d4e2-469800e36835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'male_sad', 'female_sad', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'male_angry', 'female_angry', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_fearful', 'female_fearful', 'male_calm', 'male_calm', 'male_calm', 'male_calm', 'male_calm', 'male_calm', 'male_calm', 'male_calm', 'male_happy', 'male_happy', 'male_happy', 'male_happy', 'male_happy', 'male_happy', 'male_happy', 'male_happy', 'male_sad', 'male_sad', 'male_sad', 'male_sad', 'male_sad', 'male_sad', 'male_sad', 'male_sad', 'male_angry', 'male_angry', 'male_angry', 'male_angry', 'male_angry', 'male_angry', 'male_angry', 'male_angry', 'male_fearful', 'male_fearful', 'male_fearful', 'male_fearful', 'male_fearful', 'male_fearful', 'male_fearful', 'male_fearful', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'male_calm', 'female_calm', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy', 'male_happy', 'female_happy']\n"
          ]
        }
      ],
      "source": [
        "feeling_list=[]\n",
        "for item in mylist:\n",
        "    if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_calm')\n",
        "    elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_calm')\n",
        "    elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_happy')\n",
        "    elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_happy')\n",
        "    elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_sad')\n",
        "    elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_sad')\n",
        "    elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_angry')\n",
        "    elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_angry')\n",
        "    elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
        "        feeling_list.append('female_fearful')\n",
        "    elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
        "        feeling_list.append('male_fearful')\n",
        "    elif item[:1]=='a':\n",
        "        feeling_list.append('male_angry')\n",
        "    elif item[:1]=='f':\n",
        "        feeling_list.append('male_fearful')\n",
        "    elif item[:1]=='h':\n",
        "        feeling_list.append('male_happy')\n",
        "    #elif item[:1]=='n':\n",
        "        #feeling_list.append('neutral')\n",
        "    elif item[:2]=='sa':\n",
        "        feeling_list.append('male_sad')\n",
        "\n",
        "\n",
        "print(feeling_list)\n",
        "labels = pd.DataFrame(feeling_list)\n",
        "#print(labels[500:550])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFAv2CDG18S"
      },
      "source": [
        "## Getting the features of audio files using librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "v3m7Wrl_G3Ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73dc30ca-dbc8-4f46-f0e9-36334150826c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             feature\n",
            "0  [-53.306465, -51.239155, -50.09466, -49.656696...\n",
            "1  [-52.999336, -52.90832, -52.419476, -52.407036...\n",
            "2  [-53.141987, -52.67499, -50.214226, -50.819004...\n",
            "3  [-64.933876, -64.933876, -64.933876, -64.93387...\n",
            "4  [-60.311516, -59.17811, -59.345013, -59.10887,...\n",
            "         0          1          2    ...        213        214        215\n",
            "0 -53.306465 -51.239155 -50.094662  ... -57.601852 -56.696125 -58.832737\n",
            "1 -52.999336 -52.908321 -52.419476  ... -53.704510 -53.492561 -53.001705\n",
            "2 -53.141987 -52.674992 -50.214226  ... -54.078613 -56.517986 -57.023308\n",
            "3 -64.933876 -64.933876 -64.933876  ... -64.933876 -64.933876 -64.933876\n",
            "4 -60.311516 -59.178108 -59.345013  ... -46.433640 -47.422729 -48.096535\n",
            "\n",
            "[5 rows x 216 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=['feature'])\n",
        "bookmark=0\n",
        "for index,y in enumerate(mylist):\n",
        "    if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':\n",
        "        X, sample_rate = librosa.load('./'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "        sample_rate = np.array(sample_rate)\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
        "                                            sr=sample_rate, \n",
        "                                            n_mfcc=13),\n",
        "                        axis=0)\n",
        "        feature = mfccs\n",
        "        #[float(i) for i in feature]\n",
        "        #feature1=feature[:135]\n",
        "        df.loc[bookmark] = [feature]\n",
        "        bookmark=bookmark+1\n",
        "\n",
        "\n",
        "\n",
        "print(df[:5])\n",
        "df3 = pd.DataFrame(df['feature'].values.tolist())\n",
        "print(df3[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSMN8I1BcmwP",
        "outputId": "196ab354-8650-4554-8659-8c2ccb96e230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0          1          2    ...        214        215           0  \n",
            "0   -53.306465 -51.239155 -50.094662  ... -56.696125 -58.832737    male_happy\n",
            "1   -52.999336 -52.908321 -52.419476  ... -53.492561 -53.001705  female_happy\n",
            "2   -53.141987 -52.674992 -50.214226  ... -56.517986 -57.023308    male_happy\n",
            "3   -64.933876 -64.933876 -64.933876  ... -64.933876 -64.933876  female_happy\n",
            "4   -60.311516 -59.178108 -59.345013  ... -47.422729 -48.096535    male_happy\n",
            "..         ...        ...        ...  ...        ...        ...           ...\n",
            "915 -48.113033 -48.335693 -50.188248  ... -47.045372 -48.921272  female_happy\n",
            "916 -60.501297 -62.577785 -60.446762  ... -52.179653 -53.992882    male_happy\n",
            "917 -59.302078 -57.921028 -56.350586  ... -54.675850 -56.525337  female_happy\n",
            "918 -57.354610 -56.764095 -56.161160  ... -59.327217 -62.622055    male_happy\n",
            "919 -57.820358 -57.820358 -57.820358  ... -57.820358 -57.820358  female_happy\n",
            "\n",
            "[920 rows x 217 columns]\n",
            "           0          1          2    ...        214        215             0  \n",
            "855 -65.178482 -65.065903 -65.065903  ... -52.602154 -53.381855     female_calm\n",
            "282 -29.514866 -31.776142 -36.120102  ... -48.842861 -50.810677        male_sad\n",
            "647 -45.345825 -45.345825 -45.345825  ... -44.287987 -44.375999  female_fearful\n",
            "866 -63.969830 -63.969830 -63.969830  ... -60.075424 -59.094872    female_happy\n",
            "545 -49.474644 -48.794590 -49.004753  ... -47.094955 -49.762436  female_fearful\n",
            "846 -67.851578 -68.222961 -71.009628  ... -27.564182 -24.799751       male_calm\n",
            "380 -48.647419 -48.341404 -48.439934  ... -29.235666 -28.702894      male_angry\n",
            "861 -53.185017 -47.625584 -47.243797  ... -57.382198 -57.939209     female_calm\n",
            "871 -48.981464 -47.696598 -47.206749  ... -48.981464 -48.981464    female_happy\n",
            "179 -66.139557 -66.139557 -66.139557  ... -66.139557 -66.139557      female_sad\n",
            "\n",
            "[10 rows x 217 columns]\n"
          ]
        }
      ],
      "source": [
        "newdf = pd.concat([df3,labels], axis=1)\n",
        "\n",
        "rnewdf = newdf.rename(index=str, columns={\"0\": \"label\"})\n",
        "\n",
        "print(rnewdf)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "rnewdf = shuffle(newdf)\n",
        "print(rnewdf[:10])\n",
        "\n",
        "rnewdf=rnewdf.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_lhy10dcCl"
      },
      "source": [
        "## Dividing the data into test and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9QVb-vedctd",
        "outputId": "c123992c-78fc-461a-b3ea-57138ee2069e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0          1          2    ...        214        215             0  \n",
            "328 -53.700264 -53.832932 -55.695274  ... -51.462833 -52.426777      male_angry\n",
            "630 -47.491898 -47.491898 -47.491898  ... -47.097141 -46.934822    male_fearful\n",
            "660 -58.731003 -58.746517 -58.048584  ... -54.576744 -51.933964      male_happy\n",
            "154 -54.992413 -55.728848 -55.881233  ... -50.034416 -48.532700        male_sad\n",
            "132 -36.811287 -39.613823 -47.779598  ... -49.845280 -50.194027        male_sad\n",
            "641 -48.384968 -48.573490 -48.623474  ... -46.914177 -47.299885  female_fearful\n",
            "64  -57.756821 -56.945213 -55.402336  ... -35.205841 -35.774269      male_happy\n",
            "764 -58.867973 -58.666237 -59.207607  ...   0.000000   0.000000       male_calm\n",
            "872 -55.170769 -54.091942 -53.982773  ... -56.755936 -52.897682      male_happy\n",
            "511 -64.759872 -64.759872 -64.759872  ... -37.278008 -31.755941  female_fearful\n",
            "\n",
            "[10 rows x 217 columns]\n"
          ]
        }
      ],
      "source": [
        "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
        "train = rnewdf[newdf1]\n",
        "test = rnewdf[~newdf1]\n",
        "\n",
        "print(train[250:260])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "OWtB9vs2d2mi"
      },
      "outputs": [],
      "source": [
        "trainfeatures = train.iloc[:, :-1]\n",
        "trainlabel = train.iloc[:, -1:]\n",
        "testfeatures = test.iloc[:, :-1]\n",
        "testlabel = test.iloc[:, -1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NarmNdnzeT0k",
        "outputId": "4b3ef37e-2f06-46e8-c0c4-001474c6b8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X_train = np.array(trainfeatures)\n",
        "y_train = np.array(trainlabel)\n",
        "X_test = np.array(testfeatures)\n",
        "y_test = np.array(testlabel)\n",
        "\n",
        "lb = LabelEncoder()\n",
        "\n",
        "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBcjgh9uelX9",
        "outputId": "62751fc4-f544-49fb-a490-f5d6d8e4a143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lenB6oTelMx",
        "outputId": "7e2988dd-4666-4a40-db6c-6f10b4c79b3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(737, 216)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C904jQhewnK"
      },
      "source": [
        "# Padding sequence for CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdNShUq4e0ty",
        "outputId": "4fbabc20-f487-4472-d091-8e5619c055d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad sequences\n"
          ]
        }
      ],
      "source": [
        "print('Pad sequences')\n",
        "x_traincnn =np.expand_dims(X_train, axis=2)\n",
        "x_testcnn= np.expand_dims(X_test, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "nNmxmIo9fIOx"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(216,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "opt = tensorflow.keras.optimizers.RMSprop(learning_rate =0.00001, decay=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbu_xybbfOnh",
        "outputId": "cdf467e1-15df-45a0-eb86-e340c0025895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_6 (Conv1D)           (None, 216, 128)          768       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 216, 128)          0         \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 216, 128)          82048     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 216, 128)          0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 216, 128)          0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 27, 128)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 27, 128)           0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 27, 128)           0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 27, 128)           0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 27, 128)           0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 27, 128)           82048     \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 27, 128)           0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3456)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                34570     \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 445,578\n",
            "Trainable params: 445,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "fyOH0MgMgx2A"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZWRWPKKg0Fu",
        "outputId": "97a7ec1c-e65f-4578-d9ef-b25a611eda20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2018\n",
            "12/12 [==============================] - 3s 84ms/step - loss: 2.4503 - accuracy: 0.1045 - val_loss: 2.3075 - val_accuracy: 0.1038\n",
            "Epoch 2/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.3376 - accuracy: 0.1167 - val_loss: 2.2812 - val_accuracy: 0.1366\n",
            "Epoch 3/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.3256 - accuracy: 0.1248 - val_loss: 2.2570 - val_accuracy: 0.1311\n",
            "Epoch 4/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 2.3036 - accuracy: 0.1221 - val_loss: 2.2542 - val_accuracy: 0.1366\n",
            "Epoch 5/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.2857 - accuracy: 0.1384 - val_loss: 2.2387 - val_accuracy: 0.1421\n",
            "Epoch 6/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.2612 - accuracy: 0.1601 - val_loss: 2.2305 - val_accuracy: 0.1366\n",
            "Epoch 7/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 2.2628 - accuracy: 0.1642 - val_loss: 2.2162 - val_accuracy: 0.1749\n",
            "Epoch 8/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.2319 - accuracy: 0.1777 - val_loss: 2.2116 - val_accuracy: 0.1530\n",
            "Epoch 9/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 2.2316 - accuracy: 0.1574 - val_loss: 2.2007 - val_accuracy: 0.1967\n",
            "Epoch 10/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.2216 - accuracy: 0.1845 - val_loss: 2.1968 - val_accuracy: 0.1585\n",
            "Epoch 11/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 2.2262 - accuracy: 0.1628 - val_loss: 2.1857 - val_accuracy: 0.1749\n",
            "Epoch 12/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.2132 - accuracy: 0.1777 - val_loss: 2.1774 - val_accuracy: 0.2240\n",
            "Epoch 13/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.2158 - accuracy: 0.1750 - val_loss: 2.1756 - val_accuracy: 0.2295\n",
            "Epoch 14/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.1964 - accuracy: 0.1954 - val_loss: 2.1696 - val_accuracy: 0.2077\n",
            "Epoch 15/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.1774 - accuracy: 0.1981 - val_loss: 2.1665 - val_accuracy: 0.1694\n",
            "Epoch 16/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.1733 - accuracy: 0.2076 - val_loss: 2.1538 - val_accuracy: 0.1913\n",
            "Epoch 17/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.1555 - accuracy: 0.2049 - val_loss: 2.1539 - val_accuracy: 0.1803\n",
            "Epoch 18/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.1736 - accuracy: 0.1777 - val_loss: 2.1431 - val_accuracy: 0.1913\n",
            "Epoch 19/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 2.1501 - accuracy: 0.1940 - val_loss: 2.1329 - val_accuracy: 0.2131\n",
            "Epoch 20/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.1389 - accuracy: 0.1900 - val_loss: 2.1265 - val_accuracy: 0.2240\n",
            "Epoch 21/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.1266 - accuracy: 0.2130 - val_loss: 2.1199 - val_accuracy: 0.1967\n",
            "Epoch 22/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.1209 - accuracy: 0.2049 - val_loss: 2.1121 - val_accuracy: 0.2514\n",
            "Epoch 23/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.1114 - accuracy: 0.2157 - val_loss: 2.1088 - val_accuracy: 0.2186\n",
            "Epoch 24/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 2.1231 - accuracy: 0.2130 - val_loss: 2.1024 - val_accuracy: 0.2131\n",
            "Epoch 25/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.1186 - accuracy: 0.2130 - val_loss: 2.0903 - val_accuracy: 0.2240\n",
            "Epoch 26/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0868 - accuracy: 0.2266 - val_loss: 2.0850 - val_accuracy: 0.2131\n",
            "Epoch 27/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0909 - accuracy: 0.2334 - val_loss: 2.0734 - val_accuracy: 0.2568\n",
            "Epoch 28/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0984 - accuracy: 0.1981 - val_loss: 2.0758 - val_accuracy: 0.2022\n",
            "Epoch 29/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.0865 - accuracy: 0.2307 - val_loss: 2.0582 - val_accuracy: 0.2459\n",
            "Epoch 30/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0547 - accuracy: 0.2551 - val_loss: 2.0607 - val_accuracy: 0.2350\n",
            "Epoch 31/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0419 - accuracy: 0.2646 - val_loss: 2.0632 - val_accuracy: 0.1858\n",
            "Epoch 32/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.0657 - accuracy: 0.2252 - val_loss: 2.0460 - val_accuracy: 0.2459\n",
            "Epoch 33/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 2.0525 - accuracy: 0.2347 - val_loss: 2.0475 - val_accuracy: 0.2240\n",
            "Epoch 34/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.0428 - accuracy: 0.2266 - val_loss: 2.0440 - val_accuracy: 0.1967\n",
            "Epoch 35/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.0293 - accuracy: 0.2402 - val_loss: 2.0337 - val_accuracy: 0.2240\n",
            "Epoch 36/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 2.0308 - accuracy: 0.2402 - val_loss: 2.0303 - val_accuracy: 0.2131\n",
            "Epoch 37/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0157 - accuracy: 0.2266 - val_loss: 2.0182 - val_accuracy: 0.2131\n",
            "Epoch 38/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0060 - accuracy: 0.2524 - val_loss: 1.9985 - val_accuracy: 0.2568\n",
            "Epoch 39/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0092 - accuracy: 0.2442 - val_loss: 2.0046 - val_accuracy: 0.2404\n",
            "Epoch 40/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.9992 - accuracy: 0.2388 - val_loss: 2.0000 - val_accuracy: 0.2240\n",
            "Epoch 41/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.9792 - accuracy: 0.2687 - val_loss: 1.9940 - val_accuracy: 0.2350\n",
            "Epoch 42/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.9816 - accuracy: 0.2402 - val_loss: 1.9854 - val_accuracy: 0.2514\n",
            "Epoch 43/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.9742 - accuracy: 0.2442 - val_loss: 1.9777 - val_accuracy: 0.2404\n",
            "Epoch 44/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.9746 - accuracy: 0.2510 - val_loss: 1.9738 - val_accuracy: 0.2240\n",
            "Epoch 45/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.9723 - accuracy: 0.2605 - val_loss: 1.9713 - val_accuracy: 0.2295\n",
            "Epoch 46/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.9753 - accuracy: 0.2442 - val_loss: 1.9696 - val_accuracy: 0.2404\n",
            "Epoch 47/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.9577 - accuracy: 0.2782 - val_loss: 1.9572 - val_accuracy: 0.2459\n",
            "Epoch 48/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.9678 - accuracy: 0.2469 - val_loss: 1.9545 - val_accuracy: 0.2295\n",
            "Epoch 49/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.9420 - accuracy: 0.2795 - val_loss: 1.9432 - val_accuracy: 0.2514\n",
            "Epoch 50/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.9411 - accuracy: 0.2659 - val_loss: 1.9415 - val_accuracy: 0.2678\n",
            "Epoch 51/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.9303 - accuracy: 0.2632 - val_loss: 1.9411 - val_accuracy: 0.2404\n",
            "Epoch 52/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.9141 - accuracy: 0.2687 - val_loss: 1.9352 - val_accuracy: 0.2404\n",
            "Epoch 53/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.9320 - accuracy: 0.2619 - val_loss: 1.9283 - val_accuracy: 0.2568\n",
            "Epoch 54/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.9241 - accuracy: 0.2619 - val_loss: 1.9287 - val_accuracy: 0.2514\n",
            "Epoch 55/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.8843 - accuracy: 0.2917 - val_loss: 1.9237 - val_accuracy: 0.2623\n",
            "Epoch 56/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.8893 - accuracy: 0.2972 - val_loss: 1.9259 - val_accuracy: 0.2350\n",
            "Epoch 57/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.8987 - accuracy: 0.2768 - val_loss: 1.9119 - val_accuracy: 0.2568\n",
            "Epoch 58/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.9155 - accuracy: 0.2537 - val_loss: 1.9086 - val_accuracy: 0.2623\n",
            "Epoch 59/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.8770 - accuracy: 0.2944 - val_loss: 1.8971 - val_accuracy: 0.2951\n",
            "Epoch 60/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.8988 - accuracy: 0.2809 - val_loss: 1.8969 - val_accuracy: 0.2678\n",
            "Epoch 61/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.8824 - accuracy: 0.2659 - val_loss: 1.8924 - val_accuracy: 0.2568\n",
            "Epoch 62/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.8801 - accuracy: 0.2890 - val_loss: 1.8931 - val_accuracy: 0.2514\n",
            "Epoch 63/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.8428 - accuracy: 0.3297 - val_loss: 1.8922 - val_accuracy: 0.2350\n",
            "Epoch 64/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.8529 - accuracy: 0.2904 - val_loss: 1.8797 - val_accuracy: 0.2623\n",
            "Epoch 65/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.8364 - accuracy: 0.3080 - val_loss: 1.8749 - val_accuracy: 0.2568\n",
            "Epoch 66/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.8695 - accuracy: 0.3053 - val_loss: 1.8761 - val_accuracy: 0.2568\n",
            "Epoch 67/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.8616 - accuracy: 0.2605 - val_loss: 1.8738 - val_accuracy: 0.2459\n",
            "Epoch 68/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.8237 - accuracy: 0.3216 - val_loss: 1.8779 - val_accuracy: 0.2514\n",
            "Epoch 69/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.8128 - accuracy: 0.3121 - val_loss: 1.8650 - val_accuracy: 0.2678\n",
            "Epoch 70/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.8309 - accuracy: 0.3039 - val_loss: 1.8798 - val_accuracy: 0.2623\n",
            "Epoch 71/2018\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 1.8216 - accuracy: 0.3175 - val_loss: 1.8496 - val_accuracy: 0.3005\n",
            "Epoch 72/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.7908 - accuracy: 0.3256 - val_loss: 1.8531 - val_accuracy: 0.2842\n",
            "Epoch 73/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.8132 - accuracy: 0.2985 - val_loss: 1.8451 - val_accuracy: 0.2787\n",
            "Epoch 74/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.8118 - accuracy: 0.3256 - val_loss: 1.8494 - val_accuracy: 0.2732\n",
            "Epoch 75/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.8249 - accuracy: 0.2999 - val_loss: 1.8448 - val_accuracy: 0.3005\n",
            "Epoch 76/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7953 - accuracy: 0.3189 - val_loss: 1.8460 - val_accuracy: 0.2678\n",
            "Epoch 77/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7990 - accuracy: 0.3175 - val_loss: 1.8467 - val_accuracy: 0.2951\n",
            "Epoch 78/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.8004 - accuracy: 0.3094 - val_loss: 1.8494 - val_accuracy: 0.2623\n",
            "Epoch 79/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7846 - accuracy: 0.3107 - val_loss: 1.8461 - val_accuracy: 0.2623\n",
            "Epoch 80/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.7871 - accuracy: 0.3094 - val_loss: 1.8465 - val_accuracy: 0.2568\n",
            "Epoch 81/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7871 - accuracy: 0.3446 - val_loss: 1.8409 - val_accuracy: 0.2623\n",
            "Epoch 82/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7850 - accuracy: 0.3256 - val_loss: 1.8220 - val_accuracy: 0.2732\n",
            "Epoch 83/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.7990 - accuracy: 0.3161 - val_loss: 1.8278 - val_accuracy: 0.2787\n",
            "Epoch 84/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.7862 - accuracy: 0.3107 - val_loss: 1.8292 - val_accuracy: 0.2678\n",
            "Epoch 85/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7497 - accuracy: 0.3514 - val_loss: 1.8215 - val_accuracy: 0.2678\n",
            "Epoch 86/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.7593 - accuracy: 0.3311 - val_loss: 1.8176 - val_accuracy: 0.3005\n",
            "Epoch 87/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7868 - accuracy: 0.3134 - val_loss: 1.8149 - val_accuracy: 0.2732\n",
            "Epoch 88/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.7712 - accuracy: 0.3216 - val_loss: 1.8132 - val_accuracy: 0.2896\n",
            "Epoch 89/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7713 - accuracy: 0.3121 - val_loss: 1.8129 - val_accuracy: 0.2732\n",
            "Epoch 90/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.7496 - accuracy: 0.3379 - val_loss: 1.8067 - val_accuracy: 0.2787\n",
            "Epoch 91/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7616 - accuracy: 0.3202 - val_loss: 1.8120 - val_accuracy: 0.3005\n",
            "Epoch 92/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7230 - accuracy: 0.3623 - val_loss: 1.8076 - val_accuracy: 0.3224\n",
            "Epoch 93/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.7544 - accuracy: 0.3284 - val_loss: 1.8061 - val_accuracy: 0.3060\n",
            "Epoch 94/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7472 - accuracy: 0.3284 - val_loss: 1.8024 - val_accuracy: 0.3060\n",
            "Epoch 95/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.7314 - accuracy: 0.3487 - val_loss: 1.8113 - val_accuracy: 0.2896\n",
            "Epoch 96/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7202 - accuracy: 0.3419 - val_loss: 1.7956 - val_accuracy: 0.3224\n",
            "Epoch 97/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.7324 - accuracy: 0.3460 - val_loss: 1.8005 - val_accuracy: 0.2951\n",
            "Epoch 98/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7270 - accuracy: 0.3569 - val_loss: 1.7973 - val_accuracy: 0.2787\n",
            "Epoch 99/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7289 - accuracy: 0.3419 - val_loss: 1.7955 - val_accuracy: 0.2951\n",
            "Epoch 100/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7340 - accuracy: 0.3270 - val_loss: 1.7853 - val_accuracy: 0.2787\n",
            "Epoch 101/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.7117 - accuracy: 0.3460 - val_loss: 1.7983 - val_accuracy: 0.2787\n",
            "Epoch 102/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.7252 - accuracy: 0.3474 - val_loss: 1.7844 - val_accuracy: 0.2896\n",
            "Epoch 103/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6911 - accuracy: 0.3853 - val_loss: 1.7797 - val_accuracy: 0.3060\n",
            "Epoch 104/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.6959 - accuracy: 0.3541 - val_loss: 1.7752 - val_accuracy: 0.2951\n",
            "Epoch 105/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6991 - accuracy: 0.3596 - val_loss: 1.7782 - val_accuracy: 0.2787\n",
            "Epoch 106/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6950 - accuracy: 0.3528 - val_loss: 1.7829 - val_accuracy: 0.2951\n",
            "Epoch 107/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6983 - accuracy: 0.3419 - val_loss: 1.7865 - val_accuracy: 0.2787\n",
            "Epoch 108/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6913 - accuracy: 0.3596 - val_loss: 1.7797 - val_accuracy: 0.2842\n",
            "Epoch 109/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6785 - accuracy: 0.3691 - val_loss: 1.7774 - val_accuracy: 0.2787\n",
            "Epoch 110/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.7006 - accuracy: 0.3514 - val_loss: 1.7702 - val_accuracy: 0.3060\n",
            "Epoch 111/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.6682 - accuracy: 0.3826 - val_loss: 1.7580 - val_accuracy: 0.3169\n",
            "Epoch 112/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.6818 - accuracy: 0.3487 - val_loss: 1.7709 - val_accuracy: 0.3279\n",
            "Epoch 113/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6896 - accuracy: 0.3935 - val_loss: 1.7685 - val_accuracy: 0.2951\n",
            "Epoch 114/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6604 - accuracy: 0.3881 - val_loss: 1.7602 - val_accuracy: 0.2951\n",
            "Epoch 115/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6739 - accuracy: 0.3677 - val_loss: 1.7558 - val_accuracy: 0.3060\n",
            "Epoch 116/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6566 - accuracy: 0.3731 - val_loss: 1.7578 - val_accuracy: 0.3279\n",
            "Epoch 117/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6714 - accuracy: 0.3691 - val_loss: 1.7517 - val_accuracy: 0.3060\n",
            "Epoch 118/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6538 - accuracy: 0.3921 - val_loss: 1.7440 - val_accuracy: 0.3224\n",
            "Epoch 119/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6829 - accuracy: 0.3650 - val_loss: 1.7532 - val_accuracy: 0.3005\n",
            "Epoch 120/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6428 - accuracy: 0.3772 - val_loss: 1.7633 - val_accuracy: 0.2842\n",
            "Epoch 121/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6592 - accuracy: 0.4016 - val_loss: 1.7387 - val_accuracy: 0.3115\n",
            "Epoch 122/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.6451 - accuracy: 0.3745 - val_loss: 1.7636 - val_accuracy: 0.2896\n",
            "Epoch 123/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.6670 - accuracy: 0.3691 - val_loss: 1.7361 - val_accuracy: 0.3115\n",
            "Epoch 124/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6467 - accuracy: 0.3731 - val_loss: 1.7428 - val_accuracy: 0.3115\n",
            "Epoch 125/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6308 - accuracy: 0.3704 - val_loss: 1.7456 - val_accuracy: 0.3279\n",
            "Epoch 126/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.6549 - accuracy: 0.3745 - val_loss: 1.7334 - val_accuracy: 0.3224\n",
            "Epoch 127/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6456 - accuracy: 0.3799 - val_loss: 1.7282 - val_accuracy: 0.3388\n",
            "Epoch 128/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6323 - accuracy: 0.3894 - val_loss: 1.7306 - val_accuracy: 0.3169\n",
            "Epoch 129/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6490 - accuracy: 0.3704 - val_loss: 1.7334 - val_accuracy: 0.3115\n",
            "Epoch 130/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6281 - accuracy: 0.3853 - val_loss: 1.7294 - val_accuracy: 0.3333\n",
            "Epoch 131/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6292 - accuracy: 0.3935 - val_loss: 1.7310 - val_accuracy: 0.3060\n",
            "Epoch 132/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6432 - accuracy: 0.3840 - val_loss: 1.7304 - val_accuracy: 0.3169\n",
            "Epoch 133/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6155 - accuracy: 0.4152 - val_loss: 1.7341 - val_accuracy: 0.2896\n",
            "Epoch 134/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6196 - accuracy: 0.3813 - val_loss: 1.7227 - val_accuracy: 0.2951\n",
            "Epoch 135/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6203 - accuracy: 0.3772 - val_loss: 1.7198 - val_accuracy: 0.3224\n",
            "Epoch 136/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5942 - accuracy: 0.4043 - val_loss: 1.7404 - val_accuracy: 0.2951\n",
            "Epoch 137/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.5857 - accuracy: 0.4043 - val_loss: 1.7112 - val_accuracy: 0.3115\n",
            "Epoch 138/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6057 - accuracy: 0.4057 - val_loss: 1.7143 - val_accuracy: 0.3279\n",
            "Epoch 139/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.6165 - accuracy: 0.4098 - val_loss: 1.7134 - val_accuracy: 0.3115\n",
            "Epoch 140/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5734 - accuracy: 0.4206 - val_loss: 1.7092 - val_accuracy: 0.2951\n",
            "Epoch 141/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.6069 - accuracy: 0.3935 - val_loss: 1.7138 - val_accuracy: 0.3333\n",
            "Epoch 142/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5819 - accuracy: 0.4043 - val_loss: 1.7059 - val_accuracy: 0.3169\n",
            "Epoch 143/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.5804 - accuracy: 0.4138 - val_loss: 1.7067 - val_accuracy: 0.3443\n",
            "Epoch 144/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5926 - accuracy: 0.3989 - val_loss: 1.7079 - val_accuracy: 0.2842\n",
            "Epoch 145/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5956 - accuracy: 0.4071 - val_loss: 1.6955 - val_accuracy: 0.3060\n",
            "Epoch 146/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5736 - accuracy: 0.4138 - val_loss: 1.6879 - val_accuracy: 0.3115\n",
            "Epoch 147/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.5628 - accuracy: 0.4450 - val_loss: 1.6866 - val_accuracy: 0.2896\n",
            "Epoch 148/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.5665 - accuracy: 0.4193 - val_loss: 1.6867 - val_accuracy: 0.3169\n",
            "Epoch 149/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.5608 - accuracy: 0.4166 - val_loss: 1.6793 - val_accuracy: 0.3224\n",
            "Epoch 150/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5717 - accuracy: 0.4274 - val_loss: 1.6857 - val_accuracy: 0.3224\n",
            "Epoch 151/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5611 - accuracy: 0.4084 - val_loss: 1.6759 - val_accuracy: 0.3224\n",
            "Epoch 152/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.5706 - accuracy: 0.4057 - val_loss: 1.6762 - val_accuracy: 0.3169\n",
            "Epoch 153/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.5832 - accuracy: 0.3976 - val_loss: 1.6762 - val_accuracy: 0.2951\n",
            "Epoch 154/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5320 - accuracy: 0.4410 - val_loss: 1.7092 - val_accuracy: 0.2896\n",
            "Epoch 155/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.5338 - accuracy: 0.4220 - val_loss: 1.6722 - val_accuracy: 0.3224\n",
            "Epoch 156/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.5586 - accuracy: 0.4233 - val_loss: 1.6667 - val_accuracy: 0.3169\n",
            "Epoch 157/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.5398 - accuracy: 0.4247 - val_loss: 1.6653 - val_accuracy: 0.3224\n",
            "Epoch 158/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.5554 - accuracy: 0.4233 - val_loss: 1.6703 - val_accuracy: 0.3169\n",
            "Epoch 159/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5382 - accuracy: 0.4125 - val_loss: 1.6701 - val_accuracy: 0.3169\n",
            "Epoch 160/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.5282 - accuracy: 0.4613 - val_loss: 1.6719 - val_accuracy: 0.3333\n",
            "Epoch 161/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.5428 - accuracy: 0.4233 - val_loss: 1.6577 - val_accuracy: 0.3169\n",
            "Epoch 162/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5191 - accuracy: 0.4573 - val_loss: 1.6624 - val_accuracy: 0.3224\n",
            "Epoch 163/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.4985 - accuracy: 0.4518 - val_loss: 1.6532 - val_accuracy: 0.3169\n",
            "Epoch 164/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.5028 - accuracy: 0.4586 - val_loss: 1.6605 - val_accuracy: 0.3224\n",
            "Epoch 165/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.5061 - accuracy: 0.4315 - val_loss: 1.6590 - val_accuracy: 0.3169\n",
            "Epoch 166/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5340 - accuracy: 0.4288 - val_loss: 1.6526 - val_accuracy: 0.3060\n",
            "Epoch 167/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5258 - accuracy: 0.4383 - val_loss: 1.6711 - val_accuracy: 0.3060\n",
            "Epoch 168/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.5022 - accuracy: 0.4369 - val_loss: 1.6440 - val_accuracy: 0.3333\n",
            "Epoch 169/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.5138 - accuracy: 0.4220 - val_loss: 1.6453 - val_accuracy: 0.3115\n",
            "Epoch 170/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5043 - accuracy: 0.4301 - val_loss: 1.6384 - val_accuracy: 0.3333\n",
            "Epoch 171/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.4955 - accuracy: 0.4220 - val_loss: 1.6396 - val_accuracy: 0.3552\n",
            "Epoch 172/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5059 - accuracy: 0.4355 - val_loss: 1.6307 - val_accuracy: 0.3333\n",
            "Epoch 173/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.5067 - accuracy: 0.4342 - val_loss: 1.6389 - val_accuracy: 0.3279\n",
            "Epoch 174/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4931 - accuracy: 0.4491 - val_loss: 1.6481 - val_accuracy: 0.3388\n",
            "Epoch 175/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4664 - accuracy: 0.4668 - val_loss: 1.6365 - val_accuracy: 0.3497\n",
            "Epoch 176/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.4808 - accuracy: 0.4478 - val_loss: 1.6368 - val_accuracy: 0.3333\n",
            "Epoch 177/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4831 - accuracy: 0.4559 - val_loss: 1.6353 - val_accuracy: 0.3388\n",
            "Epoch 178/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4814 - accuracy: 0.4627 - val_loss: 1.6208 - val_accuracy: 0.3388\n",
            "Epoch 179/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.4767 - accuracy: 0.4478 - val_loss: 1.6176 - val_accuracy: 0.3224\n",
            "Epoch 180/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.4584 - accuracy: 0.4559 - val_loss: 1.6321 - val_accuracy: 0.3552\n",
            "Epoch 181/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.4748 - accuracy: 0.4396 - val_loss: 1.6153 - val_accuracy: 0.3443\n",
            "Epoch 182/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4797 - accuracy: 0.4220 - val_loss: 1.6067 - val_accuracy: 0.3497\n",
            "Epoch 183/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.4763 - accuracy: 0.4586 - val_loss: 1.6038 - val_accuracy: 0.3552\n",
            "Epoch 184/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.4549 - accuracy: 0.4355 - val_loss: 1.6111 - val_accuracy: 0.3552\n",
            "Epoch 185/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4673 - accuracy: 0.4491 - val_loss: 1.6131 - val_accuracy: 0.3661\n",
            "Epoch 186/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4271 - accuracy: 0.4735 - val_loss: 1.6264 - val_accuracy: 0.3607\n",
            "Epoch 187/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4470 - accuracy: 0.4573 - val_loss: 1.5975 - val_accuracy: 0.3443\n",
            "Epoch 188/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4715 - accuracy: 0.4559 - val_loss: 1.6081 - val_accuracy: 0.3443\n",
            "Epoch 189/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4354 - accuracy: 0.4573 - val_loss: 1.6032 - val_accuracy: 0.3388\n",
            "Epoch 190/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4406 - accuracy: 0.4749 - val_loss: 1.6069 - val_accuracy: 0.3607\n",
            "Epoch 191/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4526 - accuracy: 0.4301 - val_loss: 1.6014 - val_accuracy: 0.3661\n",
            "Epoch 192/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.4246 - accuracy: 0.4803 - val_loss: 1.5878 - val_accuracy: 0.3607\n",
            "Epoch 193/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.4409 - accuracy: 0.4464 - val_loss: 1.5870 - val_accuracy: 0.3388\n",
            "Epoch 194/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4316 - accuracy: 0.4586 - val_loss: 1.6005 - val_accuracy: 0.3716\n",
            "Epoch 195/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.4203 - accuracy: 0.4722 - val_loss: 1.5973 - val_accuracy: 0.3607\n",
            "Epoch 196/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.4487 - accuracy: 0.4478 - val_loss: 1.5905 - val_accuracy: 0.3661\n",
            "Epoch 197/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4053 - accuracy: 0.4735 - val_loss: 1.5843 - val_accuracy: 0.3880\n",
            "Epoch 198/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4284 - accuracy: 0.4776 - val_loss: 1.6108 - val_accuracy: 0.3443\n",
            "Epoch 199/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.4228 - accuracy: 0.4708 - val_loss: 1.5933 - val_accuracy: 0.3169\n",
            "Epoch 200/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4275 - accuracy: 0.4776 - val_loss: 1.5897 - val_accuracy: 0.3825\n",
            "Epoch 201/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4408 - accuracy: 0.4600 - val_loss: 1.5883 - val_accuracy: 0.3661\n",
            "Epoch 202/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.4368 - accuracy: 0.4613 - val_loss: 1.5843 - val_accuracy: 0.3443\n",
            "Epoch 203/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.4156 - accuracy: 0.4573 - val_loss: 1.5789 - val_accuracy: 0.3607\n",
            "Epoch 204/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.4291 - accuracy: 0.4749 - val_loss: 1.5796 - val_accuracy: 0.3497\n",
            "Epoch 205/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4029 - accuracy: 0.4817 - val_loss: 1.5923 - val_accuracy: 0.3825\n",
            "Epoch 206/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4144 - accuracy: 0.4858 - val_loss: 1.5772 - val_accuracy: 0.3607\n",
            "Epoch 207/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.3796 - accuracy: 0.4953 - val_loss: 1.5816 - val_accuracy: 0.3552\n",
            "Epoch 208/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.4124 - accuracy: 0.4491 - val_loss: 1.5641 - val_accuracy: 0.3825\n",
            "Epoch 209/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3998 - accuracy: 0.5061 - val_loss: 1.5862 - val_accuracy: 0.3607\n",
            "Epoch 210/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3973 - accuracy: 0.4708 - val_loss: 1.5917 - val_accuracy: 0.3880\n",
            "Epoch 211/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3904 - accuracy: 0.4776 - val_loss: 1.5591 - val_accuracy: 0.3661\n",
            "Epoch 212/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4101 - accuracy: 0.4695 - val_loss: 1.5975 - val_accuracy: 0.3825\n",
            "Epoch 213/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4012 - accuracy: 0.4885 - val_loss: 1.5807 - val_accuracy: 0.3716\n",
            "Epoch 214/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3912 - accuracy: 0.4695 - val_loss: 1.5980 - val_accuracy: 0.4153\n",
            "Epoch 215/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3850 - accuracy: 0.4681 - val_loss: 1.5581 - val_accuracy: 0.3934\n",
            "Epoch 216/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3912 - accuracy: 0.4790 - val_loss: 1.5775 - val_accuracy: 0.4153\n",
            "Epoch 217/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3802 - accuracy: 0.4790 - val_loss: 1.5852 - val_accuracy: 0.3989\n",
            "Epoch 218/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3828 - accuracy: 0.4776 - val_loss: 1.5691 - val_accuracy: 0.3661\n",
            "Epoch 219/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3878 - accuracy: 0.4763 - val_loss: 1.5527 - val_accuracy: 0.3989\n",
            "Epoch 220/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3733 - accuracy: 0.4993 - val_loss: 1.5726 - val_accuracy: 0.3770\n",
            "Epoch 221/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3490 - accuracy: 0.4993 - val_loss: 1.5523 - val_accuracy: 0.3989\n",
            "Epoch 222/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3715 - accuracy: 0.4925 - val_loss: 1.5750 - val_accuracy: 0.3989\n",
            "Epoch 223/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4061 - accuracy: 0.4749 - val_loss: 1.5768 - val_accuracy: 0.3770\n",
            "Epoch 224/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3553 - accuracy: 0.5251 - val_loss: 1.5741 - val_accuracy: 0.3770\n",
            "Epoch 225/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3575 - accuracy: 0.5020 - val_loss: 1.5597 - val_accuracy: 0.3716\n",
            "Epoch 226/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3556 - accuracy: 0.4803 - val_loss: 1.5891 - val_accuracy: 0.3607\n",
            "Epoch 227/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.3622 - accuracy: 0.4939 - val_loss: 1.5646 - val_accuracy: 0.4153\n",
            "Epoch 228/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3695 - accuracy: 0.4844 - val_loss: 1.5475 - val_accuracy: 0.3770\n",
            "Epoch 229/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3806 - accuracy: 0.5007 - val_loss: 1.5519 - val_accuracy: 0.3716\n",
            "Epoch 230/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3605 - accuracy: 0.4708 - val_loss: 1.5584 - val_accuracy: 0.4153\n",
            "Epoch 231/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3298 - accuracy: 0.4980 - val_loss: 1.5493 - val_accuracy: 0.4208\n",
            "Epoch 232/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.3781 - accuracy: 0.4830 - val_loss: 1.5903 - val_accuracy: 0.3825\n",
            "Epoch 233/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.3673 - accuracy: 0.4925 - val_loss: 1.5474 - val_accuracy: 0.3880\n",
            "Epoch 234/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.3758 - accuracy: 0.4749 - val_loss: 1.5564 - val_accuracy: 0.4044\n",
            "Epoch 235/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.3492 - accuracy: 0.4966 - val_loss: 1.5419 - val_accuracy: 0.3989\n",
            "Epoch 236/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.3774 - accuracy: 0.4776 - val_loss: 1.5520 - val_accuracy: 0.4153\n",
            "Epoch 237/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3496 - accuracy: 0.4993 - val_loss: 1.5540 - val_accuracy: 0.3880\n",
            "Epoch 238/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3418 - accuracy: 0.4939 - val_loss: 1.5432 - val_accuracy: 0.4153\n",
            "Epoch 239/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3284 - accuracy: 0.5237 - val_loss: 1.5641 - val_accuracy: 0.3880\n",
            "Epoch 240/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.3502 - accuracy: 0.5305 - val_loss: 1.5654 - val_accuracy: 0.3989\n",
            "Epoch 241/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3281 - accuracy: 0.4912 - val_loss: 1.5589 - val_accuracy: 0.3880\n",
            "Epoch 242/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3728 - accuracy: 0.4939 - val_loss: 1.5320 - val_accuracy: 0.4098\n",
            "Epoch 243/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3474 - accuracy: 0.5047 - val_loss: 1.5325 - val_accuracy: 0.3934\n",
            "Epoch 244/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3395 - accuracy: 0.4925 - val_loss: 1.5432 - val_accuracy: 0.4044\n",
            "Epoch 245/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3301 - accuracy: 0.5034 - val_loss: 1.5369 - val_accuracy: 0.4153\n",
            "Epoch 246/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3320 - accuracy: 0.5156 - val_loss: 1.5349 - val_accuracy: 0.3880\n",
            "Epoch 247/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3284 - accuracy: 0.4953 - val_loss: 1.5257 - val_accuracy: 0.3934\n",
            "Epoch 248/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3376 - accuracy: 0.5007 - val_loss: 1.5338 - val_accuracy: 0.4098\n",
            "Epoch 249/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.3302 - accuracy: 0.4993 - val_loss: 1.5303 - val_accuracy: 0.4098\n",
            "Epoch 250/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3359 - accuracy: 0.4871 - val_loss: 1.5283 - val_accuracy: 0.3934\n",
            "Epoch 251/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3467 - accuracy: 0.4776 - val_loss: 1.5694 - val_accuracy: 0.4044\n",
            "Epoch 252/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3294 - accuracy: 0.5142 - val_loss: 1.5246 - val_accuracy: 0.4044\n",
            "Epoch 253/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3003 - accuracy: 0.5183 - val_loss: 1.5359 - val_accuracy: 0.3989\n",
            "Epoch 254/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3194 - accuracy: 0.5224 - val_loss: 1.5444 - val_accuracy: 0.4098\n",
            "Epoch 255/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.3369 - accuracy: 0.4939 - val_loss: 1.5234 - val_accuracy: 0.4372\n",
            "Epoch 256/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3322 - accuracy: 0.4871 - val_loss: 1.5533 - val_accuracy: 0.4044\n",
            "Epoch 257/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.3289 - accuracy: 0.5047 - val_loss: 1.5231 - val_accuracy: 0.4208\n",
            "Epoch 258/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3063 - accuracy: 0.5007 - val_loss: 1.5258 - val_accuracy: 0.3770\n",
            "Epoch 259/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3112 - accuracy: 0.5034 - val_loss: 1.5481 - val_accuracy: 0.3880\n",
            "Epoch 260/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3137 - accuracy: 0.5142 - val_loss: 1.5244 - val_accuracy: 0.4153\n",
            "Epoch 261/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3289 - accuracy: 0.4966 - val_loss: 1.5158 - val_accuracy: 0.4426\n",
            "Epoch 262/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3009 - accuracy: 0.5088 - val_loss: 1.5333 - val_accuracy: 0.4208\n",
            "Epoch 263/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3049 - accuracy: 0.5237 - val_loss: 1.5194 - val_accuracy: 0.4153\n",
            "Epoch 264/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2812 - accuracy: 0.5319 - val_loss: 1.5294 - val_accuracy: 0.4044\n",
            "Epoch 265/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2944 - accuracy: 0.5305 - val_loss: 1.5315 - val_accuracy: 0.4098\n",
            "Epoch 266/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.2909 - accuracy: 0.5183 - val_loss: 1.5230 - val_accuracy: 0.4153\n",
            "Epoch 267/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.2958 - accuracy: 0.5061 - val_loss: 1.5195 - val_accuracy: 0.3880\n",
            "Epoch 268/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3108 - accuracy: 0.5034 - val_loss: 1.5239 - val_accuracy: 0.4098\n",
            "Epoch 269/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3067 - accuracy: 0.5183 - val_loss: 1.5173 - val_accuracy: 0.4208\n",
            "Epoch 270/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3018 - accuracy: 0.5115 - val_loss: 1.5257 - val_accuracy: 0.4153\n",
            "Epoch 271/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2669 - accuracy: 0.5360 - val_loss: 1.5786 - val_accuracy: 0.3934\n",
            "Epoch 272/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2954 - accuracy: 0.5278 - val_loss: 1.5240 - val_accuracy: 0.4317\n",
            "Epoch 273/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2923 - accuracy: 0.5183 - val_loss: 1.5321 - val_accuracy: 0.4153\n",
            "Epoch 274/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2634 - accuracy: 0.5509 - val_loss: 1.5831 - val_accuracy: 0.4044\n",
            "Epoch 275/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3026 - accuracy: 0.5210 - val_loss: 1.5081 - val_accuracy: 0.4153\n",
            "Epoch 276/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2728 - accuracy: 0.5346 - val_loss: 1.5283 - val_accuracy: 0.4317\n",
            "Epoch 277/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3059 - accuracy: 0.5061 - val_loss: 1.5469 - val_accuracy: 0.3934\n",
            "Epoch 278/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.2697 - accuracy: 0.5332 - val_loss: 1.5171 - val_accuracy: 0.4044\n",
            "Epoch 279/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2627 - accuracy: 0.5292 - val_loss: 1.5181 - val_accuracy: 0.4153\n",
            "Epoch 280/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2964 - accuracy: 0.5115 - val_loss: 1.5370 - val_accuracy: 0.4208\n",
            "Epoch 281/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2882 - accuracy: 0.5332 - val_loss: 1.5547 - val_accuracy: 0.3989\n",
            "Epoch 282/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.2840 - accuracy: 0.5292 - val_loss: 1.5196 - val_accuracy: 0.4317\n",
            "Epoch 283/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2769 - accuracy: 0.5373 - val_loss: 1.5190 - val_accuracy: 0.4098\n",
            "Epoch 284/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2874 - accuracy: 0.5251 - val_loss: 1.5235 - val_accuracy: 0.4098\n",
            "Epoch 285/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2751 - accuracy: 0.5237 - val_loss: 1.5402 - val_accuracy: 0.4208\n",
            "Epoch 286/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2617 - accuracy: 0.5197 - val_loss: 1.5176 - val_accuracy: 0.4372\n",
            "Epoch 287/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2734 - accuracy: 0.5224 - val_loss: 1.5791 - val_accuracy: 0.3825\n",
            "Epoch 288/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2884 - accuracy: 0.5102 - val_loss: 1.5184 - val_accuracy: 0.4044\n",
            "Epoch 289/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2504 - accuracy: 0.5346 - val_loss: 1.5124 - val_accuracy: 0.4426\n",
            "Epoch 290/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2554 - accuracy: 0.5400 - val_loss: 1.5105 - val_accuracy: 0.4208\n",
            "Epoch 291/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2699 - accuracy: 0.5292 - val_loss: 1.5099 - val_accuracy: 0.4098\n",
            "Epoch 292/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.2863 - accuracy: 0.5332 - val_loss: 1.5016 - val_accuracy: 0.4044\n",
            "Epoch 293/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2762 - accuracy: 0.5319 - val_loss: 1.5099 - val_accuracy: 0.4426\n",
            "Epoch 294/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2661 - accuracy: 0.5387 - val_loss: 1.5433 - val_accuracy: 0.4044\n",
            "Epoch 295/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2556 - accuracy: 0.5468 - val_loss: 1.5049 - val_accuracy: 0.4372\n",
            "Epoch 296/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2486 - accuracy: 0.5427 - val_loss: 1.4992 - val_accuracy: 0.4262\n",
            "Epoch 297/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2730 - accuracy: 0.5360 - val_loss: 1.5101 - val_accuracy: 0.4153\n",
            "Epoch 298/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2565 - accuracy: 0.5332 - val_loss: 1.5028 - val_accuracy: 0.4372\n",
            "Epoch 299/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2473 - accuracy: 0.5468 - val_loss: 1.5048 - val_accuracy: 0.4153\n",
            "Epoch 300/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2434 - accuracy: 0.5373 - val_loss: 1.5039 - val_accuracy: 0.4208\n",
            "Epoch 301/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2566 - accuracy: 0.5278 - val_loss: 1.5079 - val_accuracy: 0.4208\n",
            "Epoch 302/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2508 - accuracy: 0.5360 - val_loss: 1.5111 - val_accuracy: 0.4153\n",
            "Epoch 303/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.2619 - accuracy: 0.5360 - val_loss: 1.5045 - val_accuracy: 0.4262\n",
            "Epoch 304/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2477 - accuracy: 0.5441 - val_loss: 1.5051 - val_accuracy: 0.4426\n",
            "Epoch 305/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2502 - accuracy: 0.5360 - val_loss: 1.5061 - val_accuracy: 0.4044\n",
            "Epoch 306/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2579 - accuracy: 0.5495 - val_loss: 1.4987 - val_accuracy: 0.4098\n",
            "Epoch 307/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2573 - accuracy: 0.5197 - val_loss: 1.4964 - val_accuracy: 0.4044\n",
            "Epoch 308/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.2358 - accuracy: 0.5468 - val_loss: 1.5129 - val_accuracy: 0.4481\n",
            "Epoch 309/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2075 - accuracy: 0.5821 - val_loss: 1.4968 - val_accuracy: 0.4372\n",
            "Epoch 310/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2401 - accuracy: 0.5563 - val_loss: 1.5026 - val_accuracy: 0.4317\n",
            "Epoch 311/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2453 - accuracy: 0.5360 - val_loss: 1.5092 - val_accuracy: 0.4262\n",
            "Epoch 312/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2878 - accuracy: 0.5170 - val_loss: 1.5103 - val_accuracy: 0.4262\n",
            "Epoch 313/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2490 - accuracy: 0.5441 - val_loss: 1.4967 - val_accuracy: 0.4208\n",
            "Epoch 314/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2247 - accuracy: 0.5495 - val_loss: 1.4942 - val_accuracy: 0.4372\n",
            "Epoch 315/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2260 - accuracy: 0.5414 - val_loss: 1.4933 - val_accuracy: 0.4372\n",
            "Epoch 316/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.2220 - accuracy: 0.5292 - val_loss: 1.5216 - val_accuracy: 0.4153\n",
            "Epoch 317/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2212 - accuracy: 0.5468 - val_loss: 1.5415 - val_accuracy: 0.4153\n",
            "Epoch 318/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2165 - accuracy: 0.5563 - val_loss: 1.5076 - val_accuracy: 0.4262\n",
            "Epoch 319/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2246 - accuracy: 0.5550 - val_loss: 1.5100 - val_accuracy: 0.4153\n",
            "Epoch 320/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2244 - accuracy: 0.5441 - val_loss: 1.4911 - val_accuracy: 0.4481\n",
            "Epoch 321/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2403 - accuracy: 0.5319 - val_loss: 1.4983 - val_accuracy: 0.4426\n",
            "Epoch 322/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2414 - accuracy: 0.5427 - val_loss: 1.4875 - val_accuracy: 0.4262\n",
            "Epoch 323/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2265 - accuracy: 0.5441 - val_loss: 1.5021 - val_accuracy: 0.4262\n",
            "Epoch 324/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2195 - accuracy: 0.5509 - val_loss: 1.5186 - val_accuracy: 0.4208\n",
            "Epoch 325/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2082 - accuracy: 0.5495 - val_loss: 1.4916 - val_accuracy: 0.4262\n",
            "Epoch 326/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2153 - accuracy: 0.5807 - val_loss: 1.4888 - val_accuracy: 0.4481\n",
            "Epoch 327/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2011 - accuracy: 0.5712 - val_loss: 1.4872 - val_accuracy: 0.4208\n",
            "Epoch 328/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2258 - accuracy: 0.5332 - val_loss: 1.5302 - val_accuracy: 0.4044\n",
            "Epoch 329/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2013 - accuracy: 0.5455 - val_loss: 1.5412 - val_accuracy: 0.3880\n",
            "Epoch 330/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2212 - accuracy: 0.5482 - val_loss: 1.4910 - val_accuracy: 0.4317\n",
            "Epoch 331/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2214 - accuracy: 0.5577 - val_loss: 1.5458 - val_accuracy: 0.3880\n",
            "Epoch 332/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2259 - accuracy: 0.5373 - val_loss: 1.4817 - val_accuracy: 0.4262\n",
            "Epoch 333/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2406 - accuracy: 0.5278 - val_loss: 1.5556 - val_accuracy: 0.4098\n",
            "Epoch 334/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2191 - accuracy: 0.5360 - val_loss: 1.4902 - val_accuracy: 0.4426\n",
            "Epoch 335/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2268 - accuracy: 0.5509 - val_loss: 1.4808 - val_accuracy: 0.4481\n",
            "Epoch 336/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1849 - accuracy: 0.5699 - val_loss: 1.4931 - val_accuracy: 0.4208\n",
            "Epoch 337/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2031 - accuracy: 0.5726 - val_loss: 1.5093 - val_accuracy: 0.4590\n",
            "Epoch 338/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1948 - accuracy: 0.5400 - val_loss: 1.4862 - val_accuracy: 0.4372\n",
            "Epoch 339/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2115 - accuracy: 0.5536 - val_loss: 1.4829 - val_accuracy: 0.4481\n",
            "Epoch 340/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.2056 - accuracy: 0.5536 - val_loss: 1.4979 - val_accuracy: 0.4426\n",
            "Epoch 341/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1939 - accuracy: 0.5482 - val_loss: 1.4865 - val_accuracy: 0.4317\n",
            "Epoch 342/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.1810 - accuracy: 0.5617 - val_loss: 1.4810 - val_accuracy: 0.4317\n",
            "Epoch 343/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.2041 - accuracy: 0.5604 - val_loss: 1.5019 - val_accuracy: 0.4372\n",
            "Epoch 344/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.2012 - accuracy: 0.5522 - val_loss: 1.4778 - val_accuracy: 0.4481\n",
            "Epoch 345/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1787 - accuracy: 0.5672 - val_loss: 1.4779 - val_accuracy: 0.4426\n",
            "Epoch 346/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2006 - accuracy: 0.5685 - val_loss: 1.4796 - val_accuracy: 0.4481\n",
            "Epoch 347/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1752 - accuracy: 0.5509 - val_loss: 1.5102 - val_accuracy: 0.4098\n",
            "Epoch 348/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1735 - accuracy: 0.5672 - val_loss: 1.5389 - val_accuracy: 0.3934\n",
            "Epoch 349/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1987 - accuracy: 0.5590 - val_loss: 1.5111 - val_accuracy: 0.4317\n",
            "Epoch 350/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.1655 - accuracy: 0.5712 - val_loss: 1.4813 - val_accuracy: 0.4044\n",
            "Epoch 351/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1883 - accuracy: 0.5522 - val_loss: 1.5197 - val_accuracy: 0.4317\n",
            "Epoch 352/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.2018 - accuracy: 0.5604 - val_loss: 1.4814 - val_accuracy: 0.4481\n",
            "Epoch 353/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1780 - accuracy: 0.5780 - val_loss: 1.4863 - val_accuracy: 0.4317\n",
            "Epoch 354/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1619 - accuracy: 0.5726 - val_loss: 1.5035 - val_accuracy: 0.4098\n",
            "Epoch 355/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1985 - accuracy: 0.5739 - val_loss: 1.4849 - val_accuracy: 0.4372\n",
            "Epoch 356/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1939 - accuracy: 0.5631 - val_loss: 1.4734 - val_accuracy: 0.4372\n",
            "Epoch 357/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1681 - accuracy: 0.5604 - val_loss: 1.5230 - val_accuracy: 0.4372\n",
            "Epoch 358/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1713 - accuracy: 0.5699 - val_loss: 1.4864 - val_accuracy: 0.4317\n",
            "Epoch 359/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1800 - accuracy: 0.5550 - val_loss: 1.4715 - val_accuracy: 0.4426\n",
            "Epoch 360/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1779 - accuracy: 0.5645 - val_loss: 1.4816 - val_accuracy: 0.4536\n",
            "Epoch 361/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1849 - accuracy: 0.5590 - val_loss: 1.4786 - val_accuracy: 0.4317\n",
            "Epoch 362/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1692 - accuracy: 0.5848 - val_loss: 1.4980 - val_accuracy: 0.4208\n",
            "Epoch 363/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1770 - accuracy: 0.5780 - val_loss: 1.4812 - val_accuracy: 0.4208\n",
            "Epoch 364/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1875 - accuracy: 0.5617 - val_loss: 1.4767 - val_accuracy: 0.4536\n",
            "Epoch 365/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1935 - accuracy: 0.5536 - val_loss: 1.4959 - val_accuracy: 0.4590\n",
            "Epoch 366/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2062 - accuracy: 0.5414 - val_loss: 1.4820 - val_accuracy: 0.4208\n",
            "Epoch 367/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1567 - accuracy: 0.5780 - val_loss: 1.4725 - val_accuracy: 0.4262\n",
            "Epoch 368/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1585 - accuracy: 0.5848 - val_loss: 1.4642 - val_accuracy: 0.4426\n",
            "Epoch 369/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.1507 - accuracy: 0.5821 - val_loss: 1.4704 - val_accuracy: 0.4262\n",
            "Epoch 370/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1777 - accuracy: 0.5631 - val_loss: 1.4748 - val_accuracy: 0.4590\n",
            "Epoch 371/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1703 - accuracy: 0.5726 - val_loss: 1.4748 - val_accuracy: 0.4481\n",
            "Epoch 372/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1802 - accuracy: 0.5726 - val_loss: 1.4771 - val_accuracy: 0.4536\n",
            "Epoch 373/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1797 - accuracy: 0.5807 - val_loss: 1.4641 - val_accuracy: 0.4262\n",
            "Epoch 374/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1501 - accuracy: 0.5794 - val_loss: 1.4902 - val_accuracy: 0.4317\n",
            "Epoch 375/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.1633 - accuracy: 0.5780 - val_loss: 1.4670 - val_accuracy: 0.4262\n",
            "Epoch 376/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1633 - accuracy: 0.5767 - val_loss: 1.5021 - val_accuracy: 0.4098\n",
            "Epoch 377/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1448 - accuracy: 0.5699 - val_loss: 1.4597 - val_accuracy: 0.4317\n",
            "Epoch 378/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1553 - accuracy: 0.5916 - val_loss: 1.4986 - val_accuracy: 0.4098\n",
            "Epoch 379/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.1374 - accuracy: 0.5834 - val_loss: 1.4686 - val_accuracy: 0.4481\n",
            "Epoch 380/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1403 - accuracy: 0.5889 - val_loss: 1.4577 - val_accuracy: 0.4372\n",
            "Epoch 381/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.1666 - accuracy: 0.5821 - val_loss: 1.4735 - val_accuracy: 0.4372\n",
            "Epoch 382/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1330 - accuracy: 0.5848 - val_loss: 1.4858 - val_accuracy: 0.4262\n",
            "Epoch 383/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1530 - accuracy: 0.5821 - val_loss: 1.4960 - val_accuracy: 0.4208\n",
            "Epoch 384/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.1638 - accuracy: 0.5767 - val_loss: 1.4711 - val_accuracy: 0.4426\n",
            "Epoch 385/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1401 - accuracy: 0.5739 - val_loss: 1.5123 - val_accuracy: 0.4426\n",
            "Epoch 386/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1558 - accuracy: 0.5807 - val_loss: 1.5222 - val_accuracy: 0.4317\n",
            "Epoch 387/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1471 - accuracy: 0.5658 - val_loss: 1.4677 - val_accuracy: 0.4372\n",
            "Epoch 388/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.1213 - accuracy: 0.5957 - val_loss: 1.4610 - val_accuracy: 0.4590\n",
            "Epoch 389/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1354 - accuracy: 0.5821 - val_loss: 1.4666 - val_accuracy: 0.4372\n",
            "Epoch 390/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1417 - accuracy: 0.5821 - val_loss: 1.5273 - val_accuracy: 0.4481\n",
            "Epoch 391/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1602 - accuracy: 0.5726 - val_loss: 1.5196 - val_accuracy: 0.4098\n",
            "Epoch 392/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1375 - accuracy: 0.5794 - val_loss: 1.4654 - val_accuracy: 0.4645\n",
            "Epoch 393/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1454 - accuracy: 0.5726 - val_loss: 1.4713 - val_accuracy: 0.4372\n",
            "Epoch 394/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1404 - accuracy: 0.5875 - val_loss: 1.4796 - val_accuracy: 0.4426\n",
            "Epoch 395/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1092 - accuracy: 0.6011 - val_loss: 1.4628 - val_accuracy: 0.4536\n",
            "Epoch 396/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1207 - accuracy: 0.5916 - val_loss: 1.4599 - val_accuracy: 0.4481\n",
            "Epoch 397/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1216 - accuracy: 0.5943 - val_loss: 1.4705 - val_accuracy: 0.4317\n",
            "Epoch 398/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1280 - accuracy: 0.5889 - val_loss: 1.4642 - val_accuracy: 0.4536\n",
            "Epoch 399/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1053 - accuracy: 0.6038 - val_loss: 1.5042 - val_accuracy: 0.4481\n",
            "Epoch 400/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1271 - accuracy: 0.5997 - val_loss: 1.4684 - val_accuracy: 0.4426\n",
            "Epoch 401/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1285 - accuracy: 0.5821 - val_loss: 1.4586 - val_accuracy: 0.4536\n",
            "Epoch 402/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1306 - accuracy: 0.5862 - val_loss: 1.4471 - val_accuracy: 0.4590\n",
            "Epoch 403/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1329 - accuracy: 0.6079 - val_loss: 1.4781 - val_accuracy: 0.4426\n",
            "Epoch 404/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.1122 - accuracy: 0.6065 - val_loss: 1.4957 - val_accuracy: 0.4863\n",
            "Epoch 405/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1313 - accuracy: 0.6079 - val_loss: 1.4604 - val_accuracy: 0.4590\n",
            "Epoch 406/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1524 - accuracy: 0.5753 - val_loss: 1.4629 - val_accuracy: 0.4645\n",
            "Epoch 407/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1113 - accuracy: 0.6024 - val_loss: 1.4733 - val_accuracy: 0.4481\n",
            "Epoch 408/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1198 - accuracy: 0.5807 - val_loss: 1.4705 - val_accuracy: 0.4590\n",
            "Epoch 409/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1238 - accuracy: 0.5699 - val_loss: 1.4517 - val_accuracy: 0.4426\n",
            "Epoch 410/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1247 - accuracy: 0.5780 - val_loss: 1.4501 - val_accuracy: 0.4536\n",
            "Epoch 411/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0998 - accuracy: 0.6119 - val_loss: 1.4943 - val_accuracy: 0.4481\n",
            "Epoch 412/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1194 - accuracy: 0.5821 - val_loss: 1.4564 - val_accuracy: 0.4481\n",
            "Epoch 413/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1314 - accuracy: 0.5807 - val_loss: 1.4602 - val_accuracy: 0.4426\n",
            "Epoch 414/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1164 - accuracy: 0.5902 - val_loss: 1.4524 - val_accuracy: 0.4536\n",
            "Epoch 415/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1240 - accuracy: 0.5712 - val_loss: 1.4655 - val_accuracy: 0.4699\n",
            "Epoch 416/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.1164 - accuracy: 0.5821 - val_loss: 1.4546 - val_accuracy: 0.4699\n",
            "Epoch 417/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1152 - accuracy: 0.5767 - val_loss: 1.5444 - val_accuracy: 0.4317\n",
            "Epoch 418/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1236 - accuracy: 0.6038 - val_loss: 1.4476 - val_accuracy: 0.4590\n",
            "Epoch 419/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1060 - accuracy: 0.5984 - val_loss: 1.4503 - val_accuracy: 0.4590\n",
            "Epoch 420/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1032 - accuracy: 0.5970 - val_loss: 1.4893 - val_accuracy: 0.4426\n",
            "Epoch 421/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0893 - accuracy: 0.6242 - val_loss: 1.4783 - val_accuracy: 0.4481\n",
            "Epoch 422/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0998 - accuracy: 0.6079 - val_loss: 1.4479 - val_accuracy: 0.4645\n",
            "Epoch 423/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1235 - accuracy: 0.5943 - val_loss: 1.4626 - val_accuracy: 0.4699\n",
            "Epoch 424/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1055 - accuracy: 0.5834 - val_loss: 1.4575 - val_accuracy: 0.4317\n",
            "Epoch 425/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0933 - accuracy: 0.5970 - val_loss: 1.4934 - val_accuracy: 0.4372\n",
            "Epoch 426/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0941 - accuracy: 0.6119 - val_loss: 1.4513 - val_accuracy: 0.4536\n",
            "Epoch 427/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.1194 - accuracy: 0.5875 - val_loss: 1.5296 - val_accuracy: 0.4372\n",
            "Epoch 428/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1160 - accuracy: 0.5807 - val_loss: 1.4629 - val_accuracy: 0.4536\n",
            "Epoch 429/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.1037 - accuracy: 0.6106 - val_loss: 1.4645 - val_accuracy: 0.4536\n",
            "Epoch 430/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.1072 - accuracy: 0.5617 - val_loss: 1.4506 - val_accuracy: 0.4481\n",
            "Epoch 431/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1164 - accuracy: 0.5902 - val_loss: 1.4519 - val_accuracy: 0.4590\n",
            "Epoch 432/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0672 - accuracy: 0.6187 - val_loss: 1.4503 - val_accuracy: 0.4645\n",
            "Epoch 433/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.0837 - accuracy: 0.6011 - val_loss: 1.4450 - val_accuracy: 0.4699\n",
            "Epoch 434/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0846 - accuracy: 0.6309 - val_loss: 1.4700 - val_accuracy: 0.4153\n",
            "Epoch 435/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0987 - accuracy: 0.6052 - val_loss: 1.4506 - val_accuracy: 0.4590\n",
            "Epoch 436/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.1098 - accuracy: 0.5957 - val_loss: 1.4690 - val_accuracy: 0.4426\n",
            "Epoch 437/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0900 - accuracy: 0.5957 - val_loss: 1.4543 - val_accuracy: 0.4645\n",
            "Epoch 438/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0863 - accuracy: 0.6201 - val_loss: 1.4993 - val_accuracy: 0.4262\n",
            "Epoch 439/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0940 - accuracy: 0.5929 - val_loss: 1.4535 - val_accuracy: 0.4262\n",
            "Epoch 440/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.0740 - accuracy: 0.6309 - val_loss: 1.4641 - val_accuracy: 0.4645\n",
            "Epoch 441/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0680 - accuracy: 0.6065 - val_loss: 1.4523 - val_accuracy: 0.4754\n",
            "Epoch 442/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0980 - accuracy: 0.5916 - val_loss: 1.4708 - val_accuracy: 0.4809\n",
            "Epoch 443/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0831 - accuracy: 0.6174 - val_loss: 1.4479 - val_accuracy: 0.4590\n",
            "Epoch 444/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0832 - accuracy: 0.6282 - val_loss: 1.4640 - val_accuracy: 0.4645\n",
            "Epoch 445/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0760 - accuracy: 0.6038 - val_loss: 1.4650 - val_accuracy: 0.4754\n",
            "Epoch 446/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0922 - accuracy: 0.6269 - val_loss: 1.4752 - val_accuracy: 0.4372\n",
            "Epoch 447/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0501 - accuracy: 0.6174 - val_loss: 1.4477 - val_accuracy: 0.4481\n",
            "Epoch 448/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0604 - accuracy: 0.6133 - val_loss: 1.4816 - val_accuracy: 0.4536\n",
            "Epoch 449/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0516 - accuracy: 0.6282 - val_loss: 1.4751 - val_accuracy: 0.4645\n",
            "Epoch 450/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0449 - accuracy: 0.6296 - val_loss: 1.4813 - val_accuracy: 0.4481\n",
            "Epoch 451/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0720 - accuracy: 0.6119 - val_loss: 1.4837 - val_accuracy: 0.4699\n",
            "Epoch 452/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0835 - accuracy: 0.5916 - val_loss: 1.4566 - val_accuracy: 0.4809\n",
            "Epoch 453/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0727 - accuracy: 0.6228 - val_loss: 1.4640 - val_accuracy: 0.4863\n",
            "Epoch 454/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0690 - accuracy: 0.6133 - val_loss: 1.4421 - val_accuracy: 0.4645\n",
            "Epoch 455/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0660 - accuracy: 0.6174 - val_loss: 1.4388 - val_accuracy: 0.4645\n",
            "Epoch 456/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0714 - accuracy: 0.6214 - val_loss: 1.4696 - val_accuracy: 0.4536\n",
            "Epoch 457/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.0530 - accuracy: 0.6092 - val_loss: 1.4405 - val_accuracy: 0.4645\n",
            "Epoch 458/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0705 - accuracy: 0.6133 - val_loss: 1.4375 - val_accuracy: 0.4699\n",
            "Epoch 459/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0572 - accuracy: 0.6282 - val_loss: 1.4646 - val_accuracy: 0.4645\n",
            "Epoch 460/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.0529 - accuracy: 0.6065 - val_loss: 1.4452 - val_accuracy: 0.4645\n",
            "Epoch 461/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.0675 - accuracy: 0.6147 - val_loss: 1.4568 - val_accuracy: 0.4754\n",
            "Epoch 462/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 1.0709 - accuracy: 0.6309 - val_loss: 1.4519 - val_accuracy: 0.4645\n",
            "Epoch 463/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0481 - accuracy: 0.6147 - val_loss: 1.4305 - val_accuracy: 0.4754\n",
            "Epoch 464/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0456 - accuracy: 0.6269 - val_loss: 1.4915 - val_accuracy: 0.4372\n",
            "Epoch 465/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.0435 - accuracy: 0.6418 - val_loss: 1.4454 - val_accuracy: 0.4645\n",
            "Epoch 466/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0379 - accuracy: 0.6106 - val_loss: 1.5204 - val_accuracy: 0.4645\n",
            "Epoch 467/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0673 - accuracy: 0.6187 - val_loss: 1.4500 - val_accuracy: 0.4426\n",
            "Epoch 468/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0496 - accuracy: 0.6269 - val_loss: 1.4393 - val_accuracy: 0.4590\n",
            "Epoch 469/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0623 - accuracy: 0.6092 - val_loss: 1.4773 - val_accuracy: 0.4590\n",
            "Epoch 470/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0206 - accuracy: 0.6431 - val_loss: 1.4327 - val_accuracy: 0.4754\n",
            "Epoch 471/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0332 - accuracy: 0.6214 - val_loss: 1.4399 - val_accuracy: 0.4590\n",
            "Epoch 472/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0448 - accuracy: 0.6214 - val_loss: 1.4572 - val_accuracy: 0.4699\n",
            "Epoch 473/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0535 - accuracy: 0.5984 - val_loss: 1.4527 - val_accuracy: 0.4536\n",
            "Epoch 474/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0508 - accuracy: 0.6079 - val_loss: 1.4530 - val_accuracy: 0.4645\n",
            "Epoch 475/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0487 - accuracy: 0.6228 - val_loss: 1.4346 - val_accuracy: 0.4699\n",
            "Epoch 476/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.0457 - accuracy: 0.6296 - val_loss: 1.4715 - val_accuracy: 0.4481\n",
            "Epoch 477/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0530 - accuracy: 0.6065 - val_loss: 1.4410 - val_accuracy: 0.4645\n",
            "Epoch 478/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0480 - accuracy: 0.6214 - val_loss: 1.4531 - val_accuracy: 0.4809\n",
            "Epoch 479/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.0550 - accuracy: 0.6255 - val_loss: 1.4402 - val_accuracy: 0.4645\n",
            "Epoch 480/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0238 - accuracy: 0.6418 - val_loss: 1.4529 - val_accuracy: 0.4590\n",
            "Epoch 481/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0256 - accuracy: 0.6255 - val_loss: 1.4337 - val_accuracy: 0.4699\n",
            "Epoch 482/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0578 - accuracy: 0.6065 - val_loss: 1.4502 - val_accuracy: 0.4481\n",
            "Epoch 483/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0377 - accuracy: 0.6269 - val_loss: 1.4606 - val_accuracy: 0.4536\n",
            "Epoch 484/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0413 - accuracy: 0.6119 - val_loss: 1.4562 - val_accuracy: 0.4809\n",
            "Epoch 485/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0239 - accuracy: 0.6255 - val_loss: 1.4346 - val_accuracy: 0.4809\n",
            "Epoch 486/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0227 - accuracy: 0.6269 - val_loss: 1.4399 - val_accuracy: 0.4590\n",
            "Epoch 487/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0159 - accuracy: 0.6228 - val_loss: 1.4323 - val_accuracy: 0.4590\n",
            "Epoch 488/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0241 - accuracy: 0.6404 - val_loss: 1.4416 - val_accuracy: 0.4699\n",
            "Epoch 489/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.0251 - accuracy: 0.6445 - val_loss: 1.4414 - val_accuracy: 0.4536\n",
            "Epoch 490/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0301 - accuracy: 0.6214 - val_loss: 1.4774 - val_accuracy: 0.4590\n",
            "Epoch 491/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0072 - accuracy: 0.6445 - val_loss: 1.4654 - val_accuracy: 0.4317\n",
            "Epoch 492/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.0299 - accuracy: 0.6187 - val_loss: 1.4338 - val_accuracy: 0.4754\n",
            "Epoch 493/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.0322 - accuracy: 0.6296 - val_loss: 1.4317 - val_accuracy: 0.4863\n",
            "Epoch 494/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.0137 - accuracy: 0.6119 - val_loss: 1.4757 - val_accuracy: 0.4754\n",
            "Epoch 495/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0291 - accuracy: 0.6282 - val_loss: 1.4354 - val_accuracy: 0.4918\n",
            "Epoch 496/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0274 - accuracy: 0.6228 - val_loss: 1.4306 - val_accuracy: 0.4590\n",
            "Epoch 497/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9919 - accuracy: 0.6364 - val_loss: 1.4480 - val_accuracy: 0.4481\n",
            "Epoch 498/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0472 - accuracy: 0.6133 - val_loss: 1.4377 - val_accuracy: 0.4590\n",
            "Epoch 499/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0122 - accuracy: 0.6404 - val_loss: 1.4452 - val_accuracy: 0.4481\n",
            "Epoch 500/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0173 - accuracy: 0.6336 - val_loss: 1.4339 - val_accuracy: 0.4699\n",
            "Epoch 501/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.0047 - accuracy: 0.6214 - val_loss: 1.4257 - val_accuracy: 0.4590\n",
            "Epoch 502/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0041 - accuracy: 0.6404 - val_loss: 1.4838 - val_accuracy: 0.4754\n",
            "Epoch 503/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0357 - accuracy: 0.6187 - val_loss: 1.4237 - val_accuracy: 0.4699\n",
            "Epoch 504/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0292 - accuracy: 0.6214 - val_loss: 1.4225 - val_accuracy: 0.4809\n",
            "Epoch 505/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0097 - accuracy: 0.6350 - val_loss: 1.4327 - val_accuracy: 0.4536\n",
            "Epoch 506/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0097 - accuracy: 0.6513 - val_loss: 1.4339 - val_accuracy: 0.4699\n",
            "Epoch 507/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0024 - accuracy: 0.6391 - val_loss: 1.4477 - val_accuracy: 0.4809\n",
            "Epoch 508/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.0132 - accuracy: 0.6350 - val_loss: 1.4279 - val_accuracy: 0.4645\n",
            "Epoch 509/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.0091 - accuracy: 0.6391 - val_loss: 1.4314 - val_accuracy: 0.4645\n",
            "Epoch 510/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0089 - accuracy: 0.6377 - val_loss: 1.4387 - val_accuracy: 0.4699\n",
            "Epoch 511/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0250 - accuracy: 0.6255 - val_loss: 1.4420 - val_accuracy: 0.4590\n",
            "Epoch 512/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.0092 - accuracy: 0.6296 - val_loss: 1.4221 - val_accuracy: 0.4754\n",
            "Epoch 513/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9945 - accuracy: 0.6472 - val_loss: 1.4665 - val_accuracy: 0.4863\n",
            "Epoch 514/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9892 - accuracy: 0.6486 - val_loss: 1.4272 - val_accuracy: 0.4809\n",
            "Epoch 515/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0126 - accuracy: 0.6391 - val_loss: 1.4463 - val_accuracy: 0.4590\n",
            "Epoch 516/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0039 - accuracy: 0.6526 - val_loss: 1.4346 - val_accuracy: 0.4918\n",
            "Epoch 517/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9962 - accuracy: 0.6554 - val_loss: 1.4479 - val_accuracy: 0.4699\n",
            "Epoch 518/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.9914 - accuracy: 0.6431 - val_loss: 1.4495 - val_accuracy: 0.4809\n",
            "Epoch 519/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9833 - accuracy: 0.6472 - val_loss: 1.4414 - val_accuracy: 0.4536\n",
            "Epoch 520/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9982 - accuracy: 0.6499 - val_loss: 1.4491 - val_accuracy: 0.4699\n",
            "Epoch 521/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9996 - accuracy: 0.6364 - val_loss: 1.4258 - val_accuracy: 0.4590\n",
            "Epoch 522/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9738 - accuracy: 0.6635 - val_loss: 1.4483 - val_accuracy: 0.4809\n",
            "Epoch 523/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9834 - accuracy: 0.6486 - val_loss: 1.4455 - val_accuracy: 0.4645\n",
            "Epoch 524/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9999 - accuracy: 0.6391 - val_loss: 1.4442 - val_accuracy: 0.4699\n",
            "Epoch 525/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9975 - accuracy: 0.6526 - val_loss: 1.5008 - val_accuracy: 0.4699\n",
            "Epoch 526/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0031 - accuracy: 0.6431 - val_loss: 1.4290 - val_accuracy: 0.4863\n",
            "Epoch 527/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9750 - accuracy: 0.6567 - val_loss: 1.4413 - val_accuracy: 0.4918\n",
            "Epoch 528/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9769 - accuracy: 0.6608 - val_loss: 1.4311 - val_accuracy: 0.4754\n",
            "Epoch 529/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9976 - accuracy: 0.6364 - val_loss: 1.4291 - val_accuracy: 0.4863\n",
            "Epoch 530/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9636 - accuracy: 0.6581 - val_loss: 1.4259 - val_accuracy: 0.4918\n",
            "Epoch 531/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.9807 - accuracy: 0.6418 - val_loss: 1.4366 - val_accuracy: 0.4590\n",
            "Epoch 532/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.9812 - accuracy: 0.6445 - val_loss: 1.4193 - val_accuracy: 0.4754\n",
            "Epoch 533/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.9836 - accuracy: 0.6513 - val_loss: 1.4607 - val_accuracy: 0.4536\n",
            "Epoch 534/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9669 - accuracy: 0.6513 - val_loss: 1.4332 - val_accuracy: 0.4536\n",
            "Epoch 535/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9825 - accuracy: 0.6445 - val_loss: 1.4303 - val_accuracy: 0.4973\n",
            "Epoch 536/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9800 - accuracy: 0.6635 - val_loss: 1.4188 - val_accuracy: 0.4536\n",
            "Epoch 537/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9707 - accuracy: 0.6540 - val_loss: 1.4174 - val_accuracy: 0.4809\n",
            "Epoch 538/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9578 - accuracy: 0.6581 - val_loss: 1.4184 - val_accuracy: 0.4809\n",
            "Epoch 539/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9750 - accuracy: 0.6459 - val_loss: 1.4181 - val_accuracy: 0.4754\n",
            "Epoch 540/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9797 - accuracy: 0.6554 - val_loss: 1.4149 - val_accuracy: 0.4863\n",
            "Epoch 541/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9567 - accuracy: 0.6513 - val_loss: 1.4482 - val_accuracy: 0.4809\n",
            "Epoch 542/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.9725 - accuracy: 0.6404 - val_loss: 1.4669 - val_accuracy: 0.4809\n",
            "Epoch 543/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0064 - accuracy: 0.6431 - val_loss: 1.4242 - val_accuracy: 0.4590\n",
            "Epoch 544/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.9427 - accuracy: 0.6499 - val_loss: 1.4197 - val_accuracy: 0.4863\n",
            "Epoch 545/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9548 - accuracy: 0.6581 - val_loss: 1.4424 - val_accuracy: 0.4863\n",
            "Epoch 546/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9527 - accuracy: 0.6540 - val_loss: 1.4791 - val_accuracy: 0.4426\n",
            "Epoch 547/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9875 - accuracy: 0.6445 - val_loss: 1.4282 - val_accuracy: 0.4863\n",
            "Epoch 548/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9833 - accuracy: 0.6377 - val_loss: 1.4617 - val_accuracy: 0.5027\n",
            "Epoch 549/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9713 - accuracy: 0.6567 - val_loss: 1.4173 - val_accuracy: 0.4809\n",
            "Epoch 550/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9715 - accuracy: 0.6459 - val_loss: 1.4230 - val_accuracy: 0.4754\n",
            "Epoch 551/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9521 - accuracy: 0.6594 - val_loss: 1.5348 - val_accuracy: 0.4317\n",
            "Epoch 552/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9896 - accuracy: 0.6486 - val_loss: 1.4248 - val_accuracy: 0.4809\n",
            "Epoch 553/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9410 - accuracy: 0.6594 - val_loss: 1.4203 - val_accuracy: 0.4809\n",
            "Epoch 554/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9374 - accuracy: 0.6689 - val_loss: 1.4241 - val_accuracy: 0.4918\n",
            "Epoch 555/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9694 - accuracy: 0.6608 - val_loss: 1.4241 - val_accuracy: 0.4918\n",
            "Epoch 556/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9555 - accuracy: 0.6540 - val_loss: 1.4179 - val_accuracy: 0.4754\n",
            "Epoch 557/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9322 - accuracy: 0.6526 - val_loss: 1.4191 - val_accuracy: 0.4809\n",
            "Epoch 558/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9860 - accuracy: 0.6459 - val_loss: 1.4418 - val_accuracy: 0.4536\n",
            "Epoch 559/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9370 - accuracy: 0.6839 - val_loss: 1.4327 - val_accuracy: 0.4699\n",
            "Epoch 560/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9417 - accuracy: 0.6703 - val_loss: 1.4168 - val_accuracy: 0.4754\n",
            "Epoch 561/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9415 - accuracy: 0.6581 - val_loss: 1.4683 - val_accuracy: 0.4918\n",
            "Epoch 562/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9533 - accuracy: 0.6771 - val_loss: 1.4224 - val_accuracy: 0.4699\n",
            "Epoch 563/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9406 - accuracy: 0.6594 - val_loss: 1.4256 - val_accuracy: 0.4863\n",
            "Epoch 564/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9397 - accuracy: 0.6662 - val_loss: 1.4762 - val_accuracy: 0.4863\n",
            "Epoch 565/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9687 - accuracy: 0.6513 - val_loss: 1.4242 - val_accuracy: 0.4973\n",
            "Epoch 566/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9219 - accuracy: 0.6649 - val_loss: 1.4338 - val_accuracy: 0.4754\n",
            "Epoch 567/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9246 - accuracy: 0.6744 - val_loss: 1.4521 - val_accuracy: 0.4590\n",
            "Epoch 568/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9424 - accuracy: 0.6716 - val_loss: 1.4186 - val_accuracy: 0.4699\n",
            "Epoch 569/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9340 - accuracy: 0.6676 - val_loss: 1.4145 - val_accuracy: 0.4809\n",
            "Epoch 570/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9333 - accuracy: 0.6567 - val_loss: 1.4347 - val_accuracy: 0.4863\n",
            "Epoch 571/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9368 - accuracy: 0.6784 - val_loss: 1.4142 - val_accuracy: 0.4809\n",
            "Epoch 572/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9184 - accuracy: 0.6621 - val_loss: 1.4145 - val_accuracy: 0.4754\n",
            "Epoch 573/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9351 - accuracy: 0.6703 - val_loss: 1.4307 - val_accuracy: 0.4754\n",
            "Epoch 574/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9247 - accuracy: 0.6554 - val_loss: 1.4669 - val_accuracy: 0.4481\n",
            "Epoch 575/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9320 - accuracy: 0.6635 - val_loss: 1.4102 - val_accuracy: 0.4918\n",
            "Epoch 576/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9193 - accuracy: 0.6730 - val_loss: 1.4440 - val_accuracy: 0.4590\n",
            "Epoch 577/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9228 - accuracy: 0.6662 - val_loss: 1.4226 - val_accuracy: 0.4699\n",
            "Epoch 578/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9256 - accuracy: 0.6744 - val_loss: 1.4097 - val_accuracy: 0.4754\n",
            "Epoch 579/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9417 - accuracy: 0.6635 - val_loss: 1.4086 - val_accuracy: 0.4754\n",
            "Epoch 580/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9048 - accuracy: 0.6879 - val_loss: 1.4180 - val_accuracy: 0.4809\n",
            "Epoch 581/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9037 - accuracy: 0.6811 - val_loss: 1.4061 - val_accuracy: 0.4699\n",
            "Epoch 582/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.9328 - accuracy: 0.6635 - val_loss: 1.4175 - val_accuracy: 0.4918\n",
            "Epoch 583/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9264 - accuracy: 0.6662 - val_loss: 1.4270 - val_accuracy: 0.4809\n",
            "Epoch 584/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8972 - accuracy: 0.6879 - val_loss: 1.4298 - val_accuracy: 0.4809\n",
            "Epoch 585/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9055 - accuracy: 0.6825 - val_loss: 1.4306 - val_accuracy: 0.4863\n",
            "Epoch 586/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9128 - accuracy: 0.6703 - val_loss: 1.4263 - val_accuracy: 0.4754\n",
            "Epoch 587/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9060 - accuracy: 0.6825 - val_loss: 1.4229 - val_accuracy: 0.4863\n",
            "Epoch 588/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9128 - accuracy: 0.6716 - val_loss: 1.4126 - val_accuracy: 0.4863\n",
            "Epoch 589/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9114 - accuracy: 0.6662 - val_loss: 1.4444 - val_accuracy: 0.4863\n",
            "Epoch 590/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9230 - accuracy: 0.6676 - val_loss: 1.4300 - val_accuracy: 0.4973\n",
            "Epoch 591/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9246 - accuracy: 0.6662 - val_loss: 1.4274 - val_accuracy: 0.4590\n",
            "Epoch 592/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9238 - accuracy: 0.6621 - val_loss: 1.4132 - val_accuracy: 0.4754\n",
            "Epoch 593/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9207 - accuracy: 0.6676 - val_loss: 1.4230 - val_accuracy: 0.4699\n",
            "Epoch 594/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9164 - accuracy: 0.6649 - val_loss: 1.4602 - val_accuracy: 0.4645\n",
            "Epoch 595/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8879 - accuracy: 0.6879 - val_loss: 1.4188 - val_accuracy: 0.4863\n",
            "Epoch 596/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9098 - accuracy: 0.6771 - val_loss: 1.4473 - val_accuracy: 0.4536\n",
            "Epoch 597/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9185 - accuracy: 0.6798 - val_loss: 1.4220 - val_accuracy: 0.4809\n",
            "Epoch 598/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.9260 - accuracy: 0.6784 - val_loss: 1.4019 - val_accuracy: 0.4918\n",
            "Epoch 599/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.9171 - accuracy: 0.6676 - val_loss: 1.4060 - val_accuracy: 0.4809\n",
            "Epoch 600/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.8883 - accuracy: 0.6798 - val_loss: 1.4584 - val_accuracy: 0.4973\n",
            "Epoch 601/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8998 - accuracy: 0.6716 - val_loss: 1.4167 - val_accuracy: 0.4699\n",
            "Epoch 602/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9003 - accuracy: 0.6798 - val_loss: 1.4253 - val_accuracy: 0.4536\n",
            "Epoch 603/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9315 - accuracy: 0.6744 - val_loss: 1.4361 - val_accuracy: 0.4973\n",
            "Epoch 604/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9041 - accuracy: 0.6798 - val_loss: 1.4206 - val_accuracy: 0.4863\n",
            "Epoch 605/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8830 - accuracy: 0.6961 - val_loss: 1.4129 - val_accuracy: 0.4590\n",
            "Epoch 606/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8985 - accuracy: 0.6771 - val_loss: 1.4238 - val_accuracy: 0.4590\n",
            "Epoch 607/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8739 - accuracy: 0.6866 - val_loss: 1.4134 - val_accuracy: 0.4809\n",
            "Epoch 608/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8836 - accuracy: 0.6879 - val_loss: 1.4140 - val_accuracy: 0.4809\n",
            "Epoch 609/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8970 - accuracy: 0.6757 - val_loss: 1.4654 - val_accuracy: 0.4481\n",
            "Epoch 610/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9121 - accuracy: 0.6757 - val_loss: 1.4042 - val_accuracy: 0.4863\n",
            "Epoch 611/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8844 - accuracy: 0.6798 - val_loss: 1.4142 - val_accuracy: 0.4809\n",
            "Epoch 612/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8888 - accuracy: 0.6757 - val_loss: 1.4419 - val_accuracy: 0.5082\n",
            "Epoch 613/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.8829 - accuracy: 0.6879 - val_loss: 1.4038 - val_accuracy: 0.4699\n",
            "Epoch 614/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8874 - accuracy: 0.6825 - val_loss: 1.4132 - val_accuracy: 0.4809\n",
            "Epoch 615/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.8970 - accuracy: 0.6649 - val_loss: 1.4144 - val_accuracy: 0.4863\n",
            "Epoch 616/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8851 - accuracy: 0.6771 - val_loss: 1.4112 - val_accuracy: 0.4973\n",
            "Epoch 617/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8937 - accuracy: 0.6730 - val_loss: 1.4052 - val_accuracy: 0.4809\n",
            "Epoch 618/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.8680 - accuracy: 0.7015 - val_loss: 1.4481 - val_accuracy: 0.4699\n",
            "Epoch 619/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8932 - accuracy: 0.6798 - val_loss: 1.4132 - val_accuracy: 0.4918\n",
            "Epoch 620/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8863 - accuracy: 0.6798 - val_loss: 1.4140 - val_accuracy: 0.4754\n",
            "Epoch 621/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8708 - accuracy: 0.7015 - val_loss: 1.4081 - val_accuracy: 0.4918\n",
            "Epoch 622/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9051 - accuracy: 0.6716 - val_loss: 1.4086 - val_accuracy: 0.4863\n",
            "Epoch 623/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.8700 - accuracy: 0.6988 - val_loss: 1.4394 - val_accuracy: 0.4699\n",
            "Epoch 624/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8903 - accuracy: 0.6974 - val_loss: 1.4001 - val_accuracy: 0.4918\n",
            "Epoch 625/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8510 - accuracy: 0.7232 - val_loss: 1.4102 - val_accuracy: 0.4809\n",
            "Epoch 626/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8726 - accuracy: 0.6961 - val_loss: 1.4159 - val_accuracy: 0.4863\n",
            "Epoch 627/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8710 - accuracy: 0.6920 - val_loss: 1.4163 - val_accuracy: 0.4863\n",
            "Epoch 628/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8638 - accuracy: 0.6906 - val_loss: 1.4066 - val_accuracy: 0.4699\n",
            "Epoch 629/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8574 - accuracy: 0.7096 - val_loss: 1.4026 - val_accuracy: 0.4809\n",
            "Epoch 630/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.8882 - accuracy: 0.6730 - val_loss: 1.3998 - val_accuracy: 0.4918\n",
            "Epoch 631/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8646 - accuracy: 0.7015 - val_loss: 1.4169 - val_accuracy: 0.4863\n",
            "Epoch 632/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8612 - accuracy: 0.6961 - val_loss: 1.4205 - val_accuracy: 0.4918\n",
            "Epoch 633/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8698 - accuracy: 0.6947 - val_loss: 1.5001 - val_accuracy: 0.4754\n",
            "Epoch 634/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8892 - accuracy: 0.6635 - val_loss: 1.4096 - val_accuracy: 0.4863\n",
            "Epoch 635/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8840 - accuracy: 0.7015 - val_loss: 1.4128 - val_accuracy: 0.4973\n",
            "Epoch 636/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8571 - accuracy: 0.6934 - val_loss: 1.4327 - val_accuracy: 0.5027\n",
            "Epoch 637/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.8402 - accuracy: 0.7042 - val_loss: 1.4082 - val_accuracy: 0.4699\n",
            "Epoch 638/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8806 - accuracy: 0.6703 - val_loss: 1.4239 - val_accuracy: 0.4918\n",
            "Epoch 639/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8572 - accuracy: 0.7056 - val_loss: 1.4260 - val_accuracy: 0.4809\n",
            "Epoch 640/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8579 - accuracy: 0.7069 - val_loss: 1.4142 - val_accuracy: 0.4918\n",
            "Epoch 641/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8629 - accuracy: 0.7042 - val_loss: 1.4147 - val_accuracy: 0.4645\n",
            "Epoch 642/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8688 - accuracy: 0.7028 - val_loss: 1.4312 - val_accuracy: 0.4754\n",
            "Epoch 643/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8645 - accuracy: 0.6934 - val_loss: 1.4236 - val_accuracy: 0.4809\n",
            "Epoch 644/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8713 - accuracy: 0.6934 - val_loss: 1.4034 - val_accuracy: 0.4699\n",
            "Epoch 645/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8510 - accuracy: 0.7110 - val_loss: 1.4286 - val_accuracy: 0.4973\n",
            "Epoch 646/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8473 - accuracy: 0.7164 - val_loss: 1.4149 - val_accuracy: 0.4863\n",
            "Epoch 647/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8466 - accuracy: 0.7069 - val_loss: 1.4262 - val_accuracy: 0.4754\n",
            "Epoch 648/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8460 - accuracy: 0.6920 - val_loss: 1.4269 - val_accuracy: 0.5027\n",
            "Epoch 649/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8610 - accuracy: 0.6893 - val_loss: 1.4074 - val_accuracy: 0.5082\n",
            "Epoch 650/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8547 - accuracy: 0.6879 - val_loss: 1.4361 - val_accuracy: 0.4754\n",
            "Epoch 651/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8432 - accuracy: 0.7028 - val_loss: 1.4089 - val_accuracy: 0.4754\n",
            "Epoch 652/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8421 - accuracy: 0.6988 - val_loss: 1.4397 - val_accuracy: 0.4754\n",
            "Epoch 653/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8420 - accuracy: 0.7015 - val_loss: 1.4038 - val_accuracy: 0.4809\n",
            "Epoch 654/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8501 - accuracy: 0.7069 - val_loss: 1.4096 - val_accuracy: 0.4809\n",
            "Epoch 655/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.8377 - accuracy: 0.7069 - val_loss: 1.4165 - val_accuracy: 0.4918\n",
            "Epoch 656/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8530 - accuracy: 0.7083 - val_loss: 1.4113 - val_accuracy: 0.4809\n",
            "Epoch 657/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.8259 - accuracy: 0.6974 - val_loss: 1.4079 - val_accuracy: 0.4918\n",
            "Epoch 658/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8702 - accuracy: 0.6689 - val_loss: 1.4363 - val_accuracy: 0.5027\n",
            "Epoch 659/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8437 - accuracy: 0.6839 - val_loss: 1.4209 - val_accuracy: 0.4809\n",
            "Epoch 660/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8276 - accuracy: 0.7096 - val_loss: 1.4301 - val_accuracy: 0.4809\n",
            "Epoch 661/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8241 - accuracy: 0.7164 - val_loss: 1.4510 - val_accuracy: 0.4754\n",
            "Epoch 662/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8532 - accuracy: 0.7015 - val_loss: 1.4383 - val_accuracy: 0.5027\n",
            "Epoch 663/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8335 - accuracy: 0.7151 - val_loss: 1.4423 - val_accuracy: 0.4918\n",
            "Epoch 664/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.8469 - accuracy: 0.6934 - val_loss: 1.4252 - val_accuracy: 0.4918\n",
            "Epoch 665/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.8422 - accuracy: 0.7083 - val_loss: 1.4344 - val_accuracy: 0.4973\n",
            "Epoch 666/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8482 - accuracy: 0.7123 - val_loss: 1.4189 - val_accuracy: 0.4918\n",
            "Epoch 667/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8426 - accuracy: 0.7191 - val_loss: 1.4042 - val_accuracy: 0.4918\n",
            "Epoch 668/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8223 - accuracy: 0.6893 - val_loss: 1.4118 - val_accuracy: 0.4754\n",
            "Epoch 669/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8424 - accuracy: 0.7028 - val_loss: 1.4179 - val_accuracy: 0.4754\n",
            "Epoch 670/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8331 - accuracy: 0.6934 - val_loss: 1.4392 - val_accuracy: 0.4918\n",
            "Epoch 671/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8297 - accuracy: 0.6988 - val_loss: 1.4428 - val_accuracy: 0.4918\n",
            "Epoch 672/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8328 - accuracy: 0.6988 - val_loss: 1.4181 - val_accuracy: 0.4973\n",
            "Epoch 673/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8349 - accuracy: 0.7218 - val_loss: 1.4286 - val_accuracy: 0.4699\n",
            "Epoch 674/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8112 - accuracy: 0.7123 - val_loss: 1.4085 - val_accuracy: 0.4699\n",
            "Epoch 675/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8032 - accuracy: 0.7137 - val_loss: 1.4039 - val_accuracy: 0.4809\n",
            "Epoch 676/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8103 - accuracy: 0.7205 - val_loss: 1.4204 - val_accuracy: 0.4973\n",
            "Epoch 677/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8481 - accuracy: 0.6879 - val_loss: 1.4045 - val_accuracy: 0.4918\n",
            "Epoch 678/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8090 - accuracy: 0.6988 - val_loss: 1.4239 - val_accuracy: 0.4809\n",
            "Epoch 679/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.8177 - accuracy: 0.7300 - val_loss: 1.4465 - val_accuracy: 0.4809\n",
            "Epoch 680/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8240 - accuracy: 0.7096 - val_loss: 1.4184 - val_accuracy: 0.4918\n",
            "Epoch 681/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8149 - accuracy: 0.7327 - val_loss: 1.4132 - val_accuracy: 0.4863\n",
            "Epoch 682/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7871 - accuracy: 0.7286 - val_loss: 1.4530 - val_accuracy: 0.4918\n",
            "Epoch 683/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8212 - accuracy: 0.7164 - val_loss: 1.4066 - val_accuracy: 0.5027\n",
            "Epoch 684/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8376 - accuracy: 0.7191 - val_loss: 1.4244 - val_accuracy: 0.5027\n",
            "Epoch 685/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7984 - accuracy: 0.7246 - val_loss: 1.4052 - val_accuracy: 0.4754\n",
            "Epoch 686/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8044 - accuracy: 0.7137 - val_loss: 1.4138 - val_accuracy: 0.5082\n",
            "Epoch 687/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8161 - accuracy: 0.7232 - val_loss: 1.4333 - val_accuracy: 0.4754\n",
            "Epoch 688/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8097 - accuracy: 0.7042 - val_loss: 1.4184 - val_accuracy: 0.4863\n",
            "Epoch 689/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.8038 - accuracy: 0.7341 - val_loss: 1.4012 - val_accuracy: 0.4918\n",
            "Epoch 690/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8248 - accuracy: 0.6947 - val_loss: 1.4198 - val_accuracy: 0.5082\n",
            "Epoch 691/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7598 - accuracy: 0.7273 - val_loss: 1.4129 - val_accuracy: 0.4918\n",
            "Epoch 692/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7894 - accuracy: 0.7096 - val_loss: 1.4274 - val_accuracy: 0.4809\n",
            "Epoch 693/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.7944 - accuracy: 0.7273 - val_loss: 1.4125 - val_accuracy: 0.5082\n",
            "Epoch 694/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8008 - accuracy: 0.7259 - val_loss: 1.4093 - val_accuracy: 0.4973\n",
            "Epoch 695/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7887 - accuracy: 0.7246 - val_loss: 1.4220 - val_accuracy: 0.4754\n",
            "Epoch 696/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7769 - accuracy: 0.7327 - val_loss: 1.4382 - val_accuracy: 0.4699\n",
            "Epoch 697/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7749 - accuracy: 0.7096 - val_loss: 1.4220 - val_accuracy: 0.4645\n",
            "Epoch 698/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7928 - accuracy: 0.7205 - val_loss: 1.4162 - val_accuracy: 0.5082\n",
            "Epoch 699/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8090 - accuracy: 0.7164 - val_loss: 1.4312 - val_accuracy: 0.4973\n",
            "Epoch 700/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.7922 - accuracy: 0.7273 - val_loss: 1.4199 - val_accuracy: 0.4754\n",
            "Epoch 701/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7761 - accuracy: 0.7164 - val_loss: 1.4140 - val_accuracy: 0.4863\n",
            "Epoch 702/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8191 - accuracy: 0.6961 - val_loss: 1.4022 - val_accuracy: 0.4918\n",
            "Epoch 703/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7894 - accuracy: 0.7151 - val_loss: 1.4120 - val_accuracy: 0.5027\n",
            "Epoch 704/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7872 - accuracy: 0.7191 - val_loss: 1.4169 - val_accuracy: 0.4863\n",
            "Epoch 705/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7912 - accuracy: 0.7123 - val_loss: 1.4105 - val_accuracy: 0.4973\n",
            "Epoch 706/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7935 - accuracy: 0.7354 - val_loss: 1.4109 - val_accuracy: 0.4918\n",
            "Epoch 707/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7666 - accuracy: 0.7368 - val_loss: 1.4113 - val_accuracy: 0.4918\n",
            "Epoch 708/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7504 - accuracy: 0.7368 - val_loss: 1.4147 - val_accuracy: 0.4918\n",
            "Epoch 709/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.7882 - accuracy: 0.7164 - val_loss: 1.4039 - val_accuracy: 0.5027\n",
            "Epoch 710/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7797 - accuracy: 0.7205 - val_loss: 1.4275 - val_accuracy: 0.5082\n",
            "Epoch 711/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7964 - accuracy: 0.7178 - val_loss: 1.4893 - val_accuracy: 0.4863\n",
            "Epoch 712/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7774 - accuracy: 0.7151 - val_loss: 1.4162 - val_accuracy: 0.4863\n",
            "Epoch 713/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7852 - accuracy: 0.7205 - val_loss: 1.4145 - val_accuracy: 0.4918\n",
            "Epoch 714/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7788 - accuracy: 0.7246 - val_loss: 1.4243 - val_accuracy: 0.4754\n",
            "Epoch 715/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7916 - accuracy: 0.7246 - val_loss: 1.4295 - val_accuracy: 0.5027\n",
            "Epoch 716/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7665 - accuracy: 0.7246 - val_loss: 1.4524 - val_accuracy: 0.4754\n",
            "Epoch 717/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7710 - accuracy: 0.7273 - val_loss: 1.4339 - val_accuracy: 0.4809\n",
            "Epoch 718/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7814 - accuracy: 0.7313 - val_loss: 1.4230 - val_accuracy: 0.5301\n",
            "Epoch 719/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.7897 - accuracy: 0.7137 - val_loss: 1.4155 - val_accuracy: 0.5027\n",
            "Epoch 720/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7624 - accuracy: 0.7259 - val_loss: 1.4184 - val_accuracy: 0.4863\n",
            "Epoch 721/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.7763 - accuracy: 0.7313 - val_loss: 1.4329 - val_accuracy: 0.4809\n",
            "Epoch 722/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7716 - accuracy: 0.7218 - val_loss: 1.4372 - val_accuracy: 0.4863\n",
            "Epoch 723/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7606 - accuracy: 0.7273 - val_loss: 1.4092 - val_accuracy: 0.4918\n",
            "Epoch 724/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7624 - accuracy: 0.7110 - val_loss: 1.4333 - val_accuracy: 0.4973\n",
            "Epoch 725/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7670 - accuracy: 0.7191 - val_loss: 1.4443 - val_accuracy: 0.5191\n",
            "Epoch 726/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7851 - accuracy: 0.7137 - val_loss: 1.4169 - val_accuracy: 0.4973\n",
            "Epoch 727/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7714 - accuracy: 0.7123 - val_loss: 1.4391 - val_accuracy: 0.4973\n",
            "Epoch 728/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7503 - accuracy: 0.7395 - val_loss: 1.4392 - val_accuracy: 0.4973\n",
            "Epoch 729/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7624 - accuracy: 0.7395 - val_loss: 1.4346 - val_accuracy: 0.5082\n",
            "Epoch 730/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7195 - accuracy: 0.7612 - val_loss: 1.4271 - val_accuracy: 0.4863\n",
            "Epoch 731/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7603 - accuracy: 0.7408 - val_loss: 1.4584 - val_accuracy: 0.4918\n",
            "Epoch 732/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7491 - accuracy: 0.7327 - val_loss: 1.4497 - val_accuracy: 0.4918\n",
            "Epoch 733/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7631 - accuracy: 0.7408 - val_loss: 1.4152 - val_accuracy: 0.4918\n",
            "Epoch 734/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7563 - accuracy: 0.7327 - val_loss: 1.4322 - val_accuracy: 0.4754\n",
            "Epoch 735/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7636 - accuracy: 0.7368 - val_loss: 1.4273 - val_accuracy: 0.4973\n",
            "Epoch 736/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7472 - accuracy: 0.7341 - val_loss: 1.4241 - val_accuracy: 0.4973\n",
            "Epoch 737/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.7412 - accuracy: 0.7354 - val_loss: 1.4131 - val_accuracy: 0.4973\n",
            "Epoch 738/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7509 - accuracy: 0.7354 - val_loss: 1.4356 - val_accuracy: 0.4863\n",
            "Epoch 739/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7570 - accuracy: 0.7422 - val_loss: 1.4138 - val_accuracy: 0.4918\n",
            "Epoch 740/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7525 - accuracy: 0.7354 - val_loss: 1.4406 - val_accuracy: 0.5082\n",
            "Epoch 741/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.7609 - accuracy: 0.7341 - val_loss: 1.4386 - val_accuracy: 0.4863\n",
            "Epoch 742/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7453 - accuracy: 0.7408 - val_loss: 1.4196 - val_accuracy: 0.4918\n",
            "Epoch 743/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.7374 - accuracy: 0.7436 - val_loss: 1.4350 - val_accuracy: 0.5027\n",
            "Epoch 744/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7588 - accuracy: 0.7354 - val_loss: 1.4173 - val_accuracy: 0.4863\n",
            "Epoch 745/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7481 - accuracy: 0.7313 - val_loss: 1.4250 - val_accuracy: 0.4918\n",
            "Epoch 746/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7505 - accuracy: 0.7354 - val_loss: 1.4223 - val_accuracy: 0.4918\n",
            "Epoch 747/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7489 - accuracy: 0.7232 - val_loss: 1.4177 - val_accuracy: 0.4918\n",
            "Epoch 748/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7710 - accuracy: 0.7327 - val_loss: 1.4048 - val_accuracy: 0.4918\n",
            "Epoch 749/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7443 - accuracy: 0.7436 - val_loss: 1.4125 - val_accuracy: 0.4863\n",
            "Epoch 750/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.7424 - accuracy: 0.7300 - val_loss: 1.4153 - val_accuracy: 0.4863\n",
            "Epoch 751/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7242 - accuracy: 0.7612 - val_loss: 1.4239 - val_accuracy: 0.4863\n",
            "Epoch 752/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.7410 - accuracy: 0.7408 - val_loss: 1.4456 - val_accuracy: 0.5191\n",
            "Epoch 753/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7141 - accuracy: 0.7544 - val_loss: 1.4104 - val_accuracy: 0.4918\n",
            "Epoch 754/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7399 - accuracy: 0.7463 - val_loss: 1.4077 - val_accuracy: 0.4863\n",
            "Epoch 755/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7306 - accuracy: 0.7503 - val_loss: 1.4019 - val_accuracy: 0.5027\n",
            "Epoch 756/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7223 - accuracy: 0.7313 - val_loss: 1.4174 - val_accuracy: 0.4918\n",
            "Epoch 757/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.7531 - accuracy: 0.7286 - val_loss: 1.4459 - val_accuracy: 0.5082\n",
            "Epoch 758/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7316 - accuracy: 0.7354 - val_loss: 1.4123 - val_accuracy: 0.4918\n",
            "Epoch 759/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7101 - accuracy: 0.7598 - val_loss: 1.4188 - val_accuracy: 0.4973\n",
            "Epoch 760/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7357 - accuracy: 0.7341 - val_loss: 1.4071 - val_accuracy: 0.5027\n",
            "Epoch 761/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.7029 - accuracy: 0.7639 - val_loss: 1.4646 - val_accuracy: 0.5082\n",
            "Epoch 762/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7386 - accuracy: 0.7476 - val_loss: 1.5339 - val_accuracy: 0.4863\n",
            "Epoch 763/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7353 - accuracy: 0.7286 - val_loss: 1.4303 - val_accuracy: 0.4918\n",
            "Epoch 764/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.7175 - accuracy: 0.7503 - val_loss: 1.4197 - val_accuracy: 0.4918\n",
            "Epoch 765/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7149 - accuracy: 0.7503 - val_loss: 1.4080 - val_accuracy: 0.4863\n",
            "Epoch 766/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7024 - accuracy: 0.7680 - val_loss: 1.4200 - val_accuracy: 0.5191\n",
            "Epoch 767/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7198 - accuracy: 0.7476 - val_loss: 1.4412 - val_accuracy: 0.4863\n",
            "Epoch 768/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7365 - accuracy: 0.7354 - val_loss: 1.4421 - val_accuracy: 0.4809\n",
            "Epoch 769/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7095 - accuracy: 0.7531 - val_loss: 1.4256 - val_accuracy: 0.4863\n",
            "Epoch 770/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7208 - accuracy: 0.7476 - val_loss: 1.4370 - val_accuracy: 0.4863\n",
            "Epoch 771/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7141 - accuracy: 0.7544 - val_loss: 1.4166 - val_accuracy: 0.5027\n",
            "Epoch 772/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6892 - accuracy: 0.7626 - val_loss: 1.4438 - val_accuracy: 0.4863\n",
            "Epoch 773/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7151 - accuracy: 0.7544 - val_loss: 1.4340 - val_accuracy: 0.4754\n",
            "Epoch 774/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7282 - accuracy: 0.7463 - val_loss: 1.4122 - val_accuracy: 0.5027\n",
            "Epoch 775/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7268 - accuracy: 0.7408 - val_loss: 1.4589 - val_accuracy: 0.4973\n",
            "Epoch 776/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7089 - accuracy: 0.7490 - val_loss: 1.4329 - val_accuracy: 0.4863\n",
            "Epoch 777/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.7013 - accuracy: 0.7720 - val_loss: 1.4337 - val_accuracy: 0.5137\n",
            "Epoch 778/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7057 - accuracy: 0.7354 - val_loss: 1.4458 - val_accuracy: 0.4754\n",
            "Epoch 779/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7163 - accuracy: 0.7476 - val_loss: 1.4575 - val_accuracy: 0.4918\n",
            "Epoch 780/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6671 - accuracy: 0.7707 - val_loss: 1.4419 - val_accuracy: 0.5137\n",
            "Epoch 781/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7306 - accuracy: 0.7205 - val_loss: 1.4129 - val_accuracy: 0.4918\n",
            "Epoch 782/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7183 - accuracy: 0.7490 - val_loss: 1.4100 - val_accuracy: 0.4973\n",
            "Epoch 783/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7010 - accuracy: 0.7531 - val_loss: 1.4115 - val_accuracy: 0.4863\n",
            "Epoch 784/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7137 - accuracy: 0.7449 - val_loss: 1.4223 - val_accuracy: 0.5027\n",
            "Epoch 785/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7055 - accuracy: 0.7666 - val_loss: 1.4325 - val_accuracy: 0.4863\n",
            "Epoch 786/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6732 - accuracy: 0.7788 - val_loss: 1.4343 - val_accuracy: 0.5082\n",
            "Epoch 787/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6810 - accuracy: 0.7680 - val_loss: 1.4237 - val_accuracy: 0.5082\n",
            "Epoch 788/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6904 - accuracy: 0.7558 - val_loss: 1.4293 - val_accuracy: 0.4809\n",
            "Epoch 789/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6980 - accuracy: 0.7680 - val_loss: 1.4285 - val_accuracy: 0.4809\n",
            "Epoch 790/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7031 - accuracy: 0.7558 - val_loss: 1.4324 - val_accuracy: 0.5082\n",
            "Epoch 791/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6849 - accuracy: 0.7558 - val_loss: 1.4194 - val_accuracy: 0.5082\n",
            "Epoch 792/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6910 - accuracy: 0.7639 - val_loss: 1.4193 - val_accuracy: 0.4863\n",
            "Epoch 793/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6942 - accuracy: 0.7612 - val_loss: 1.4277 - val_accuracy: 0.4918\n",
            "Epoch 794/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6993 - accuracy: 0.7476 - val_loss: 1.4449 - val_accuracy: 0.5082\n",
            "Epoch 795/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6896 - accuracy: 0.7626 - val_loss: 1.4328 - val_accuracy: 0.5027\n",
            "Epoch 796/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7033 - accuracy: 0.7544 - val_loss: 1.4196 - val_accuracy: 0.4973\n",
            "Epoch 797/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6787 - accuracy: 0.7693 - val_loss: 1.4223 - val_accuracy: 0.5082\n",
            "Epoch 798/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6952 - accuracy: 0.7490 - val_loss: 1.4415 - val_accuracy: 0.4918\n",
            "Epoch 799/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6841 - accuracy: 0.7612 - val_loss: 1.4624 - val_accuracy: 0.4863\n",
            "Epoch 800/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6789 - accuracy: 0.7707 - val_loss: 1.4252 - val_accuracy: 0.4863\n",
            "Epoch 801/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6587 - accuracy: 0.7775 - val_loss: 1.4725 - val_accuracy: 0.4809\n",
            "Epoch 802/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.6769 - accuracy: 0.7748 - val_loss: 1.4486 - val_accuracy: 0.4918\n",
            "Epoch 803/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6750 - accuracy: 0.7734 - val_loss: 1.4198 - val_accuracy: 0.4918\n",
            "Epoch 804/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6685 - accuracy: 0.7693 - val_loss: 1.4289 - val_accuracy: 0.4973\n",
            "Epoch 805/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6813 - accuracy: 0.7653 - val_loss: 1.4291 - val_accuracy: 0.4809\n",
            "Epoch 806/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6667 - accuracy: 0.7707 - val_loss: 1.4364 - val_accuracy: 0.4973\n",
            "Epoch 807/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6707 - accuracy: 0.7693 - val_loss: 1.4436 - val_accuracy: 0.5191\n",
            "Epoch 808/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6783 - accuracy: 0.7544 - val_loss: 1.4697 - val_accuracy: 0.4863\n",
            "Epoch 809/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6674 - accuracy: 0.7598 - val_loss: 1.4837 - val_accuracy: 0.5137\n",
            "Epoch 810/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6702 - accuracy: 0.7748 - val_loss: 1.4723 - val_accuracy: 0.4754\n",
            "Epoch 811/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6876 - accuracy: 0.7531 - val_loss: 1.4203 - val_accuracy: 0.4863\n",
            "Epoch 812/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6861 - accuracy: 0.7558 - val_loss: 1.4351 - val_accuracy: 0.4809\n",
            "Epoch 813/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6701 - accuracy: 0.7653 - val_loss: 1.4584 - val_accuracy: 0.4918\n",
            "Epoch 814/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6748 - accuracy: 0.7666 - val_loss: 1.4638 - val_accuracy: 0.4973\n",
            "Epoch 815/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6493 - accuracy: 0.7734 - val_loss: 1.4542 - val_accuracy: 0.4754\n",
            "Epoch 816/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6745 - accuracy: 0.7666 - val_loss: 1.4473 - val_accuracy: 0.4809\n",
            "Epoch 817/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6691 - accuracy: 0.7680 - val_loss: 1.4237 - val_accuracy: 0.4973\n",
            "Epoch 818/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6941 - accuracy: 0.7544 - val_loss: 1.4625 - val_accuracy: 0.5027\n",
            "Epoch 819/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6900 - accuracy: 0.7680 - val_loss: 1.4546 - val_accuracy: 0.5027\n",
            "Epoch 820/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6803 - accuracy: 0.7571 - val_loss: 1.4251 - val_accuracy: 0.5027\n",
            "Epoch 821/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6552 - accuracy: 0.7856 - val_loss: 1.4336 - val_accuracy: 0.5082\n",
            "Epoch 822/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6458 - accuracy: 0.7815 - val_loss: 1.4394 - val_accuracy: 0.5191\n",
            "Epoch 823/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6618 - accuracy: 0.7639 - val_loss: 1.4247 - val_accuracy: 0.5191\n",
            "Epoch 824/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6558 - accuracy: 0.7680 - val_loss: 1.5282 - val_accuracy: 0.4754\n",
            "Epoch 825/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6474 - accuracy: 0.7720 - val_loss: 1.4565 - val_accuracy: 0.5246\n",
            "Epoch 826/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6634 - accuracy: 0.7531 - val_loss: 1.4444 - val_accuracy: 0.5137\n",
            "Epoch 827/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6773 - accuracy: 0.7490 - val_loss: 1.4791 - val_accuracy: 0.4809\n",
            "Epoch 828/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6556 - accuracy: 0.7775 - val_loss: 1.4463 - val_accuracy: 0.5137\n",
            "Epoch 829/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6524 - accuracy: 0.7788 - val_loss: 1.4281 - val_accuracy: 0.5082\n",
            "Epoch 830/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6581 - accuracy: 0.7748 - val_loss: 1.4791 - val_accuracy: 0.4863\n",
            "Epoch 831/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6367 - accuracy: 0.7856 - val_loss: 1.4547 - val_accuracy: 0.4809\n",
            "Epoch 832/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6429 - accuracy: 0.7829 - val_loss: 1.4472 - val_accuracy: 0.4973\n",
            "Epoch 833/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6579 - accuracy: 0.7666 - val_loss: 1.4910 - val_accuracy: 0.4699\n",
            "Epoch 834/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6742 - accuracy: 0.7436 - val_loss: 1.4399 - val_accuracy: 0.5246\n",
            "Epoch 835/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6337 - accuracy: 0.7870 - val_loss: 1.4684 - val_accuracy: 0.4973\n",
            "Epoch 836/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6309 - accuracy: 0.7951 - val_loss: 1.4909 - val_accuracy: 0.4918\n",
            "Epoch 837/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6479 - accuracy: 0.7910 - val_loss: 1.4744 - val_accuracy: 0.4918\n",
            "Epoch 838/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6398 - accuracy: 0.7626 - val_loss: 1.4865 - val_accuracy: 0.4699\n",
            "Epoch 839/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6495 - accuracy: 0.7707 - val_loss: 1.4383 - val_accuracy: 0.5027\n",
            "Epoch 840/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6482 - accuracy: 0.7843 - val_loss: 1.4363 - val_accuracy: 0.4918\n",
            "Epoch 841/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6428 - accuracy: 0.7761 - val_loss: 1.4580 - val_accuracy: 0.4809\n",
            "Epoch 842/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6693 - accuracy: 0.7707 - val_loss: 1.4657 - val_accuracy: 0.4754\n",
            "Epoch 843/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6456 - accuracy: 0.7910 - val_loss: 1.4437 - val_accuracy: 0.5027\n",
            "Epoch 844/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6195 - accuracy: 0.7856 - val_loss: 1.5228 - val_accuracy: 0.4645\n",
            "Epoch 845/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6225 - accuracy: 0.7748 - val_loss: 1.4378 - val_accuracy: 0.5082\n",
            "Epoch 846/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6290 - accuracy: 0.7802 - val_loss: 1.4450 - val_accuracy: 0.5082\n",
            "Epoch 847/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6464 - accuracy: 0.7761 - val_loss: 1.4656 - val_accuracy: 0.5082\n",
            "Epoch 848/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6350 - accuracy: 0.7734 - val_loss: 1.4877 - val_accuracy: 0.4645\n",
            "Epoch 849/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6195 - accuracy: 0.8019 - val_loss: 1.4585 - val_accuracy: 0.4973\n",
            "Epoch 850/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6182 - accuracy: 0.8019 - val_loss: 1.4290 - val_accuracy: 0.5027\n",
            "Epoch 851/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6310 - accuracy: 0.7897 - val_loss: 1.4467 - val_accuracy: 0.4918\n",
            "Epoch 852/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6253 - accuracy: 0.7951 - val_loss: 1.4449 - val_accuracy: 0.4863\n",
            "Epoch 853/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6360 - accuracy: 0.7870 - val_loss: 1.4573 - val_accuracy: 0.5027\n",
            "Epoch 854/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6386 - accuracy: 0.7870 - val_loss: 1.4507 - val_accuracy: 0.5082\n",
            "Epoch 855/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6176 - accuracy: 0.7910 - val_loss: 1.4392 - val_accuracy: 0.4973\n",
            "Epoch 856/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6150 - accuracy: 0.7897 - val_loss: 1.4863 - val_accuracy: 0.5027\n",
            "Epoch 857/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6382 - accuracy: 0.7748 - val_loss: 1.4538 - val_accuracy: 0.5082\n",
            "Epoch 858/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6460 - accuracy: 0.7693 - val_loss: 1.4367 - val_accuracy: 0.5191\n",
            "Epoch 859/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6309 - accuracy: 0.7815 - val_loss: 1.4379 - val_accuracy: 0.4918\n",
            "Epoch 860/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6219 - accuracy: 0.7843 - val_loss: 1.4695 - val_accuracy: 0.4754\n",
            "Epoch 861/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6099 - accuracy: 0.7965 - val_loss: 1.4534 - val_accuracy: 0.4973\n",
            "Epoch 862/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6291 - accuracy: 0.7924 - val_loss: 1.4426 - val_accuracy: 0.4973\n",
            "Epoch 863/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6182 - accuracy: 0.7883 - val_loss: 1.4480 - val_accuracy: 0.4973\n",
            "Epoch 864/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6046 - accuracy: 0.7938 - val_loss: 1.4770 - val_accuracy: 0.4699\n",
            "Epoch 865/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6272 - accuracy: 0.7788 - val_loss: 1.4357 - val_accuracy: 0.4918\n",
            "Epoch 866/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6219 - accuracy: 0.7788 - val_loss: 1.4411 - val_accuracy: 0.4918\n",
            "Epoch 867/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6039 - accuracy: 0.7938 - val_loss: 1.4563 - val_accuracy: 0.5082\n",
            "Epoch 868/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6451 - accuracy: 0.7734 - val_loss: 1.4355 - val_accuracy: 0.5027\n",
            "Epoch 869/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6111 - accuracy: 0.8019 - val_loss: 1.4930 - val_accuracy: 0.4809\n",
            "Epoch 870/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6235 - accuracy: 0.7748 - val_loss: 1.4383 - val_accuracy: 0.4863\n",
            "Epoch 871/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6132 - accuracy: 0.7843 - val_loss: 1.4557 - val_accuracy: 0.4863\n",
            "Epoch 872/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5989 - accuracy: 0.8073 - val_loss: 1.4681 - val_accuracy: 0.4809\n",
            "Epoch 873/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6049 - accuracy: 0.7924 - val_loss: 1.4513 - val_accuracy: 0.5246\n",
            "Epoch 874/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6222 - accuracy: 0.7856 - val_loss: 1.4682 - val_accuracy: 0.4918\n",
            "Epoch 875/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5980 - accuracy: 0.8073 - val_loss: 1.4652 - val_accuracy: 0.5246\n",
            "Epoch 876/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5975 - accuracy: 0.8046 - val_loss: 1.4485 - val_accuracy: 0.5137\n",
            "Epoch 877/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6171 - accuracy: 0.7897 - val_loss: 1.4583 - val_accuracy: 0.4863\n",
            "Epoch 878/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6032 - accuracy: 0.8019 - val_loss: 1.4533 - val_accuracy: 0.5027\n",
            "Epoch 879/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.6150 - accuracy: 0.7870 - val_loss: 1.4366 - val_accuracy: 0.5301\n",
            "Epoch 880/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.6167 - accuracy: 0.7761 - val_loss: 1.4327 - val_accuracy: 0.4973\n",
            "Epoch 881/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.5974 - accuracy: 0.7870 - val_loss: 1.4314 - val_accuracy: 0.5082\n",
            "Epoch 882/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.5919 - accuracy: 0.7938 - val_loss: 1.4426 - val_accuracy: 0.5027\n",
            "Epoch 883/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6034 - accuracy: 0.7829 - val_loss: 1.4789 - val_accuracy: 0.4809\n",
            "Epoch 884/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5908 - accuracy: 0.8100 - val_loss: 1.4920 - val_accuracy: 0.5082\n",
            "Epoch 885/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5933 - accuracy: 0.7910 - val_loss: 1.4511 - val_accuracy: 0.5191\n",
            "Epoch 886/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5884 - accuracy: 0.8019 - val_loss: 1.4862 - val_accuracy: 0.4973\n",
            "Epoch 887/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5880 - accuracy: 0.8128 - val_loss: 1.4414 - val_accuracy: 0.5027\n",
            "Epoch 888/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5929 - accuracy: 0.8019 - val_loss: 1.4571 - val_accuracy: 0.4973\n",
            "Epoch 889/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5971 - accuracy: 0.7992 - val_loss: 1.4550 - val_accuracy: 0.5027\n",
            "Epoch 890/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5899 - accuracy: 0.8073 - val_loss: 1.4819 - val_accuracy: 0.4973\n",
            "Epoch 891/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6177 - accuracy: 0.7951 - val_loss: 1.4352 - val_accuracy: 0.5137\n",
            "Epoch 892/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6225 - accuracy: 0.7843 - val_loss: 1.4482 - val_accuracy: 0.5137\n",
            "Epoch 893/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5882 - accuracy: 0.8033 - val_loss: 1.4534 - val_accuracy: 0.5191\n",
            "Epoch 894/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5871 - accuracy: 0.7978 - val_loss: 1.4716 - val_accuracy: 0.5137\n",
            "Epoch 895/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5877 - accuracy: 0.8005 - val_loss: 1.4665 - val_accuracy: 0.5082\n",
            "Epoch 896/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5879 - accuracy: 0.8019 - val_loss: 1.4560 - val_accuracy: 0.5137\n",
            "Epoch 897/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5816 - accuracy: 0.7978 - val_loss: 1.4415 - val_accuracy: 0.5027\n",
            "Epoch 898/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.5645 - accuracy: 0.8304 - val_loss: 1.4780 - val_accuracy: 0.4973\n",
            "Epoch 899/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5835 - accuracy: 0.7924 - val_loss: 1.5048 - val_accuracy: 0.5191\n",
            "Epoch 900/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6039 - accuracy: 0.7992 - val_loss: 1.4994 - val_accuracy: 0.4754\n",
            "Epoch 901/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5854 - accuracy: 0.8073 - val_loss: 1.4542 - val_accuracy: 0.5082\n",
            "Epoch 902/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5707 - accuracy: 0.8046 - val_loss: 1.4436 - val_accuracy: 0.5301\n",
            "Epoch 903/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5611 - accuracy: 0.8182 - val_loss: 1.6543 - val_accuracy: 0.4699\n",
            "Epoch 904/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.5957 - accuracy: 0.8073 - val_loss: 1.4483 - val_accuracy: 0.5137\n",
            "Epoch 905/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.5801 - accuracy: 0.7978 - val_loss: 1.4545 - val_accuracy: 0.4973\n",
            "Epoch 906/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.5617 - accuracy: 0.8128 - val_loss: 1.4623 - val_accuracy: 0.4918\n",
            "Epoch 907/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6012 - accuracy: 0.7951 - val_loss: 1.4643 - val_accuracy: 0.5137\n",
            "Epoch 908/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5830 - accuracy: 0.8005 - val_loss: 1.5062 - val_accuracy: 0.4918\n",
            "Epoch 909/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5664 - accuracy: 0.8100 - val_loss: 1.4462 - val_accuracy: 0.4973\n",
            "Epoch 910/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5579 - accuracy: 0.8236 - val_loss: 1.4478 - val_accuracy: 0.5191\n",
            "Epoch 911/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5496 - accuracy: 0.8209 - val_loss: 1.4650 - val_accuracy: 0.5137\n",
            "Epoch 912/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5796 - accuracy: 0.7978 - val_loss: 1.4871 - val_accuracy: 0.4863\n",
            "Epoch 913/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.6007 - accuracy: 0.7951 - val_loss: 1.4511 - val_accuracy: 0.5082\n",
            "Epoch 914/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5746 - accuracy: 0.8005 - val_loss: 1.4691 - val_accuracy: 0.5082\n",
            "Epoch 915/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5726 - accuracy: 0.7965 - val_loss: 1.4619 - val_accuracy: 0.4973\n",
            "Epoch 916/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5638 - accuracy: 0.8019 - val_loss: 1.5479 - val_accuracy: 0.4918\n",
            "Epoch 917/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5935 - accuracy: 0.7992 - val_loss: 1.4565 - val_accuracy: 0.5082\n",
            "Epoch 918/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5668 - accuracy: 0.8060 - val_loss: 1.4617 - val_accuracy: 0.5191\n",
            "Epoch 919/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5874 - accuracy: 0.8019 - val_loss: 1.4716 - val_accuracy: 0.4973\n",
            "Epoch 920/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5772 - accuracy: 0.7897 - val_loss: 1.4630 - val_accuracy: 0.5191\n",
            "Epoch 921/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5571 - accuracy: 0.8073 - val_loss: 1.5209 - val_accuracy: 0.5137\n",
            "Epoch 922/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.5506 - accuracy: 0.8141 - val_loss: 1.5113 - val_accuracy: 0.4863\n",
            "Epoch 923/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.5577 - accuracy: 0.8114 - val_loss: 1.4583 - val_accuracy: 0.5464\n",
            "Epoch 924/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5528 - accuracy: 0.8100 - val_loss: 1.4588 - val_accuracy: 0.5027\n",
            "Epoch 925/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.5476 - accuracy: 0.8168 - val_loss: 1.4994 - val_accuracy: 0.4973\n",
            "Epoch 926/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.5412 - accuracy: 0.8141 - val_loss: 1.4941 - val_accuracy: 0.4809\n",
            "Epoch 927/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5439 - accuracy: 0.8195 - val_loss: 1.4588 - val_accuracy: 0.5355\n",
            "Epoch 928/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5674 - accuracy: 0.8073 - val_loss: 1.4620 - val_accuracy: 0.5137\n",
            "Epoch 929/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5519 - accuracy: 0.8223 - val_loss: 1.5014 - val_accuracy: 0.4863\n",
            "Epoch 930/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5836 - accuracy: 0.8141 - val_loss: 1.4809 - val_accuracy: 0.5191\n",
            "Epoch 931/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5313 - accuracy: 0.8263 - val_loss: 1.4687 - val_accuracy: 0.4973\n",
            "Epoch 932/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.5430 - accuracy: 0.8141 - val_loss: 1.4867 - val_accuracy: 0.4973\n",
            "Epoch 933/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5684 - accuracy: 0.7951 - val_loss: 1.4609 - val_accuracy: 0.5355\n",
            "Epoch 934/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5442 - accuracy: 0.8182 - val_loss: 1.4619 - val_accuracy: 0.5027\n",
            "Epoch 935/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5344 - accuracy: 0.8372 - val_loss: 1.4662 - val_accuracy: 0.5137\n",
            "Epoch 936/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.5382 - accuracy: 0.8168 - val_loss: 1.4794 - val_accuracy: 0.4863\n",
            "Epoch 937/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5348 - accuracy: 0.8277 - val_loss: 1.5035 - val_accuracy: 0.5027\n",
            "Epoch 938/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5318 - accuracy: 0.8290 - val_loss: 1.4614 - val_accuracy: 0.5137\n",
            "Epoch 939/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.5342 - accuracy: 0.8209 - val_loss: 1.4663 - val_accuracy: 0.5301\n",
            "Epoch 940/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5587 - accuracy: 0.8141 - val_loss: 1.4849 - val_accuracy: 0.4863\n",
            "Epoch 941/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.5490 - accuracy: 0.8087 - val_loss: 1.4650 - val_accuracy: 0.5137\n",
            "Epoch 942/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5485 - accuracy: 0.8128 - val_loss: 1.4757 - val_accuracy: 0.4973\n",
            "Epoch 943/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5501 - accuracy: 0.8073 - val_loss: 1.4769 - val_accuracy: 0.5301\n",
            "Epoch 944/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5374 - accuracy: 0.8223 - val_loss: 1.4662 - val_accuracy: 0.5137\n",
            "Epoch 945/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5393 - accuracy: 0.8033 - val_loss: 1.4865 - val_accuracy: 0.5027\n",
            "Epoch 946/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5514 - accuracy: 0.8087 - val_loss: 1.4812 - val_accuracy: 0.5027\n",
            "Epoch 947/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.5342 - accuracy: 0.8304 - val_loss: 1.5073 - val_accuracy: 0.5082\n",
            "Epoch 948/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5418 - accuracy: 0.8250 - val_loss: 1.4692 - val_accuracy: 0.5246\n",
            "Epoch 949/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5474 - accuracy: 0.8033 - val_loss: 1.4670 - val_accuracy: 0.5246\n",
            "Epoch 950/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5487 - accuracy: 0.8073 - val_loss: 1.5007 - val_accuracy: 0.4918\n",
            "Epoch 951/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5434 - accuracy: 0.8168 - val_loss: 1.4707 - val_accuracy: 0.5027\n",
            "Epoch 952/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5355 - accuracy: 0.8209 - val_loss: 1.4833 - val_accuracy: 0.4918\n",
            "Epoch 953/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.5279 - accuracy: 0.8182 - val_loss: 1.4762 - val_accuracy: 0.5410\n",
            "Epoch 954/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5142 - accuracy: 0.8331 - val_loss: 1.4793 - val_accuracy: 0.5082\n",
            "Epoch 955/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.5365 - accuracy: 0.8182 - val_loss: 1.4949 - val_accuracy: 0.5137\n",
            "Epoch 956/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5441 - accuracy: 0.8290 - val_loss: 1.4933 - val_accuracy: 0.4918\n",
            "Epoch 957/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5079 - accuracy: 0.8453 - val_loss: 1.4858 - val_accuracy: 0.4918\n",
            "Epoch 958/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.5094 - accuracy: 0.8412 - val_loss: 1.4952 - val_accuracy: 0.4973\n",
            "Epoch 959/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5391 - accuracy: 0.8128 - val_loss: 1.4924 - val_accuracy: 0.4863\n",
            "Epoch 960/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5208 - accuracy: 0.8236 - val_loss: 1.5021 - val_accuracy: 0.4918\n",
            "Epoch 961/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.5304 - accuracy: 0.8100 - val_loss: 1.5111 - val_accuracy: 0.5027\n",
            "Epoch 962/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5260 - accuracy: 0.8331 - val_loss: 1.4772 - val_accuracy: 0.5082\n",
            "Epoch 963/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5283 - accuracy: 0.8318 - val_loss: 1.4891 - val_accuracy: 0.5027\n",
            "Epoch 964/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5062 - accuracy: 0.8507 - val_loss: 1.4864 - val_accuracy: 0.5027\n",
            "Epoch 965/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.5411 - accuracy: 0.8304 - val_loss: 1.4892 - val_accuracy: 0.5355\n",
            "Epoch 966/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.5347 - accuracy: 0.8141 - val_loss: 1.5201 - val_accuracy: 0.4863\n",
            "Epoch 967/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5141 - accuracy: 0.8331 - val_loss: 1.4837 - val_accuracy: 0.5082\n",
            "Epoch 968/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5176 - accuracy: 0.8304 - val_loss: 1.4918 - val_accuracy: 0.5301\n",
            "Epoch 969/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5167 - accuracy: 0.8331 - val_loss: 1.4865 - val_accuracy: 0.5246\n",
            "Epoch 970/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.5172 - accuracy: 0.8318 - val_loss: 1.5252 - val_accuracy: 0.4918\n",
            "Epoch 971/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.5177 - accuracy: 0.8345 - val_loss: 1.5039 - val_accuracy: 0.5027\n",
            "Epoch 972/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5112 - accuracy: 0.8168 - val_loss: 1.4809 - val_accuracy: 0.5137\n",
            "Epoch 973/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.5132 - accuracy: 0.8195 - val_loss: 1.5485 - val_accuracy: 0.5027\n",
            "Epoch 974/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.5348 - accuracy: 0.8277 - val_loss: 1.4889 - val_accuracy: 0.5137\n",
            "Epoch 975/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.5228 - accuracy: 0.8290 - val_loss: 1.4931 - val_accuracy: 0.5027\n",
            "Epoch 976/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5207 - accuracy: 0.8223 - val_loss: 1.4948 - val_accuracy: 0.5082\n",
            "Epoch 977/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.5072 - accuracy: 0.8304 - val_loss: 1.5394 - val_accuracy: 0.4918\n",
            "Epoch 978/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5171 - accuracy: 0.8250 - val_loss: 1.5193 - val_accuracy: 0.5082\n",
            "Epoch 979/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5298 - accuracy: 0.8277 - val_loss: 1.4879 - val_accuracy: 0.5027\n",
            "Epoch 980/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4948 - accuracy: 0.8277 - val_loss: 1.5230 - val_accuracy: 0.5082\n",
            "Epoch 981/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5083 - accuracy: 0.8250 - val_loss: 1.4997 - val_accuracy: 0.5082\n",
            "Epoch 982/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4966 - accuracy: 0.8331 - val_loss: 1.5327 - val_accuracy: 0.5137\n",
            "Epoch 983/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4887 - accuracy: 0.8453 - val_loss: 1.5037 - val_accuracy: 0.5027\n",
            "Epoch 984/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4984 - accuracy: 0.8440 - val_loss: 1.4854 - val_accuracy: 0.5246\n",
            "Epoch 985/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5047 - accuracy: 0.8331 - val_loss: 1.5122 - val_accuracy: 0.5027\n",
            "Epoch 986/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5010 - accuracy: 0.8385 - val_loss: 1.5471 - val_accuracy: 0.5137\n",
            "Epoch 987/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4842 - accuracy: 0.8467 - val_loss: 1.5096 - val_accuracy: 0.4973\n",
            "Epoch 988/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5152 - accuracy: 0.8277 - val_loss: 1.5047 - val_accuracy: 0.5137\n",
            "Epoch 989/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4665 - accuracy: 0.8562 - val_loss: 1.5349 - val_accuracy: 0.4918\n",
            "Epoch 990/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4946 - accuracy: 0.8331 - val_loss: 1.5775 - val_accuracy: 0.5082\n",
            "Epoch 991/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.5176 - accuracy: 0.8372 - val_loss: 1.5002 - val_accuracy: 0.5082\n",
            "Epoch 992/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4837 - accuracy: 0.8467 - val_loss: 1.5028 - val_accuracy: 0.5464\n",
            "Epoch 993/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4836 - accuracy: 0.8453 - val_loss: 1.5331 - val_accuracy: 0.5027\n",
            "Epoch 994/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4854 - accuracy: 0.8331 - val_loss: 1.5302 - val_accuracy: 0.4973\n",
            "Epoch 995/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.4576 - accuracy: 0.8684 - val_loss: 1.4968 - val_accuracy: 0.5246\n",
            "Epoch 996/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.5028 - accuracy: 0.8345 - val_loss: 1.5564 - val_accuracy: 0.4973\n",
            "Epoch 997/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4933 - accuracy: 0.8250 - val_loss: 1.5245 - val_accuracy: 0.4918\n",
            "Epoch 998/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4723 - accuracy: 0.8562 - val_loss: 1.5158 - val_accuracy: 0.5027\n",
            "Epoch 999/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5029 - accuracy: 0.8358 - val_loss: 1.4969 - val_accuracy: 0.5027\n",
            "Epoch 1000/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4833 - accuracy: 0.8263 - val_loss: 1.5367 - val_accuracy: 0.4973\n",
            "Epoch 1001/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4580 - accuracy: 0.8494 - val_loss: 1.5023 - val_accuracy: 0.5082\n",
            "Epoch 1002/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4923 - accuracy: 0.8318 - val_loss: 1.5237 - val_accuracy: 0.5027\n",
            "Epoch 1003/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4861 - accuracy: 0.8345 - val_loss: 1.5173 - val_accuracy: 0.5027\n",
            "Epoch 1004/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4685 - accuracy: 0.8602 - val_loss: 1.5861 - val_accuracy: 0.4973\n",
            "Epoch 1005/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.4524 - accuracy: 0.8589 - val_loss: 1.5046 - val_accuracy: 0.5137\n",
            "Epoch 1006/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4733 - accuracy: 0.8440 - val_loss: 1.5330 - val_accuracy: 0.5082\n",
            "Epoch 1007/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4618 - accuracy: 0.8494 - val_loss: 1.5095 - val_accuracy: 0.5191\n",
            "Epoch 1008/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4808 - accuracy: 0.8426 - val_loss: 1.5733 - val_accuracy: 0.5082\n",
            "Epoch 1009/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4726 - accuracy: 0.8602 - val_loss: 1.5078 - val_accuracy: 0.5027\n",
            "Epoch 1010/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4738 - accuracy: 0.8507 - val_loss: 1.5239 - val_accuracy: 0.4973\n",
            "Epoch 1011/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.5069 - accuracy: 0.8236 - val_loss: 1.5065 - val_accuracy: 0.5082\n",
            "Epoch 1012/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4789 - accuracy: 0.8426 - val_loss: 1.5113 - val_accuracy: 0.5137\n",
            "Epoch 1013/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.4874 - accuracy: 0.8345 - val_loss: 1.5054 - val_accuracy: 0.5137\n",
            "Epoch 1014/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4727 - accuracy: 0.8426 - val_loss: 1.5062 - val_accuracy: 0.5246\n",
            "Epoch 1015/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4653 - accuracy: 0.8372 - val_loss: 1.4968 - val_accuracy: 0.5464\n",
            "Epoch 1016/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4556 - accuracy: 0.8643 - val_loss: 1.5113 - val_accuracy: 0.5246\n",
            "Epoch 1017/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4516 - accuracy: 0.8630 - val_loss: 1.5205 - val_accuracy: 0.5191\n",
            "Epoch 1018/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.4618 - accuracy: 0.8440 - val_loss: 1.5084 - val_accuracy: 0.5027\n",
            "Epoch 1019/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4658 - accuracy: 0.8467 - val_loss: 1.5426 - val_accuracy: 0.5027\n",
            "Epoch 1020/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4850 - accuracy: 0.8399 - val_loss: 1.5617 - val_accuracy: 0.5027\n",
            "Epoch 1021/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4854 - accuracy: 0.8372 - val_loss: 1.5398 - val_accuracy: 0.4973\n",
            "Epoch 1022/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4549 - accuracy: 0.8521 - val_loss: 1.5123 - val_accuracy: 0.5191\n",
            "Epoch 1023/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4945 - accuracy: 0.8521 - val_loss: 1.4993 - val_accuracy: 0.5137\n",
            "Epoch 1024/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4561 - accuracy: 0.8548 - val_loss: 1.5184 - val_accuracy: 0.5301\n",
            "Epoch 1025/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4619 - accuracy: 0.8616 - val_loss: 1.5210 - val_accuracy: 0.5137\n",
            "Epoch 1026/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4480 - accuracy: 0.8426 - val_loss: 1.5253 - val_accuracy: 0.4809\n",
            "Epoch 1027/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4808 - accuracy: 0.8467 - val_loss: 1.5051 - val_accuracy: 0.5301\n",
            "Epoch 1028/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.4602 - accuracy: 0.8494 - val_loss: 1.5005 - val_accuracy: 0.5355\n",
            "Epoch 1029/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4534 - accuracy: 0.8507 - val_loss: 1.5221 - val_accuracy: 0.5027\n",
            "Epoch 1030/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4700 - accuracy: 0.8385 - val_loss: 1.5340 - val_accuracy: 0.5246\n",
            "Epoch 1031/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4545 - accuracy: 0.8535 - val_loss: 1.5222 - val_accuracy: 0.5410\n",
            "Epoch 1032/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4832 - accuracy: 0.8412 - val_loss: 1.5163 - val_accuracy: 0.5082\n",
            "Epoch 1033/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.4688 - accuracy: 0.8426 - val_loss: 1.5250 - val_accuracy: 0.4973\n",
            "Epoch 1034/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4569 - accuracy: 0.8412 - val_loss: 1.5105 - val_accuracy: 0.5246\n",
            "Epoch 1035/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4499 - accuracy: 0.8684 - val_loss: 1.5055 - val_accuracy: 0.5519\n",
            "Epoch 1036/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4657 - accuracy: 0.8358 - val_loss: 1.5081 - val_accuracy: 0.5137\n",
            "Epoch 1037/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4503 - accuracy: 0.8535 - val_loss: 1.5486 - val_accuracy: 0.5082\n",
            "Epoch 1038/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4524 - accuracy: 0.8507 - val_loss: 1.5623 - val_accuracy: 0.4699\n",
            "Epoch 1039/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4610 - accuracy: 0.8575 - val_loss: 1.5202 - val_accuracy: 0.5191\n",
            "Epoch 1040/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.4476 - accuracy: 0.8494 - val_loss: 1.5195 - val_accuracy: 0.5027\n",
            "Epoch 1041/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4363 - accuracy: 0.8616 - val_loss: 1.5700 - val_accuracy: 0.4754\n",
            "Epoch 1042/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4542 - accuracy: 0.8602 - val_loss: 1.5333 - val_accuracy: 0.5410\n",
            "Epoch 1043/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4627 - accuracy: 0.8521 - val_loss: 1.5413 - val_accuracy: 0.5410\n",
            "Epoch 1044/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4377 - accuracy: 0.8521 - val_loss: 1.5104 - val_accuracy: 0.5191\n",
            "Epoch 1045/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4415 - accuracy: 0.8548 - val_loss: 1.5381 - val_accuracy: 0.5137\n",
            "Epoch 1046/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4361 - accuracy: 0.8575 - val_loss: 1.5299 - val_accuracy: 0.5301\n",
            "Epoch 1047/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4197 - accuracy: 0.8752 - val_loss: 1.5674 - val_accuracy: 0.5191\n",
            "Epoch 1048/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4414 - accuracy: 0.8643 - val_loss: 1.5433 - val_accuracy: 0.4918\n",
            "Epoch 1049/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4308 - accuracy: 0.8643 - val_loss: 1.5156 - val_accuracy: 0.5137\n",
            "Epoch 1050/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4186 - accuracy: 0.8765 - val_loss: 1.5248 - val_accuracy: 0.5464\n",
            "Epoch 1051/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4421 - accuracy: 0.8535 - val_loss: 1.5344 - val_accuracy: 0.5137\n",
            "Epoch 1052/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4466 - accuracy: 0.8440 - val_loss: 1.5195 - val_accuracy: 0.5301\n",
            "Epoch 1053/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4292 - accuracy: 0.8765 - val_loss: 1.5930 - val_accuracy: 0.5027\n",
            "Epoch 1054/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4404 - accuracy: 0.8467 - val_loss: 1.5273 - val_accuracy: 0.5191\n",
            "Epoch 1055/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4338 - accuracy: 0.8521 - val_loss: 1.5310 - val_accuracy: 0.5519\n",
            "Epoch 1056/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4478 - accuracy: 0.8535 - val_loss: 1.6033 - val_accuracy: 0.5137\n",
            "Epoch 1057/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4374 - accuracy: 0.8643 - val_loss: 1.5309 - val_accuracy: 0.5027\n",
            "Epoch 1058/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4283 - accuracy: 0.8670 - val_loss: 1.5172 - val_accuracy: 0.5246\n",
            "Epoch 1059/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4230 - accuracy: 0.8562 - val_loss: 1.5551 - val_accuracy: 0.5082\n",
            "Epoch 1060/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.4461 - accuracy: 0.8548 - val_loss: 1.5422 - val_accuracy: 0.5574\n",
            "Epoch 1061/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4251 - accuracy: 0.8494 - val_loss: 1.5746 - val_accuracy: 0.5137\n",
            "Epoch 1062/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4260 - accuracy: 0.8711 - val_loss: 1.5359 - val_accuracy: 0.5301\n",
            "Epoch 1063/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4251 - accuracy: 0.8643 - val_loss: 1.5608 - val_accuracy: 0.5137\n",
            "Epoch 1064/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4107 - accuracy: 0.8779 - val_loss: 1.5522 - val_accuracy: 0.4863\n",
            "Epoch 1065/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.4351 - accuracy: 0.8602 - val_loss: 1.5755 - val_accuracy: 0.5082\n",
            "Epoch 1066/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4340 - accuracy: 0.8507 - val_loss: 1.5864 - val_accuracy: 0.5137\n",
            "Epoch 1067/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4163 - accuracy: 0.8779 - val_loss: 1.5313 - val_accuracy: 0.4973\n",
            "Epoch 1068/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4403 - accuracy: 0.8670 - val_loss: 1.5393 - val_accuracy: 0.5410\n",
            "Epoch 1069/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3946 - accuracy: 0.8901 - val_loss: 1.5614 - val_accuracy: 0.5137\n",
            "Epoch 1070/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4279 - accuracy: 0.8589 - val_loss: 1.5393 - val_accuracy: 0.5246\n",
            "Epoch 1071/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4134 - accuracy: 0.8752 - val_loss: 1.6084 - val_accuracy: 0.4973\n",
            "Epoch 1072/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4052 - accuracy: 0.8806 - val_loss: 1.5457 - val_accuracy: 0.5027\n",
            "Epoch 1073/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.4124 - accuracy: 0.8602 - val_loss: 1.5663 - val_accuracy: 0.5137\n",
            "Epoch 1074/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4336 - accuracy: 0.8697 - val_loss: 1.5442 - val_accuracy: 0.5027\n",
            "Epoch 1075/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4428 - accuracy: 0.8535 - val_loss: 1.5383 - val_accuracy: 0.5355\n",
            "Epoch 1076/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4021 - accuracy: 0.8616 - val_loss: 1.5548 - val_accuracy: 0.5301\n",
            "Epoch 1077/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4179 - accuracy: 0.8738 - val_loss: 1.5777 - val_accuracy: 0.5246\n",
            "Epoch 1078/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4180 - accuracy: 0.8602 - val_loss: 1.5406 - val_accuracy: 0.5191\n",
            "Epoch 1079/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4199 - accuracy: 0.8792 - val_loss: 1.5823 - val_accuracy: 0.5027\n",
            "Epoch 1080/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4086 - accuracy: 0.8711 - val_loss: 1.5820 - val_accuracy: 0.4973\n",
            "Epoch 1081/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.4132 - accuracy: 0.8697 - val_loss: 1.5989 - val_accuracy: 0.5137\n",
            "Epoch 1082/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3985 - accuracy: 0.8738 - val_loss: 1.5907 - val_accuracy: 0.5137\n",
            "Epoch 1083/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4199 - accuracy: 0.8860 - val_loss: 1.5588 - val_accuracy: 0.5137\n",
            "Epoch 1084/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.3972 - accuracy: 0.8725 - val_loss: 1.5859 - val_accuracy: 0.4918\n",
            "Epoch 1085/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4082 - accuracy: 0.8670 - val_loss: 1.6047 - val_accuracy: 0.5191\n",
            "Epoch 1086/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.4270 - accuracy: 0.8657 - val_loss: 1.5780 - val_accuracy: 0.5027\n",
            "Epoch 1087/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.4007 - accuracy: 0.8657 - val_loss: 1.5963 - val_accuracy: 0.4973\n",
            "Epoch 1088/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3924 - accuracy: 0.8806 - val_loss: 1.5581 - val_accuracy: 0.5246\n",
            "Epoch 1089/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3939 - accuracy: 0.8833 - val_loss: 1.6067 - val_accuracy: 0.5027\n",
            "Epoch 1090/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.4013 - accuracy: 0.8887 - val_loss: 1.5938 - val_accuracy: 0.4918\n",
            "Epoch 1091/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4105 - accuracy: 0.8643 - val_loss: 1.5634 - val_accuracy: 0.5137\n",
            "Epoch 1092/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.3817 - accuracy: 0.8806 - val_loss: 1.5383 - val_accuracy: 0.5137\n",
            "Epoch 1093/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3977 - accuracy: 0.8792 - val_loss: 1.5653 - val_accuracy: 0.5191\n",
            "Epoch 1094/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3872 - accuracy: 0.8874 - val_loss: 1.6211 - val_accuracy: 0.4918\n",
            "Epoch 1095/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4062 - accuracy: 0.8752 - val_loss: 1.5627 - val_accuracy: 0.4973\n",
            "Epoch 1096/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3891 - accuracy: 0.8779 - val_loss: 1.5568 - val_accuracy: 0.5137\n",
            "Epoch 1097/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3886 - accuracy: 0.8792 - val_loss: 1.5709 - val_accuracy: 0.5027\n",
            "Epoch 1098/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.4021 - accuracy: 0.8792 - val_loss: 1.5651 - val_accuracy: 0.5027\n",
            "Epoch 1099/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4027 - accuracy: 0.8657 - val_loss: 1.5674 - val_accuracy: 0.5137\n",
            "Epoch 1100/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3942 - accuracy: 0.8725 - val_loss: 1.5729 - val_accuracy: 0.5191\n",
            "Epoch 1101/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3839 - accuracy: 0.8860 - val_loss: 1.5637 - val_accuracy: 0.5137\n",
            "Epoch 1102/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3801 - accuracy: 0.8833 - val_loss: 1.5617 - val_accuracy: 0.5082\n",
            "Epoch 1103/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3822 - accuracy: 0.8915 - val_loss: 1.5563 - val_accuracy: 0.5191\n",
            "Epoch 1104/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3775 - accuracy: 0.8982 - val_loss: 1.5827 - val_accuracy: 0.5082\n",
            "Epoch 1105/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4010 - accuracy: 0.8670 - val_loss: 1.5599 - val_accuracy: 0.5082\n",
            "Epoch 1106/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.4103 - accuracy: 0.8725 - val_loss: 1.5756 - val_accuracy: 0.4973\n",
            "Epoch 1107/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3821 - accuracy: 0.8725 - val_loss: 1.5683 - val_accuracy: 0.5027\n",
            "Epoch 1108/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3770 - accuracy: 0.8887 - val_loss: 1.5914 - val_accuracy: 0.4973\n",
            "Epoch 1109/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3905 - accuracy: 0.8860 - val_loss: 1.6139 - val_accuracy: 0.5137\n",
            "Epoch 1110/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3803 - accuracy: 0.8657 - val_loss: 1.5716 - val_accuracy: 0.5191\n",
            "Epoch 1111/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3710 - accuracy: 0.8833 - val_loss: 1.6039 - val_accuracy: 0.4973\n",
            "Epoch 1112/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3759 - accuracy: 0.8792 - val_loss: 1.5974 - val_accuracy: 0.5191\n",
            "Epoch 1113/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3879 - accuracy: 0.8697 - val_loss: 1.5553 - val_accuracy: 0.5191\n",
            "Epoch 1114/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3659 - accuracy: 0.8806 - val_loss: 1.5642 - val_accuracy: 0.4973\n",
            "Epoch 1115/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3816 - accuracy: 0.8820 - val_loss: 1.6245 - val_accuracy: 0.5082\n",
            "Epoch 1116/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3868 - accuracy: 0.8860 - val_loss: 1.5727 - val_accuracy: 0.4918\n",
            "Epoch 1117/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3849 - accuracy: 0.8765 - val_loss: 1.6869 - val_accuracy: 0.5027\n",
            "Epoch 1118/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3893 - accuracy: 0.8860 - val_loss: 1.6263 - val_accuracy: 0.5027\n",
            "Epoch 1119/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.3653 - accuracy: 0.9037 - val_loss: 1.5663 - val_accuracy: 0.5137\n",
            "Epoch 1120/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3598 - accuracy: 0.8969 - val_loss: 1.6483 - val_accuracy: 0.5082\n",
            "Epoch 1121/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3674 - accuracy: 0.8887 - val_loss: 1.5809 - val_accuracy: 0.5246\n",
            "Epoch 1122/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.3661 - accuracy: 0.8928 - val_loss: 1.5768 - val_accuracy: 0.5082\n",
            "Epoch 1123/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3772 - accuracy: 0.8833 - val_loss: 1.6022 - val_accuracy: 0.5027\n",
            "Epoch 1124/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.3606 - accuracy: 0.8915 - val_loss: 1.6289 - val_accuracy: 0.5137\n",
            "Epoch 1125/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3727 - accuracy: 0.8738 - val_loss: 1.6367 - val_accuracy: 0.5191\n",
            "Epoch 1126/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3842 - accuracy: 0.8874 - val_loss: 1.5920 - val_accuracy: 0.4973\n",
            "Epoch 1127/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3552 - accuracy: 0.8996 - val_loss: 1.6302 - val_accuracy: 0.4973\n",
            "Epoch 1128/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3837 - accuracy: 0.8833 - val_loss: 1.6029 - val_accuracy: 0.5191\n",
            "Epoch 1129/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3647 - accuracy: 0.8792 - val_loss: 1.5825 - val_accuracy: 0.5301\n",
            "Epoch 1130/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3691 - accuracy: 0.8982 - val_loss: 1.6340 - val_accuracy: 0.4809\n",
            "Epoch 1131/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3664 - accuracy: 0.8915 - val_loss: 1.6108 - val_accuracy: 0.5082\n",
            "Epoch 1132/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3802 - accuracy: 0.8955 - val_loss: 1.5811 - val_accuracy: 0.4973\n",
            "Epoch 1133/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3844 - accuracy: 0.8752 - val_loss: 1.5975 - val_accuracy: 0.4973\n",
            "Epoch 1134/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.3690 - accuracy: 0.8792 - val_loss: 1.6038 - val_accuracy: 0.4918\n",
            "Epoch 1135/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.3569 - accuracy: 0.8942 - val_loss: 1.5800 - val_accuracy: 0.5027\n",
            "Epoch 1136/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3297 - accuracy: 0.8969 - val_loss: 1.6072 - val_accuracy: 0.5410\n",
            "Epoch 1137/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3599 - accuracy: 0.8915 - val_loss: 1.5865 - val_accuracy: 0.5246\n",
            "Epoch 1138/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3361 - accuracy: 0.8996 - val_loss: 1.6276 - val_accuracy: 0.4809\n",
            "Epoch 1139/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3647 - accuracy: 0.8860 - val_loss: 1.6448 - val_accuracy: 0.5082\n",
            "Epoch 1140/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3695 - accuracy: 0.8779 - val_loss: 1.5940 - val_accuracy: 0.5027\n",
            "Epoch 1141/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3497 - accuracy: 0.8887 - val_loss: 1.5930 - val_accuracy: 0.4973\n",
            "Epoch 1142/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3462 - accuracy: 0.8901 - val_loss: 1.6015 - val_accuracy: 0.5027\n",
            "Epoch 1143/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3512 - accuracy: 0.9023 - val_loss: 1.6226 - val_accuracy: 0.5191\n",
            "Epoch 1144/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3559 - accuracy: 0.8996 - val_loss: 1.5955 - val_accuracy: 0.5191\n",
            "Epoch 1145/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3530 - accuracy: 0.8996 - val_loss: 1.6315 - val_accuracy: 0.5082\n",
            "Epoch 1146/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3346 - accuracy: 0.9077 - val_loss: 1.6058 - val_accuracy: 0.5246\n",
            "Epoch 1147/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3447 - accuracy: 0.8996 - val_loss: 1.6113 - val_accuracy: 0.5137\n",
            "Epoch 1148/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3451 - accuracy: 0.8901 - val_loss: 1.7095 - val_accuracy: 0.5027\n",
            "Epoch 1149/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.3641 - accuracy: 0.8887 - val_loss: 1.6060 - val_accuracy: 0.5246\n",
            "Epoch 1150/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3612 - accuracy: 0.8820 - val_loss: 1.6190 - val_accuracy: 0.5191\n",
            "Epoch 1151/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3503 - accuracy: 0.8860 - val_loss: 1.6456 - val_accuracy: 0.4973\n",
            "Epoch 1152/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3695 - accuracy: 0.8765 - val_loss: 1.6070 - val_accuracy: 0.5082\n",
            "Epoch 1153/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3544 - accuracy: 0.8915 - val_loss: 1.6101 - val_accuracy: 0.5137\n",
            "Epoch 1154/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3510 - accuracy: 0.8901 - val_loss: 1.6625 - val_accuracy: 0.4973\n",
            "Epoch 1155/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3603 - accuracy: 0.8874 - val_loss: 1.6233 - val_accuracy: 0.5246\n",
            "Epoch 1156/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3308 - accuracy: 0.9023 - val_loss: 1.6355 - val_accuracy: 0.5191\n",
            "Epoch 1157/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3455 - accuracy: 0.8982 - val_loss: 1.6294 - val_accuracy: 0.5137\n",
            "Epoch 1158/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3409 - accuracy: 0.8806 - val_loss: 1.6498 - val_accuracy: 0.5027\n",
            "Epoch 1159/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3501 - accuracy: 0.8901 - val_loss: 1.6779 - val_accuracy: 0.5137\n",
            "Epoch 1160/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3488 - accuracy: 0.8792 - val_loss: 1.6364 - val_accuracy: 0.5191\n",
            "Epoch 1161/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3426 - accuracy: 0.9037 - val_loss: 1.6148 - val_accuracy: 0.5027\n",
            "Epoch 1162/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3509 - accuracy: 0.8887 - val_loss: 1.7345 - val_accuracy: 0.4973\n",
            "Epoch 1163/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3533 - accuracy: 0.8874 - val_loss: 1.6240 - val_accuracy: 0.4973\n",
            "Epoch 1164/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3271 - accuracy: 0.9064 - val_loss: 1.6527 - val_accuracy: 0.5246\n",
            "Epoch 1165/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3386 - accuracy: 0.9064 - val_loss: 1.6274 - val_accuracy: 0.5137\n",
            "Epoch 1166/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3497 - accuracy: 0.8901 - val_loss: 1.6686 - val_accuracy: 0.4973\n",
            "Epoch 1167/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3379 - accuracy: 0.9009 - val_loss: 1.6451 - val_accuracy: 0.5082\n",
            "Epoch 1168/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3484 - accuracy: 0.8928 - val_loss: 1.6303 - val_accuracy: 0.5082\n",
            "Epoch 1169/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3314 - accuracy: 0.8887 - val_loss: 1.6577 - val_accuracy: 0.4973\n",
            "Epoch 1170/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3209 - accuracy: 0.8996 - val_loss: 1.6458 - val_accuracy: 0.5027\n",
            "Epoch 1171/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3425 - accuracy: 0.8955 - val_loss: 1.6432 - val_accuracy: 0.5082\n",
            "Epoch 1172/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3469 - accuracy: 0.8901 - val_loss: 1.6535 - val_accuracy: 0.4973\n",
            "Epoch 1173/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3317 - accuracy: 0.9077 - val_loss: 1.6477 - val_accuracy: 0.5027\n",
            "Epoch 1174/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3082 - accuracy: 0.9050 - val_loss: 1.6350 - val_accuracy: 0.4973\n",
            "Epoch 1175/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3405 - accuracy: 0.8942 - val_loss: 1.6431 - val_accuracy: 0.5191\n",
            "Epoch 1176/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3346 - accuracy: 0.8847 - val_loss: 1.6110 - val_accuracy: 0.4918\n",
            "Epoch 1177/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3550 - accuracy: 0.8820 - val_loss: 1.6317 - val_accuracy: 0.5137\n",
            "Epoch 1178/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3428 - accuracy: 0.9037 - val_loss: 1.6775 - val_accuracy: 0.4918\n",
            "Epoch 1179/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3294 - accuracy: 0.8928 - val_loss: 1.6254 - val_accuracy: 0.5027\n",
            "Epoch 1180/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3486 - accuracy: 0.8860 - val_loss: 1.6250 - val_accuracy: 0.5082\n",
            "Epoch 1181/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3205 - accuracy: 0.8955 - val_loss: 1.6292 - val_accuracy: 0.5082\n",
            "Epoch 1182/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3366 - accuracy: 0.8915 - val_loss: 1.6381 - val_accuracy: 0.4863\n",
            "Epoch 1183/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.3109 - accuracy: 0.9064 - val_loss: 1.6332 - val_accuracy: 0.5137\n",
            "Epoch 1184/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3330 - accuracy: 0.8955 - val_loss: 1.6455 - val_accuracy: 0.5246\n",
            "Epoch 1185/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3491 - accuracy: 0.8779 - val_loss: 1.6568 - val_accuracy: 0.5027\n",
            "Epoch 1186/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.3254 - accuracy: 0.9132 - val_loss: 1.6334 - val_accuracy: 0.5191\n",
            "Epoch 1187/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3034 - accuracy: 0.9172 - val_loss: 1.6377 - val_accuracy: 0.4863\n",
            "Epoch 1188/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3012 - accuracy: 0.9145 - val_loss: 1.6469 - val_accuracy: 0.5082\n",
            "Epoch 1189/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2950 - accuracy: 0.9213 - val_loss: 1.7038 - val_accuracy: 0.5082\n",
            "Epoch 1190/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3366 - accuracy: 0.8955 - val_loss: 1.6889 - val_accuracy: 0.4863\n",
            "Epoch 1191/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3298 - accuracy: 0.8847 - val_loss: 1.6285 - val_accuracy: 0.5301\n",
            "Epoch 1192/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3156 - accuracy: 0.9077 - val_loss: 1.6413 - val_accuracy: 0.4809\n",
            "Epoch 1193/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3366 - accuracy: 0.9023 - val_loss: 1.6224 - val_accuracy: 0.5137\n",
            "Epoch 1194/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3108 - accuracy: 0.9077 - val_loss: 1.6556 - val_accuracy: 0.5246\n",
            "Epoch 1195/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3205 - accuracy: 0.9050 - val_loss: 1.6499 - val_accuracy: 0.5082\n",
            "Epoch 1196/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3178 - accuracy: 0.8942 - val_loss: 1.6490 - val_accuracy: 0.5191\n",
            "Epoch 1197/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3114 - accuracy: 0.9064 - val_loss: 1.6459 - val_accuracy: 0.5137\n",
            "Epoch 1198/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3355 - accuracy: 0.8996 - val_loss: 1.7511 - val_accuracy: 0.4863\n",
            "Epoch 1199/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3243 - accuracy: 0.9023 - val_loss: 1.6459 - val_accuracy: 0.5355\n",
            "Epoch 1200/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3118 - accuracy: 0.8982 - val_loss: 1.6661 - val_accuracy: 0.5246\n",
            "Epoch 1201/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.3282 - accuracy: 0.9023 - val_loss: 1.6286 - val_accuracy: 0.4918\n",
            "Epoch 1202/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3306 - accuracy: 0.8887 - val_loss: 1.6422 - val_accuracy: 0.5137\n",
            "Epoch 1203/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3134 - accuracy: 0.8942 - val_loss: 1.6976 - val_accuracy: 0.4863\n",
            "Epoch 1204/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3161 - accuracy: 0.8982 - val_loss: 1.6489 - val_accuracy: 0.5191\n",
            "Epoch 1205/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.3107 - accuracy: 0.8969 - val_loss: 1.6614 - val_accuracy: 0.4809\n",
            "Epoch 1206/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3287 - accuracy: 0.8928 - val_loss: 1.6572 - val_accuracy: 0.4918\n",
            "Epoch 1207/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3055 - accuracy: 0.9186 - val_loss: 1.6586 - val_accuracy: 0.5137\n",
            "Epoch 1208/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3315 - accuracy: 0.8915 - val_loss: 1.6374 - val_accuracy: 0.4918\n",
            "Epoch 1209/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2917 - accuracy: 0.9132 - val_loss: 1.6560 - val_accuracy: 0.5082\n",
            "Epoch 1210/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2946 - accuracy: 0.9199 - val_loss: 1.6954 - val_accuracy: 0.5082\n",
            "Epoch 1211/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2992 - accuracy: 0.9186 - val_loss: 1.6644 - val_accuracy: 0.4973\n",
            "Epoch 1212/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2933 - accuracy: 0.9199 - val_loss: 1.6488 - val_accuracy: 0.5082\n",
            "Epoch 1213/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2935 - accuracy: 0.9091 - val_loss: 1.6800 - val_accuracy: 0.5082\n",
            "Epoch 1214/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2785 - accuracy: 0.9213 - val_loss: 1.7297 - val_accuracy: 0.4973\n",
            "Epoch 1215/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2993 - accuracy: 0.9009 - val_loss: 1.6589 - val_accuracy: 0.5082\n",
            "Epoch 1216/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3184 - accuracy: 0.9009 - val_loss: 1.6539 - val_accuracy: 0.5137\n",
            "Epoch 1217/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3154 - accuracy: 0.9077 - val_loss: 1.6792 - val_accuracy: 0.5027\n",
            "Epoch 1218/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2925 - accuracy: 0.9254 - val_loss: 1.6553 - val_accuracy: 0.5246\n",
            "Epoch 1219/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2886 - accuracy: 0.9091 - val_loss: 1.6551 - val_accuracy: 0.5191\n",
            "Epoch 1220/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2905 - accuracy: 0.9145 - val_loss: 1.6806 - val_accuracy: 0.4973\n",
            "Epoch 1221/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2888 - accuracy: 0.9172 - val_loss: 1.7750 - val_accuracy: 0.5027\n",
            "Epoch 1222/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3201 - accuracy: 0.8915 - val_loss: 1.6836 - val_accuracy: 0.5027\n",
            "Epoch 1223/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.3056 - accuracy: 0.9091 - val_loss: 1.6901 - val_accuracy: 0.4809\n",
            "Epoch 1224/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.2847 - accuracy: 0.9172 - val_loss: 1.6680 - val_accuracy: 0.5082\n",
            "Epoch 1225/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.3045 - accuracy: 0.8996 - val_loss: 1.7132 - val_accuracy: 0.4809\n",
            "Epoch 1226/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2824 - accuracy: 0.9186 - val_loss: 1.8376 - val_accuracy: 0.5027\n",
            "Epoch 1227/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.3008 - accuracy: 0.9064 - val_loss: 1.6905 - val_accuracy: 0.4918\n",
            "Epoch 1228/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2923 - accuracy: 0.9118 - val_loss: 1.7119 - val_accuracy: 0.5027\n",
            "Epoch 1229/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2790 - accuracy: 0.9267 - val_loss: 1.6894 - val_accuracy: 0.5027\n",
            "Epoch 1230/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.2902 - accuracy: 0.9172 - val_loss: 1.7015 - val_accuracy: 0.5027\n",
            "Epoch 1231/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2934 - accuracy: 0.9023 - val_loss: 1.6967 - val_accuracy: 0.4863\n",
            "Epoch 1232/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2964 - accuracy: 0.9104 - val_loss: 1.6949 - val_accuracy: 0.5191\n",
            "Epoch 1233/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2826 - accuracy: 0.9240 - val_loss: 1.6748 - val_accuracy: 0.5082\n",
            "Epoch 1234/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2953 - accuracy: 0.8982 - val_loss: 1.6806 - val_accuracy: 0.5246\n",
            "Epoch 1235/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2678 - accuracy: 0.9281 - val_loss: 1.6967 - val_accuracy: 0.5082\n",
            "Epoch 1236/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2806 - accuracy: 0.9199 - val_loss: 1.6866 - val_accuracy: 0.5137\n",
            "Epoch 1237/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2827 - accuracy: 0.9186 - val_loss: 1.6837 - val_accuracy: 0.5137\n",
            "Epoch 1238/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2773 - accuracy: 0.9186 - val_loss: 1.7150 - val_accuracy: 0.4973\n",
            "Epoch 1239/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2723 - accuracy: 0.9199 - val_loss: 1.6897 - val_accuracy: 0.5027\n",
            "Epoch 1240/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2782 - accuracy: 0.9213 - val_loss: 1.7347 - val_accuracy: 0.5191\n",
            "Epoch 1241/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2665 - accuracy: 0.9172 - val_loss: 1.6992 - val_accuracy: 0.5137\n",
            "Epoch 1242/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2741 - accuracy: 0.9132 - val_loss: 1.7056 - val_accuracy: 0.5082\n",
            "Epoch 1243/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2712 - accuracy: 0.9227 - val_loss: 1.6971 - val_accuracy: 0.5027\n",
            "Epoch 1244/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2653 - accuracy: 0.9172 - val_loss: 1.6915 - val_accuracy: 0.5082\n",
            "Epoch 1245/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2887 - accuracy: 0.9104 - val_loss: 1.7169 - val_accuracy: 0.5137\n",
            "Epoch 1246/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2787 - accuracy: 0.9145 - val_loss: 1.7408 - val_accuracy: 0.5082\n",
            "Epoch 1247/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.2868 - accuracy: 0.8969 - val_loss: 1.8140 - val_accuracy: 0.4918\n",
            "Epoch 1248/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2681 - accuracy: 0.9199 - val_loss: 1.6992 - val_accuracy: 0.4973\n",
            "Epoch 1249/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2486 - accuracy: 0.9227 - val_loss: 1.7054 - val_accuracy: 0.5027\n",
            "Epoch 1250/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2901 - accuracy: 0.9118 - val_loss: 1.7464 - val_accuracy: 0.5027\n",
            "Epoch 1251/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2636 - accuracy: 0.9186 - val_loss: 1.7666 - val_accuracy: 0.4973\n",
            "Epoch 1252/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2724 - accuracy: 0.9227 - val_loss: 1.7300 - val_accuracy: 0.5301\n",
            "Epoch 1253/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2686 - accuracy: 0.9104 - val_loss: 1.7359 - val_accuracy: 0.5137\n",
            "Epoch 1254/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2713 - accuracy: 0.9145 - val_loss: 1.7272 - val_accuracy: 0.5027\n",
            "Epoch 1255/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2770 - accuracy: 0.9064 - val_loss: 1.7285 - val_accuracy: 0.5137\n",
            "Epoch 1256/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2825 - accuracy: 0.9159 - val_loss: 1.7203 - val_accuracy: 0.4918\n",
            "Epoch 1257/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2516 - accuracy: 0.9349 - val_loss: 1.7305 - val_accuracy: 0.5137\n",
            "Epoch 1258/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2602 - accuracy: 0.9159 - val_loss: 1.7068 - val_accuracy: 0.5355\n",
            "Epoch 1259/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2800 - accuracy: 0.9213 - val_loss: 1.7162 - val_accuracy: 0.4973\n",
            "Epoch 1260/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2583 - accuracy: 0.9308 - val_loss: 1.7183 - val_accuracy: 0.5027\n",
            "Epoch 1261/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2518 - accuracy: 0.9267 - val_loss: 1.7065 - val_accuracy: 0.5137\n",
            "Epoch 1262/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3066 - accuracy: 0.9050 - val_loss: 1.7162 - val_accuracy: 0.5191\n",
            "Epoch 1263/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2607 - accuracy: 0.9240 - val_loss: 1.7161 - val_accuracy: 0.5137\n",
            "Epoch 1264/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2652 - accuracy: 0.9132 - val_loss: 1.7308 - val_accuracy: 0.4863\n",
            "Epoch 1265/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2604 - accuracy: 0.9254 - val_loss: 1.7175 - val_accuracy: 0.5137\n",
            "Epoch 1266/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2536 - accuracy: 0.9267 - val_loss: 1.7228 - val_accuracy: 0.5246\n",
            "Epoch 1267/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2620 - accuracy: 0.9213 - val_loss: 1.7153 - val_accuracy: 0.4973\n",
            "Epoch 1268/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2604 - accuracy: 0.9240 - val_loss: 1.7096 - val_accuracy: 0.4973\n",
            "Epoch 1269/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2575 - accuracy: 0.9267 - val_loss: 1.7881 - val_accuracy: 0.5191\n",
            "Epoch 1270/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2717 - accuracy: 0.9159 - val_loss: 1.7371 - val_accuracy: 0.5137\n",
            "Epoch 1271/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2645 - accuracy: 0.9199 - val_loss: 1.7568 - val_accuracy: 0.5027\n",
            "Epoch 1272/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2459 - accuracy: 0.9322 - val_loss: 1.7302 - val_accuracy: 0.5137\n",
            "Epoch 1273/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2705 - accuracy: 0.9145 - val_loss: 1.7584 - val_accuracy: 0.5246\n",
            "Epoch 1274/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2386 - accuracy: 0.9281 - val_loss: 1.7140 - val_accuracy: 0.5027\n",
            "Epoch 1275/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.2631 - accuracy: 0.9186 - val_loss: 1.7496 - val_accuracy: 0.5191\n",
            "Epoch 1276/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2385 - accuracy: 0.9376 - val_loss: 1.7239 - val_accuracy: 0.5082\n",
            "Epoch 1277/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2571 - accuracy: 0.9159 - val_loss: 1.7226 - val_accuracy: 0.5191\n",
            "Epoch 1278/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2549 - accuracy: 0.9227 - val_loss: 1.8077 - val_accuracy: 0.5137\n",
            "Epoch 1279/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2664 - accuracy: 0.9267 - val_loss: 1.7507 - val_accuracy: 0.5137\n",
            "Epoch 1280/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2632 - accuracy: 0.9172 - val_loss: 1.7573 - val_accuracy: 0.5082\n",
            "Epoch 1281/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2371 - accuracy: 0.9349 - val_loss: 1.7853 - val_accuracy: 0.5355\n",
            "Epoch 1282/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2583 - accuracy: 0.9172 - val_loss: 1.7359 - val_accuracy: 0.5137\n",
            "Epoch 1283/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2381 - accuracy: 0.9281 - val_loss: 1.7339 - val_accuracy: 0.5082\n",
            "Epoch 1284/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2620 - accuracy: 0.9227 - val_loss: 1.7242 - val_accuracy: 0.4973\n",
            "Epoch 1285/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2565 - accuracy: 0.9254 - val_loss: 1.7575 - val_accuracy: 0.4918\n",
            "Epoch 1286/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2522 - accuracy: 0.9267 - val_loss: 1.7497 - val_accuracy: 0.4973\n",
            "Epoch 1287/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2529 - accuracy: 0.9240 - val_loss: 1.7408 - val_accuracy: 0.4863\n",
            "Epoch 1288/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2435 - accuracy: 0.9281 - val_loss: 1.7433 - val_accuracy: 0.4918\n",
            "Epoch 1289/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.2388 - accuracy: 0.9362 - val_loss: 1.7349 - val_accuracy: 0.5027\n",
            "Epoch 1290/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2708 - accuracy: 0.9172 - val_loss: 1.7756 - val_accuracy: 0.5082\n",
            "Epoch 1291/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2536 - accuracy: 0.9199 - val_loss: 1.7532 - val_accuracy: 0.5301\n",
            "Epoch 1292/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2496 - accuracy: 0.9281 - val_loss: 1.7768 - val_accuracy: 0.5027\n",
            "Epoch 1293/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2419 - accuracy: 0.9281 - val_loss: 1.7318 - val_accuracy: 0.5301\n",
            "Epoch 1294/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2609 - accuracy: 0.9240 - val_loss: 1.7548 - val_accuracy: 0.5191\n",
            "Epoch 1295/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2633 - accuracy: 0.9091 - val_loss: 1.7521 - val_accuracy: 0.5082\n",
            "Epoch 1296/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2378 - accuracy: 0.9281 - val_loss: 1.8008 - val_accuracy: 0.5191\n",
            "Epoch 1297/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2404 - accuracy: 0.9294 - val_loss: 1.7762 - val_accuracy: 0.5191\n",
            "Epoch 1298/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2459 - accuracy: 0.9294 - val_loss: 1.7534 - val_accuracy: 0.4973\n",
            "Epoch 1299/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2569 - accuracy: 0.9227 - val_loss: 1.7454 - val_accuracy: 0.5191\n",
            "Epoch 1300/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2379 - accuracy: 0.9294 - val_loss: 1.7667 - val_accuracy: 0.5137\n",
            "Epoch 1301/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.2291 - accuracy: 0.9349 - val_loss: 1.7677 - val_accuracy: 0.5082\n",
            "Epoch 1302/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2415 - accuracy: 0.9254 - val_loss: 1.7485 - val_accuracy: 0.4973\n",
            "Epoch 1303/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2578 - accuracy: 0.9159 - val_loss: 1.7714 - val_accuracy: 0.5191\n",
            "Epoch 1304/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2389 - accuracy: 0.9240 - val_loss: 1.7598 - val_accuracy: 0.4918\n",
            "Epoch 1305/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2531 - accuracy: 0.9159 - val_loss: 1.7885 - val_accuracy: 0.5027\n",
            "Epoch 1306/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2336 - accuracy: 0.9362 - val_loss: 1.7619 - val_accuracy: 0.4918\n",
            "Epoch 1307/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2424 - accuracy: 0.9294 - val_loss: 1.7492 - val_accuracy: 0.4918\n",
            "Epoch 1308/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2417 - accuracy: 0.9335 - val_loss: 1.7450 - val_accuracy: 0.5355\n",
            "Epoch 1309/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2463 - accuracy: 0.9267 - val_loss: 1.7515 - val_accuracy: 0.5191\n",
            "Epoch 1310/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2603 - accuracy: 0.9335 - val_loss: 1.7710 - val_accuracy: 0.5082\n",
            "Epoch 1311/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2264 - accuracy: 0.9349 - val_loss: 1.7511 - val_accuracy: 0.4918\n",
            "Epoch 1312/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2244 - accuracy: 0.9389 - val_loss: 1.8018 - val_accuracy: 0.4918\n",
            "Epoch 1313/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2244 - accuracy: 0.9403 - val_loss: 1.7597 - val_accuracy: 0.5082\n",
            "Epoch 1314/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2275 - accuracy: 0.9362 - val_loss: 1.7715 - val_accuracy: 0.5301\n",
            "Epoch 1315/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2345 - accuracy: 0.9376 - val_loss: 1.7505 - val_accuracy: 0.5191\n",
            "Epoch 1316/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2210 - accuracy: 0.9376 - val_loss: 1.8464 - val_accuracy: 0.5464\n",
            "Epoch 1317/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2362 - accuracy: 0.9294 - val_loss: 1.8036 - val_accuracy: 0.5082\n",
            "Epoch 1318/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2245 - accuracy: 0.9389 - val_loss: 1.8174 - val_accuracy: 0.5027\n",
            "Epoch 1319/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2575 - accuracy: 0.9199 - val_loss: 1.7586 - val_accuracy: 0.4973\n",
            "Epoch 1320/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2449 - accuracy: 0.9349 - val_loss: 1.7741 - val_accuracy: 0.5082\n",
            "Epoch 1321/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2561 - accuracy: 0.9132 - val_loss: 1.7640 - val_accuracy: 0.5137\n",
            "Epoch 1322/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2548 - accuracy: 0.9145 - val_loss: 1.7601 - val_accuracy: 0.5082\n",
            "Epoch 1323/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2183 - accuracy: 0.9417 - val_loss: 1.7554 - val_accuracy: 0.5027\n",
            "Epoch 1324/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2259 - accuracy: 0.9430 - val_loss: 1.7806 - val_accuracy: 0.5082\n",
            "Epoch 1325/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2197 - accuracy: 0.9376 - val_loss: 1.7800 - val_accuracy: 0.5191\n",
            "Epoch 1326/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2013 - accuracy: 0.9566 - val_loss: 1.8723 - val_accuracy: 0.5027\n",
            "Epoch 1327/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2186 - accuracy: 0.9484 - val_loss: 1.8025 - val_accuracy: 0.5137\n",
            "Epoch 1328/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2391 - accuracy: 0.9335 - val_loss: 1.8144 - val_accuracy: 0.5027\n",
            "Epoch 1329/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2118 - accuracy: 0.9525 - val_loss: 1.8338 - val_accuracy: 0.5082\n",
            "Epoch 1330/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2321 - accuracy: 0.9267 - val_loss: 1.7764 - val_accuracy: 0.5137\n",
            "Epoch 1331/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2308 - accuracy: 0.9294 - val_loss: 1.7755 - val_accuracy: 0.5191\n",
            "Epoch 1332/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2208 - accuracy: 0.9322 - val_loss: 1.7782 - val_accuracy: 0.5082\n",
            "Epoch 1333/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2164 - accuracy: 0.9471 - val_loss: 1.7944 - val_accuracy: 0.4973\n",
            "Epoch 1334/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2247 - accuracy: 0.9430 - val_loss: 1.8207 - val_accuracy: 0.5027\n",
            "Epoch 1335/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2073 - accuracy: 0.9457 - val_loss: 1.8302 - val_accuracy: 0.5246\n",
            "Epoch 1336/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2062 - accuracy: 0.9417 - val_loss: 1.7901 - val_accuracy: 0.5137\n",
            "Epoch 1337/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.2032 - accuracy: 0.9444 - val_loss: 1.8338 - val_accuracy: 0.5082\n",
            "Epoch 1338/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2063 - accuracy: 0.9430 - val_loss: 1.8135 - val_accuracy: 0.4918\n",
            "Epoch 1339/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2157 - accuracy: 0.9389 - val_loss: 1.8099 - val_accuracy: 0.4863\n",
            "Epoch 1340/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2273 - accuracy: 0.9376 - val_loss: 1.8222 - val_accuracy: 0.5137\n",
            "Epoch 1341/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2250 - accuracy: 0.9322 - val_loss: 1.8009 - val_accuracy: 0.4863\n",
            "Epoch 1342/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2195 - accuracy: 0.9322 - val_loss: 1.7986 - val_accuracy: 0.5027\n",
            "Epoch 1343/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2245 - accuracy: 0.9362 - val_loss: 1.8524 - val_accuracy: 0.5137\n",
            "Epoch 1344/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.2078 - accuracy: 0.9457 - val_loss: 1.8266 - val_accuracy: 0.5027\n",
            "Epoch 1345/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2012 - accuracy: 0.9498 - val_loss: 1.8116 - val_accuracy: 0.5137\n",
            "Epoch 1346/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2373 - accuracy: 0.9308 - val_loss: 1.8139 - val_accuracy: 0.5082\n",
            "Epoch 1347/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.2188 - accuracy: 0.9376 - val_loss: 1.8244 - val_accuracy: 0.5082\n",
            "Epoch 1348/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2019 - accuracy: 0.9349 - val_loss: 1.8246 - val_accuracy: 0.5137\n",
            "Epoch 1349/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2169 - accuracy: 0.9403 - val_loss: 1.7951 - val_accuracy: 0.5027\n",
            "Epoch 1350/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1984 - accuracy: 0.9498 - val_loss: 1.8136 - val_accuracy: 0.5246\n",
            "Epoch 1351/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2116 - accuracy: 0.9362 - val_loss: 1.8109 - val_accuracy: 0.5082\n",
            "Epoch 1352/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.2159 - accuracy: 0.9294 - val_loss: 1.8271 - val_accuracy: 0.5137\n",
            "Epoch 1353/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2046 - accuracy: 0.9403 - val_loss: 1.8333 - val_accuracy: 0.5191\n",
            "Epoch 1354/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2006 - accuracy: 0.9525 - val_loss: 1.8153 - val_accuracy: 0.4809\n",
            "Epoch 1355/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1981 - accuracy: 0.9566 - val_loss: 1.8405 - val_accuracy: 0.4973\n",
            "Epoch 1356/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2052 - accuracy: 0.9471 - val_loss: 1.7999 - val_accuracy: 0.5027\n",
            "Epoch 1357/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1855 - accuracy: 0.9552 - val_loss: 1.8275 - val_accuracy: 0.4863\n",
            "Epoch 1358/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2071 - accuracy: 0.9444 - val_loss: 1.9076 - val_accuracy: 0.5137\n",
            "Epoch 1359/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2032 - accuracy: 0.9444 - val_loss: 1.8281 - val_accuracy: 0.4973\n",
            "Epoch 1360/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2040 - accuracy: 0.9444 - val_loss: 1.8200 - val_accuracy: 0.4918\n",
            "Epoch 1361/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1848 - accuracy: 0.9552 - val_loss: 1.8260 - val_accuracy: 0.5137\n",
            "Epoch 1362/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2339 - accuracy: 0.9281 - val_loss: 1.8158 - val_accuracy: 0.5137\n",
            "Epoch 1363/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2221 - accuracy: 0.9376 - val_loss: 1.8076 - val_accuracy: 0.4918\n",
            "Epoch 1364/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2097 - accuracy: 0.9430 - val_loss: 1.8437 - val_accuracy: 0.5082\n",
            "Epoch 1365/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2176 - accuracy: 0.9308 - val_loss: 1.8525 - val_accuracy: 0.4973\n",
            "Epoch 1366/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.2027 - accuracy: 0.9444 - val_loss: 1.8346 - val_accuracy: 0.5137\n",
            "Epoch 1367/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2196 - accuracy: 0.9294 - val_loss: 1.8480 - val_accuracy: 0.5137\n",
            "Epoch 1368/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2083 - accuracy: 0.9471 - val_loss: 1.8757 - val_accuracy: 0.5246\n",
            "Epoch 1369/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1874 - accuracy: 0.9552 - val_loss: 1.9470 - val_accuracy: 0.5137\n",
            "Epoch 1370/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1989 - accuracy: 0.9471 - val_loss: 1.8668 - val_accuracy: 0.5027\n",
            "Epoch 1371/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1952 - accuracy: 0.9552 - val_loss: 1.8409 - val_accuracy: 0.5137\n",
            "Epoch 1372/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2018 - accuracy: 0.9471 - val_loss: 1.8394 - val_accuracy: 0.5082\n",
            "Epoch 1373/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2150 - accuracy: 0.9362 - val_loss: 1.9217 - val_accuracy: 0.4973\n",
            "Epoch 1374/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1853 - accuracy: 0.9484 - val_loss: 1.8500 - val_accuracy: 0.5191\n",
            "Epoch 1375/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1774 - accuracy: 0.9539 - val_loss: 1.8650 - val_accuracy: 0.4918\n",
            "Epoch 1376/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1963 - accuracy: 0.9512 - val_loss: 1.8532 - val_accuracy: 0.4863\n",
            "Epoch 1377/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2061 - accuracy: 0.9417 - val_loss: 1.8457 - val_accuracy: 0.5246\n",
            "Epoch 1378/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1857 - accuracy: 0.9525 - val_loss: 1.8985 - val_accuracy: 0.4973\n",
            "Epoch 1379/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2133 - accuracy: 0.9417 - val_loss: 1.8719 - val_accuracy: 0.5191\n",
            "Epoch 1380/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2028 - accuracy: 0.9430 - val_loss: 1.8546 - val_accuracy: 0.4918\n",
            "Epoch 1381/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1966 - accuracy: 0.9376 - val_loss: 1.8523 - val_accuracy: 0.5137\n",
            "Epoch 1382/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.2050 - accuracy: 0.9403 - val_loss: 1.9163 - val_accuracy: 0.4863\n",
            "Epoch 1383/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.1922 - accuracy: 0.9498 - val_loss: 1.8739 - val_accuracy: 0.5082\n",
            "Epoch 1384/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1804 - accuracy: 0.9539 - val_loss: 1.8462 - val_accuracy: 0.4973\n",
            "Epoch 1385/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2063 - accuracy: 0.9430 - val_loss: 1.8311 - val_accuracy: 0.5027\n",
            "Epoch 1386/2018\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.2038 - accuracy: 0.9498 - val_loss: 1.9124 - val_accuracy: 0.5137\n",
            "Epoch 1387/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1977 - accuracy: 0.9539 - val_loss: 1.8521 - val_accuracy: 0.5246\n",
            "Epoch 1388/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2090 - accuracy: 0.9389 - val_loss: 1.8576 - val_accuracy: 0.5027\n",
            "Epoch 1389/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.2068 - accuracy: 0.9362 - val_loss: 1.8549 - val_accuracy: 0.5027\n",
            "Epoch 1390/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1885 - accuracy: 0.9484 - val_loss: 1.9165 - val_accuracy: 0.5027\n",
            "Epoch 1391/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1904 - accuracy: 0.9498 - val_loss: 1.8890 - val_accuracy: 0.5027\n",
            "Epoch 1392/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.2012 - accuracy: 0.9444 - val_loss: 1.8794 - val_accuracy: 0.5082\n",
            "Epoch 1393/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1980 - accuracy: 0.9457 - val_loss: 1.9113 - val_accuracy: 0.5027\n",
            "Epoch 1394/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2034 - accuracy: 0.9389 - val_loss: 1.8958 - val_accuracy: 0.5137\n",
            "Epoch 1395/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1629 - accuracy: 0.9634 - val_loss: 1.8493 - val_accuracy: 0.5082\n",
            "Epoch 1396/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1866 - accuracy: 0.9457 - val_loss: 1.8393 - val_accuracy: 0.5082\n",
            "Epoch 1397/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1727 - accuracy: 0.9634 - val_loss: 1.8641 - val_accuracy: 0.5137\n",
            "Epoch 1398/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1808 - accuracy: 0.9620 - val_loss: 1.8778 - val_accuracy: 0.5191\n",
            "Epoch 1399/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1865 - accuracy: 0.9457 - val_loss: 1.8610 - val_accuracy: 0.5082\n",
            "Epoch 1400/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1843 - accuracy: 0.9484 - val_loss: 1.9956 - val_accuracy: 0.5191\n",
            "Epoch 1401/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.2053 - accuracy: 0.9484 - val_loss: 1.8647 - val_accuracy: 0.4918\n",
            "Epoch 1402/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1835 - accuracy: 0.9498 - val_loss: 1.8372 - val_accuracy: 0.5137\n",
            "Epoch 1403/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1750 - accuracy: 0.9593 - val_loss: 1.8843 - val_accuracy: 0.5246\n",
            "Epoch 1404/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1836 - accuracy: 0.9430 - val_loss: 1.8836 - val_accuracy: 0.5082\n",
            "Epoch 1405/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1912 - accuracy: 0.9457 - val_loss: 1.8912 - val_accuracy: 0.5191\n",
            "Epoch 1406/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1722 - accuracy: 0.9552 - val_loss: 1.8732 - val_accuracy: 0.5027\n",
            "Epoch 1407/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1671 - accuracy: 0.9607 - val_loss: 1.9718 - val_accuracy: 0.4973\n",
            "Epoch 1408/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1797 - accuracy: 0.9457 - val_loss: 1.8572 - val_accuracy: 0.5137\n",
            "Epoch 1409/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1726 - accuracy: 0.9579 - val_loss: 1.8817 - val_accuracy: 0.5027\n",
            "Epoch 1410/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1668 - accuracy: 0.9620 - val_loss: 1.8966 - val_accuracy: 0.5082\n",
            "Epoch 1411/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1963 - accuracy: 0.9335 - val_loss: 1.9083 - val_accuracy: 0.5246\n",
            "Epoch 1412/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1840 - accuracy: 0.9525 - val_loss: 1.8763 - val_accuracy: 0.5082\n",
            "Epoch 1413/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1660 - accuracy: 0.9593 - val_loss: 1.9908 - val_accuracy: 0.5137\n",
            "Epoch 1414/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1732 - accuracy: 0.9661 - val_loss: 1.8833 - val_accuracy: 0.4973\n",
            "Epoch 1415/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1879 - accuracy: 0.9457 - val_loss: 1.8824 - val_accuracy: 0.5027\n",
            "Epoch 1416/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1752 - accuracy: 0.9579 - val_loss: 1.8922 - val_accuracy: 0.5191\n",
            "Epoch 1417/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1518 - accuracy: 0.9701 - val_loss: 1.8979 - val_accuracy: 0.5137\n",
            "Epoch 1418/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1762 - accuracy: 0.9566 - val_loss: 1.9447 - val_accuracy: 0.5246\n",
            "Epoch 1419/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1723 - accuracy: 0.9539 - val_loss: 1.9912 - val_accuracy: 0.4973\n",
            "Epoch 1420/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1916 - accuracy: 0.9484 - val_loss: 1.9221 - val_accuracy: 0.5246\n",
            "Epoch 1421/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1694 - accuracy: 0.9579 - val_loss: 1.9065 - val_accuracy: 0.4863\n",
            "Epoch 1422/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1929 - accuracy: 0.9417 - val_loss: 1.9120 - val_accuracy: 0.5027\n",
            "Epoch 1423/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1912 - accuracy: 0.9457 - val_loss: 1.8916 - val_accuracy: 0.5191\n",
            "Epoch 1424/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1607 - accuracy: 0.9647 - val_loss: 1.8879 - val_accuracy: 0.4863\n",
            "Epoch 1425/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1886 - accuracy: 0.9471 - val_loss: 1.9327 - val_accuracy: 0.5246\n",
            "Epoch 1426/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1688 - accuracy: 0.9566 - val_loss: 1.9168 - val_accuracy: 0.4973\n",
            "Epoch 1427/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1841 - accuracy: 0.9471 - val_loss: 1.8919 - val_accuracy: 0.4918\n",
            "Epoch 1428/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1740 - accuracy: 0.9620 - val_loss: 1.9006 - val_accuracy: 0.5027\n",
            "Epoch 1429/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1850 - accuracy: 0.9430 - val_loss: 1.9488 - val_accuracy: 0.4973\n",
            "Epoch 1430/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1758 - accuracy: 0.9525 - val_loss: 1.9194 - val_accuracy: 0.5082\n",
            "Epoch 1431/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1719 - accuracy: 0.9579 - val_loss: 1.9036 - val_accuracy: 0.5082\n",
            "Epoch 1432/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1549 - accuracy: 0.9647 - val_loss: 1.9264 - val_accuracy: 0.4973\n",
            "Epoch 1433/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1641 - accuracy: 0.9634 - val_loss: 1.9099 - val_accuracy: 0.5082\n",
            "Epoch 1434/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1521 - accuracy: 0.9661 - val_loss: 1.9155 - val_accuracy: 0.5027\n",
            "Epoch 1435/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1835 - accuracy: 0.9362 - val_loss: 1.9096 - val_accuracy: 0.4973\n",
            "Epoch 1436/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1524 - accuracy: 0.9661 - val_loss: 1.9274 - val_accuracy: 0.4973\n",
            "Epoch 1437/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1676 - accuracy: 0.9539 - val_loss: 1.9075 - val_accuracy: 0.5137\n",
            "Epoch 1438/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1643 - accuracy: 0.9539 - val_loss: 1.9647 - val_accuracy: 0.5137\n",
            "Epoch 1439/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1560 - accuracy: 0.9634 - val_loss: 1.9329 - val_accuracy: 0.4973\n",
            "Epoch 1440/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1803 - accuracy: 0.9457 - val_loss: 1.9135 - val_accuracy: 0.4973\n",
            "Epoch 1441/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1627 - accuracy: 0.9525 - val_loss: 2.0064 - val_accuracy: 0.5191\n",
            "Epoch 1442/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1528 - accuracy: 0.9634 - val_loss: 1.9348 - val_accuracy: 0.5191\n",
            "Epoch 1443/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1701 - accuracy: 0.9512 - val_loss: 1.9394 - val_accuracy: 0.5082\n",
            "Epoch 1444/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1822 - accuracy: 0.9512 - val_loss: 1.9066 - val_accuracy: 0.5082\n",
            "Epoch 1445/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1613 - accuracy: 0.9647 - val_loss: 1.9264 - val_accuracy: 0.5082\n",
            "Epoch 1446/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1643 - accuracy: 0.9634 - val_loss: 1.9568 - val_accuracy: 0.5246\n",
            "Epoch 1447/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1500 - accuracy: 0.9647 - val_loss: 1.9729 - val_accuracy: 0.5137\n",
            "Epoch 1448/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1681 - accuracy: 0.9430 - val_loss: 1.9270 - val_accuracy: 0.4973\n",
            "Epoch 1449/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1571 - accuracy: 0.9593 - val_loss: 1.9790 - val_accuracy: 0.5246\n",
            "Epoch 1450/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1703 - accuracy: 0.9634 - val_loss: 1.9388 - val_accuracy: 0.4918\n",
            "Epoch 1451/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1744 - accuracy: 0.9403 - val_loss: 1.9440 - val_accuracy: 0.5137\n",
            "Epoch 1452/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1695 - accuracy: 0.9634 - val_loss: 1.9490 - val_accuracy: 0.5082\n",
            "Epoch 1453/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1537 - accuracy: 0.9579 - val_loss: 1.9488 - val_accuracy: 0.5082\n",
            "Epoch 1454/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1600 - accuracy: 0.9539 - val_loss: 1.9420 - val_accuracy: 0.5301\n",
            "Epoch 1455/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1721 - accuracy: 0.9457 - val_loss: 1.9331 - val_accuracy: 0.5027\n",
            "Epoch 1456/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1513 - accuracy: 0.9620 - val_loss: 1.9734 - val_accuracy: 0.5082\n",
            "Epoch 1457/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1597 - accuracy: 0.9620 - val_loss: 1.9394 - val_accuracy: 0.4973\n",
            "Epoch 1458/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1816 - accuracy: 0.9471 - val_loss: 1.9663 - val_accuracy: 0.5027\n",
            "Epoch 1459/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1557 - accuracy: 0.9593 - val_loss: 1.9635 - val_accuracy: 0.4918\n",
            "Epoch 1460/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1483 - accuracy: 0.9620 - val_loss: 1.9591 - val_accuracy: 0.5027\n",
            "Epoch 1461/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1466 - accuracy: 0.9620 - val_loss: 1.9490 - val_accuracy: 0.5137\n",
            "Epoch 1462/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1792 - accuracy: 0.9512 - val_loss: 1.9653 - val_accuracy: 0.5191\n",
            "Epoch 1463/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1430 - accuracy: 0.9661 - val_loss: 1.9855 - val_accuracy: 0.4918\n",
            "Epoch 1464/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1458 - accuracy: 0.9715 - val_loss: 1.9681 - val_accuracy: 0.4973\n",
            "Epoch 1465/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1519 - accuracy: 0.9620 - val_loss: 1.9993 - val_accuracy: 0.5082\n",
            "Epoch 1466/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1414 - accuracy: 0.9688 - val_loss: 1.9696 - val_accuracy: 0.5027\n",
            "Epoch 1467/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1616 - accuracy: 0.9539 - val_loss: 1.9579 - val_accuracy: 0.5301\n",
            "Epoch 1468/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.1466 - accuracy: 0.9634 - val_loss: 2.0106 - val_accuracy: 0.5191\n",
            "Epoch 1469/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1488 - accuracy: 0.9647 - val_loss: 1.9560 - val_accuracy: 0.5027\n",
            "Epoch 1470/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1683 - accuracy: 0.9552 - val_loss: 1.9573 - val_accuracy: 0.5301\n",
            "Epoch 1471/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1588 - accuracy: 0.9552 - val_loss: 1.9509 - val_accuracy: 0.5137\n",
            "Epoch 1472/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.1500 - accuracy: 0.9607 - val_loss: 1.9506 - val_accuracy: 0.5082\n",
            "Epoch 1473/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1631 - accuracy: 0.9552 - val_loss: 1.9584 - val_accuracy: 0.5137\n",
            "Epoch 1474/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.1470 - accuracy: 0.9620 - val_loss: 1.9736 - val_accuracy: 0.5137\n",
            "Epoch 1475/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1539 - accuracy: 0.9661 - val_loss: 1.9593 - val_accuracy: 0.4918\n",
            "Epoch 1476/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1463 - accuracy: 0.9579 - val_loss: 1.9583 - val_accuracy: 0.4918\n",
            "Epoch 1477/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1452 - accuracy: 0.9674 - val_loss: 2.0062 - val_accuracy: 0.5027\n",
            "Epoch 1478/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1565 - accuracy: 0.9620 - val_loss: 1.9502 - val_accuracy: 0.5191\n",
            "Epoch 1479/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1575 - accuracy: 0.9579 - val_loss: 1.9856 - val_accuracy: 0.5027\n",
            "Epoch 1480/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1441 - accuracy: 0.9729 - val_loss: 1.9496 - val_accuracy: 0.5082\n",
            "Epoch 1481/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1407 - accuracy: 0.9593 - val_loss: 1.9783 - val_accuracy: 0.5137\n",
            "Epoch 1482/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1375 - accuracy: 0.9634 - val_loss: 2.0003 - val_accuracy: 0.4973\n",
            "Epoch 1483/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1308 - accuracy: 0.9715 - val_loss: 1.9721 - val_accuracy: 0.4918\n",
            "Epoch 1484/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1625 - accuracy: 0.9539 - val_loss: 1.9871 - val_accuracy: 0.5027\n",
            "Epoch 1485/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1404 - accuracy: 0.9715 - val_loss: 2.0430 - val_accuracy: 0.4863\n",
            "Epoch 1486/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1444 - accuracy: 0.9661 - val_loss: 1.9775 - val_accuracy: 0.5137\n",
            "Epoch 1487/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1471 - accuracy: 0.9607 - val_loss: 2.0227 - val_accuracy: 0.5191\n",
            "Epoch 1488/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1331 - accuracy: 0.9701 - val_loss: 1.9702 - val_accuracy: 0.5082\n",
            "Epoch 1489/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1496 - accuracy: 0.9674 - val_loss: 2.0334 - val_accuracy: 0.4973\n",
            "Epoch 1490/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1354 - accuracy: 0.9688 - val_loss: 2.0083 - val_accuracy: 0.5301\n",
            "Epoch 1491/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1410 - accuracy: 0.9661 - val_loss: 2.1101 - val_accuracy: 0.5137\n",
            "Epoch 1492/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1334 - accuracy: 0.9674 - val_loss: 2.0137 - val_accuracy: 0.4918\n",
            "Epoch 1493/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1505 - accuracy: 0.9634 - val_loss: 2.0279 - val_accuracy: 0.5082\n",
            "Epoch 1494/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1352 - accuracy: 0.9674 - val_loss: 1.9932 - val_accuracy: 0.5027\n",
            "Epoch 1495/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1342 - accuracy: 0.9647 - val_loss: 2.0130 - val_accuracy: 0.5027\n",
            "Epoch 1496/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1366 - accuracy: 0.9607 - val_loss: 1.9752 - val_accuracy: 0.5082\n",
            "Epoch 1497/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1318 - accuracy: 0.9674 - val_loss: 2.0573 - val_accuracy: 0.5082\n",
            "Epoch 1498/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1582 - accuracy: 0.9512 - val_loss: 2.0183 - val_accuracy: 0.5137\n",
            "Epoch 1499/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1443 - accuracy: 0.9647 - val_loss: 1.9901 - val_accuracy: 0.5027\n",
            "Epoch 1500/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1483 - accuracy: 0.9607 - val_loss: 1.9853 - val_accuracy: 0.5027\n",
            "Epoch 1501/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1403 - accuracy: 0.9715 - val_loss: 2.0648 - val_accuracy: 0.4918\n",
            "Epoch 1502/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1272 - accuracy: 0.9715 - val_loss: 2.0228 - val_accuracy: 0.4973\n",
            "Epoch 1503/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1376 - accuracy: 0.9715 - val_loss: 1.9940 - val_accuracy: 0.5027\n",
            "Epoch 1504/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1445 - accuracy: 0.9634 - val_loss: 2.0284 - val_accuracy: 0.5027\n",
            "Epoch 1505/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1402 - accuracy: 0.9607 - val_loss: 2.0321 - val_accuracy: 0.5137\n",
            "Epoch 1506/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1380 - accuracy: 0.9552 - val_loss: 2.0191 - val_accuracy: 0.5082\n",
            "Epoch 1507/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1331 - accuracy: 0.9729 - val_loss: 2.0001 - val_accuracy: 0.5027\n",
            "Epoch 1508/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1314 - accuracy: 0.9742 - val_loss: 2.0129 - val_accuracy: 0.5082\n",
            "Epoch 1509/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1366 - accuracy: 0.9674 - val_loss: 2.0164 - val_accuracy: 0.5137\n",
            "Epoch 1510/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1402 - accuracy: 0.9674 - val_loss: 2.0649 - val_accuracy: 0.5137\n",
            "Epoch 1511/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1418 - accuracy: 0.9634 - val_loss: 2.1208 - val_accuracy: 0.5027\n",
            "Epoch 1512/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1411 - accuracy: 0.9674 - val_loss: 2.0524 - val_accuracy: 0.5027\n",
            "Epoch 1513/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1291 - accuracy: 0.9715 - val_loss: 2.0842 - val_accuracy: 0.5027\n",
            "Epoch 1514/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1339 - accuracy: 0.9661 - val_loss: 1.9960 - val_accuracy: 0.5027\n",
            "Epoch 1515/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1467 - accuracy: 0.9634 - val_loss: 2.0306 - val_accuracy: 0.5246\n",
            "Epoch 1516/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1463 - accuracy: 0.9566 - val_loss: 1.9831 - val_accuracy: 0.5191\n",
            "Epoch 1517/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1402 - accuracy: 0.9701 - val_loss: 1.9865 - val_accuracy: 0.5082\n",
            "Epoch 1518/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1512 - accuracy: 0.9566 - val_loss: 2.0024 - val_accuracy: 0.4918\n",
            "Epoch 1519/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1416 - accuracy: 0.9620 - val_loss: 2.0340 - val_accuracy: 0.5137\n",
            "Epoch 1520/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1368 - accuracy: 0.9742 - val_loss: 2.0070 - val_accuracy: 0.4918\n",
            "Epoch 1521/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1181 - accuracy: 0.9769 - val_loss: 2.0451 - val_accuracy: 0.5082\n",
            "Epoch 1522/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1276 - accuracy: 0.9701 - val_loss: 1.9992 - val_accuracy: 0.4973\n",
            "Epoch 1523/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1284 - accuracy: 0.9674 - val_loss: 2.0653 - val_accuracy: 0.5027\n",
            "Epoch 1524/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1235 - accuracy: 0.9729 - val_loss: 2.0097 - val_accuracy: 0.5027\n",
            "Epoch 1525/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1261 - accuracy: 0.9783 - val_loss: 2.0792 - val_accuracy: 0.5082\n",
            "Epoch 1526/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1150 - accuracy: 0.9796 - val_loss: 2.0456 - val_accuracy: 0.4973\n",
            "Epoch 1527/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1285 - accuracy: 0.9701 - val_loss: 2.0509 - val_accuracy: 0.4918\n",
            "Epoch 1528/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1320 - accuracy: 0.9783 - val_loss: 2.0883 - val_accuracy: 0.5082\n",
            "Epoch 1529/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1208 - accuracy: 0.9729 - val_loss: 2.0335 - val_accuracy: 0.5027\n",
            "Epoch 1530/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1386 - accuracy: 0.9715 - val_loss: 2.0799 - val_accuracy: 0.4918\n",
            "Epoch 1531/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1201 - accuracy: 0.9715 - val_loss: 2.0586 - val_accuracy: 0.5191\n",
            "Epoch 1532/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1154 - accuracy: 0.9810 - val_loss: 2.0549 - val_accuracy: 0.5137\n",
            "Epoch 1533/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1164 - accuracy: 0.9769 - val_loss: 2.0881 - val_accuracy: 0.4863\n",
            "Epoch 1534/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1309 - accuracy: 0.9715 - val_loss: 2.0766 - val_accuracy: 0.5027\n",
            "Epoch 1535/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1228 - accuracy: 0.9701 - val_loss: 2.0479 - val_accuracy: 0.5027\n",
            "Epoch 1536/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1196 - accuracy: 0.9783 - val_loss: 2.1942 - val_accuracy: 0.4918\n",
            "Epoch 1537/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1352 - accuracy: 0.9688 - val_loss: 2.0524 - val_accuracy: 0.5137\n",
            "Epoch 1538/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1262 - accuracy: 0.9729 - val_loss: 2.0502 - val_accuracy: 0.5137\n",
            "Epoch 1539/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1271 - accuracy: 0.9688 - val_loss: 2.0583 - val_accuracy: 0.5191\n",
            "Epoch 1540/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1191 - accuracy: 0.9783 - val_loss: 2.0717 - val_accuracy: 0.5191\n",
            "Epoch 1541/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1259 - accuracy: 0.9661 - val_loss: 2.0259 - val_accuracy: 0.5082\n",
            "Epoch 1542/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1197 - accuracy: 0.9769 - val_loss: 2.0562 - val_accuracy: 0.5027\n",
            "Epoch 1543/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1213 - accuracy: 0.9634 - val_loss: 2.0833 - val_accuracy: 0.5191\n",
            "Epoch 1544/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1205 - accuracy: 0.9661 - val_loss: 2.0622 - val_accuracy: 0.5137\n",
            "Epoch 1545/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1196 - accuracy: 0.9715 - val_loss: 2.0737 - val_accuracy: 0.5027\n",
            "Epoch 1546/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1193 - accuracy: 0.9742 - val_loss: 2.0688 - val_accuracy: 0.5137\n",
            "Epoch 1547/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1338 - accuracy: 0.9634 - val_loss: 2.0658 - val_accuracy: 0.5082\n",
            "Epoch 1548/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1277 - accuracy: 0.9674 - val_loss: 2.0387 - val_accuracy: 0.5082\n",
            "Epoch 1549/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1351 - accuracy: 0.9634 - val_loss: 2.1054 - val_accuracy: 0.5301\n",
            "Epoch 1550/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1175 - accuracy: 0.9715 - val_loss: 2.0704 - val_accuracy: 0.5191\n",
            "Epoch 1551/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1190 - accuracy: 0.9729 - val_loss: 2.1020 - val_accuracy: 0.5246\n",
            "Epoch 1552/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1290 - accuracy: 0.9715 - val_loss: 2.0360 - val_accuracy: 0.5082\n",
            "Epoch 1553/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1265 - accuracy: 0.9674 - val_loss: 2.0645 - val_accuracy: 0.5082\n",
            "Epoch 1554/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1200 - accuracy: 0.9729 - val_loss: 2.0613 - val_accuracy: 0.5191\n",
            "Epoch 1555/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1250 - accuracy: 0.9620 - val_loss: 2.0525 - val_accuracy: 0.5137\n",
            "Epoch 1556/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1217 - accuracy: 0.9729 - val_loss: 2.0579 - val_accuracy: 0.4918\n",
            "Epoch 1557/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1048 - accuracy: 0.9824 - val_loss: 2.0441 - val_accuracy: 0.5082\n",
            "Epoch 1558/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1156 - accuracy: 0.9756 - val_loss: 2.1400 - val_accuracy: 0.5082\n",
            "Epoch 1559/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1206 - accuracy: 0.9756 - val_loss: 2.2471 - val_accuracy: 0.5027\n",
            "Epoch 1560/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1266 - accuracy: 0.9647 - val_loss: 2.0997 - val_accuracy: 0.5027\n",
            "Epoch 1561/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.1157 - accuracy: 0.9769 - val_loss: 2.1043 - val_accuracy: 0.5082\n",
            "Epoch 1562/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1298 - accuracy: 0.9674 - val_loss: 2.1264 - val_accuracy: 0.5191\n",
            "Epoch 1563/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1027 - accuracy: 0.9742 - val_loss: 2.0789 - val_accuracy: 0.5027\n",
            "Epoch 1564/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1394 - accuracy: 0.9579 - val_loss: 2.1317 - val_accuracy: 0.5027\n",
            "Epoch 1565/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1159 - accuracy: 0.9769 - val_loss: 2.0687 - val_accuracy: 0.5191\n",
            "Epoch 1566/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1100 - accuracy: 0.9742 - val_loss: 2.0916 - val_accuracy: 0.5082\n",
            "Epoch 1567/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1094 - accuracy: 0.9756 - val_loss: 2.0985 - val_accuracy: 0.5082\n",
            "Epoch 1568/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1051 - accuracy: 0.9837 - val_loss: 2.1037 - val_accuracy: 0.4918\n",
            "Epoch 1569/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1065 - accuracy: 0.9810 - val_loss: 2.1075 - val_accuracy: 0.5027\n",
            "Epoch 1570/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1218 - accuracy: 0.9783 - val_loss: 2.1011 - val_accuracy: 0.5191\n",
            "Epoch 1571/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1162 - accuracy: 0.9647 - val_loss: 2.1491 - val_accuracy: 0.4973\n",
            "Epoch 1572/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1146 - accuracy: 0.9824 - val_loss: 2.1206 - val_accuracy: 0.5082\n",
            "Epoch 1573/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1106 - accuracy: 0.9756 - val_loss: 2.1265 - val_accuracy: 0.5191\n",
            "Epoch 1574/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1105 - accuracy: 0.9783 - val_loss: 2.0884 - val_accuracy: 0.5191\n",
            "Epoch 1575/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0956 - accuracy: 0.9864 - val_loss: 2.1091 - val_accuracy: 0.5082\n",
            "Epoch 1576/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1180 - accuracy: 0.9661 - val_loss: 2.0918 - val_accuracy: 0.5191\n",
            "Epoch 1577/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1274 - accuracy: 0.9661 - val_loss: 2.1216 - val_accuracy: 0.5082\n",
            "Epoch 1578/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1135 - accuracy: 0.9769 - val_loss: 2.1063 - val_accuracy: 0.4973\n",
            "Epoch 1579/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1115 - accuracy: 0.9729 - val_loss: 2.1314 - val_accuracy: 0.5082\n",
            "Epoch 1580/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1222 - accuracy: 0.9742 - val_loss: 2.0999 - val_accuracy: 0.5191\n",
            "Epoch 1581/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1076 - accuracy: 0.9729 - val_loss: 2.1362 - val_accuracy: 0.5082\n",
            "Epoch 1582/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1066 - accuracy: 0.9796 - val_loss: 2.1127 - val_accuracy: 0.5191\n",
            "Epoch 1583/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0962 - accuracy: 0.9783 - val_loss: 2.1175 - val_accuracy: 0.5137\n",
            "Epoch 1584/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1107 - accuracy: 0.9756 - val_loss: 2.1966 - val_accuracy: 0.4973\n",
            "Epoch 1585/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1121 - accuracy: 0.9756 - val_loss: 2.1509 - val_accuracy: 0.5027\n",
            "Epoch 1586/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1022 - accuracy: 0.9769 - val_loss: 2.1120 - val_accuracy: 0.5082\n",
            "Epoch 1587/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1226 - accuracy: 0.9756 - val_loss: 2.1066 - val_accuracy: 0.5137\n",
            "Epoch 1588/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1027 - accuracy: 0.9742 - val_loss: 2.1256 - val_accuracy: 0.5246\n",
            "Epoch 1589/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1032 - accuracy: 0.9756 - val_loss: 2.1442 - val_accuracy: 0.5246\n",
            "Epoch 1590/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1178 - accuracy: 0.9620 - val_loss: 2.1329 - val_accuracy: 0.5082\n",
            "Epoch 1591/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1075 - accuracy: 0.9715 - val_loss: 2.1558 - val_accuracy: 0.4918\n",
            "Epoch 1592/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1045 - accuracy: 0.9796 - val_loss: 2.1299 - val_accuracy: 0.5246\n",
            "Epoch 1593/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1072 - accuracy: 0.9796 - val_loss: 2.1000 - val_accuracy: 0.5301\n",
            "Epoch 1594/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.1107 - accuracy: 0.9742 - val_loss: 2.0879 - val_accuracy: 0.5137\n",
            "Epoch 1595/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1079 - accuracy: 0.9729 - val_loss: 2.2201 - val_accuracy: 0.5191\n",
            "Epoch 1596/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1098 - accuracy: 0.9742 - val_loss: 2.1310 - val_accuracy: 0.5027\n",
            "Epoch 1597/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.1141 - accuracy: 0.9742 - val_loss: 2.1589 - val_accuracy: 0.5082\n",
            "Epoch 1598/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0954 - accuracy: 0.9878 - val_loss: 2.1269 - val_accuracy: 0.4973\n",
            "Epoch 1599/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1041 - accuracy: 0.9783 - val_loss: 2.1814 - val_accuracy: 0.5137\n",
            "Epoch 1600/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0973 - accuracy: 0.9810 - val_loss: 2.1426 - val_accuracy: 0.5082\n",
            "Epoch 1601/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1030 - accuracy: 0.9769 - val_loss: 2.1361 - val_accuracy: 0.5082\n",
            "Epoch 1602/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0967 - accuracy: 0.9837 - val_loss: 2.1453 - val_accuracy: 0.4973\n",
            "Epoch 1603/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1253 - accuracy: 0.9661 - val_loss: 2.1540 - val_accuracy: 0.4973\n",
            "Epoch 1604/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1034 - accuracy: 0.9715 - val_loss: 2.1848 - val_accuracy: 0.5082\n",
            "Epoch 1605/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0887 - accuracy: 0.9769 - val_loss: 2.2036 - val_accuracy: 0.5355\n",
            "Epoch 1606/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.1126 - accuracy: 0.9701 - val_loss: 2.1717 - val_accuracy: 0.5082\n",
            "Epoch 1607/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0911 - accuracy: 0.9769 - val_loss: 2.1761 - val_accuracy: 0.5082\n",
            "Epoch 1608/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1041 - accuracy: 0.9756 - val_loss: 2.1713 - val_accuracy: 0.5246\n",
            "Epoch 1609/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0956 - accuracy: 0.9810 - val_loss: 2.1962 - val_accuracy: 0.5082\n",
            "Epoch 1610/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1178 - accuracy: 0.9688 - val_loss: 2.1487 - val_accuracy: 0.5301\n",
            "Epoch 1611/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0974 - accuracy: 0.9837 - val_loss: 2.1833 - val_accuracy: 0.5082\n",
            "Epoch 1612/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1117 - accuracy: 0.9742 - val_loss: 2.1895 - val_accuracy: 0.5137\n",
            "Epoch 1613/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1047 - accuracy: 0.9769 - val_loss: 2.1677 - val_accuracy: 0.5027\n",
            "Epoch 1614/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0906 - accuracy: 0.9769 - val_loss: 2.1615 - val_accuracy: 0.5191\n",
            "Epoch 1615/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1081 - accuracy: 0.9715 - val_loss: 2.2214 - val_accuracy: 0.5246\n",
            "Epoch 1616/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0877 - accuracy: 0.9864 - val_loss: 2.1552 - val_accuracy: 0.5137\n",
            "Epoch 1617/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1226 - accuracy: 0.9715 - val_loss: 2.1546 - val_accuracy: 0.5137\n",
            "Epoch 1618/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0985 - accuracy: 0.9769 - val_loss: 2.1553 - val_accuracy: 0.5027\n",
            "Epoch 1619/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0890 - accuracy: 0.9851 - val_loss: 2.1225 - val_accuracy: 0.5137\n",
            "Epoch 1620/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0956 - accuracy: 0.9769 - val_loss: 2.1744 - val_accuracy: 0.5137\n",
            "Epoch 1621/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0928 - accuracy: 0.9824 - val_loss: 2.1513 - val_accuracy: 0.5082\n",
            "Epoch 1622/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1081 - accuracy: 0.9688 - val_loss: 2.1118 - val_accuracy: 0.5191\n",
            "Epoch 1623/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0794 - accuracy: 0.9878 - val_loss: 2.2095 - val_accuracy: 0.5137\n",
            "Epoch 1624/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0967 - accuracy: 0.9769 - val_loss: 2.2539 - val_accuracy: 0.5137\n",
            "Epoch 1625/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1061 - accuracy: 0.9647 - val_loss: 2.1640 - val_accuracy: 0.4918\n",
            "Epoch 1626/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1004 - accuracy: 0.9810 - val_loss: 2.1836 - val_accuracy: 0.5082\n",
            "Epoch 1627/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0897 - accuracy: 0.9824 - val_loss: 2.2181 - val_accuracy: 0.5191\n",
            "Epoch 1628/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0970 - accuracy: 0.9769 - val_loss: 2.1548 - val_accuracy: 0.5027\n",
            "Epoch 1629/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0930 - accuracy: 0.9824 - val_loss: 2.1688 - val_accuracy: 0.4918\n",
            "Epoch 1630/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0979 - accuracy: 0.9796 - val_loss: 2.1435 - val_accuracy: 0.5191\n",
            "Epoch 1631/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0984 - accuracy: 0.9756 - val_loss: 2.1819 - val_accuracy: 0.5027\n",
            "Epoch 1632/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1006 - accuracy: 0.9729 - val_loss: 2.2334 - val_accuracy: 0.5082\n",
            "Epoch 1633/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.1097 - accuracy: 0.9674 - val_loss: 2.1945 - val_accuracy: 0.5191\n",
            "Epoch 1634/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0887 - accuracy: 0.9851 - val_loss: 2.1559 - val_accuracy: 0.5082\n",
            "Epoch 1635/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1118 - accuracy: 0.9715 - val_loss: 2.1751 - val_accuracy: 0.5082\n",
            "Epoch 1636/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1017 - accuracy: 0.9796 - val_loss: 2.2547 - val_accuracy: 0.4973\n",
            "Epoch 1637/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0942 - accuracy: 0.9783 - val_loss: 2.1782 - val_accuracy: 0.5082\n",
            "Epoch 1638/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0923 - accuracy: 0.9783 - val_loss: 2.2098 - val_accuracy: 0.5137\n",
            "Epoch 1639/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0801 - accuracy: 0.9891 - val_loss: 2.2228 - val_accuracy: 0.5027\n",
            "Epoch 1640/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.1021 - accuracy: 0.9783 - val_loss: 2.2046 - val_accuracy: 0.5027\n",
            "Epoch 1641/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0952 - accuracy: 0.9783 - val_loss: 2.2066 - val_accuracy: 0.4918\n",
            "Epoch 1642/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0933 - accuracy: 0.9756 - val_loss: 2.2419 - val_accuracy: 0.5246\n",
            "Epoch 1643/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0896 - accuracy: 0.9864 - val_loss: 2.2081 - val_accuracy: 0.4973\n",
            "Epoch 1644/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0917 - accuracy: 0.9796 - val_loss: 2.2032 - val_accuracy: 0.5246\n",
            "Epoch 1645/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0839 - accuracy: 0.9824 - val_loss: 2.1983 - val_accuracy: 0.4973\n",
            "Epoch 1646/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0911 - accuracy: 0.9756 - val_loss: 2.2351 - val_accuracy: 0.4918\n",
            "Epoch 1647/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.1021 - accuracy: 0.9742 - val_loss: 2.1713 - val_accuracy: 0.5137\n",
            "Epoch 1648/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0952 - accuracy: 0.9810 - val_loss: 2.1575 - val_accuracy: 0.5027\n",
            "Epoch 1649/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0853 - accuracy: 0.9837 - val_loss: 2.1807 - val_accuracy: 0.5137\n",
            "Epoch 1650/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0966 - accuracy: 0.9796 - val_loss: 2.1769 - val_accuracy: 0.5082\n",
            "Epoch 1651/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0935 - accuracy: 0.9810 - val_loss: 2.2315 - val_accuracy: 0.5191\n",
            "Epoch 1652/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0828 - accuracy: 0.9837 - val_loss: 2.2335 - val_accuracy: 0.5082\n",
            "Epoch 1653/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0932 - accuracy: 0.9742 - val_loss: 2.1765 - val_accuracy: 0.5246\n",
            "Epoch 1654/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0983 - accuracy: 0.9715 - val_loss: 2.1895 - val_accuracy: 0.5082\n",
            "Epoch 1655/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0866 - accuracy: 0.9837 - val_loss: 2.2302 - val_accuracy: 0.5191\n",
            "Epoch 1656/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0827 - accuracy: 0.9796 - val_loss: 2.2483 - val_accuracy: 0.5137\n",
            "Epoch 1657/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0874 - accuracy: 0.9783 - val_loss: 2.2264 - val_accuracy: 0.4973\n",
            "Epoch 1658/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0796 - accuracy: 0.9864 - val_loss: 2.2406 - val_accuracy: 0.5191\n",
            "Epoch 1659/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0921 - accuracy: 0.9796 - val_loss: 2.1821 - val_accuracy: 0.5027\n",
            "Epoch 1660/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0805 - accuracy: 0.9824 - val_loss: 2.2334 - val_accuracy: 0.5137\n",
            "Epoch 1661/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0831 - accuracy: 0.9891 - val_loss: 2.2957 - val_accuracy: 0.5137\n",
            "Epoch 1662/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0800 - accuracy: 0.9878 - val_loss: 2.2325 - val_accuracy: 0.4973\n",
            "Epoch 1663/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0912 - accuracy: 0.9756 - val_loss: 2.2068 - val_accuracy: 0.5137\n",
            "Epoch 1664/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0796 - accuracy: 0.9851 - val_loss: 2.1971 - val_accuracy: 0.5137\n",
            "Epoch 1665/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0764 - accuracy: 0.9878 - val_loss: 2.2902 - val_accuracy: 0.5301\n",
            "Epoch 1666/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0849 - accuracy: 0.9851 - val_loss: 2.3201 - val_accuracy: 0.5191\n",
            "Epoch 1667/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0792 - accuracy: 0.9864 - val_loss: 2.2477 - val_accuracy: 0.5027\n",
            "Epoch 1668/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0839 - accuracy: 0.9837 - val_loss: 2.2208 - val_accuracy: 0.5082\n",
            "Epoch 1669/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0933 - accuracy: 0.9783 - val_loss: 2.2331 - val_accuracy: 0.5027\n",
            "Epoch 1670/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0800 - accuracy: 0.9864 - val_loss: 2.2192 - val_accuracy: 0.5137\n",
            "Epoch 1671/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0978 - accuracy: 0.9742 - val_loss: 2.2544 - val_accuracy: 0.5082\n",
            "Epoch 1672/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0879 - accuracy: 0.9783 - val_loss: 2.3315 - val_accuracy: 0.5137\n",
            "Epoch 1673/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0945 - accuracy: 0.9837 - val_loss: 2.2403 - val_accuracy: 0.5246\n",
            "Epoch 1674/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0963 - accuracy: 0.9796 - val_loss: 2.2229 - val_accuracy: 0.5027\n",
            "Epoch 1675/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0745 - accuracy: 0.9824 - val_loss: 2.2613 - val_accuracy: 0.5027\n",
            "Epoch 1676/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0817 - accuracy: 0.9919 - val_loss: 2.2755 - val_accuracy: 0.5191\n",
            "Epoch 1677/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0804 - accuracy: 0.9864 - val_loss: 2.2721 - val_accuracy: 0.5137\n",
            "Epoch 1678/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0836 - accuracy: 0.9837 - val_loss: 2.2683 - val_accuracy: 0.5082\n",
            "Epoch 1679/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0770 - accuracy: 0.9905 - val_loss: 2.3220 - val_accuracy: 0.5137\n",
            "Epoch 1680/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0862 - accuracy: 0.9783 - val_loss: 2.3540 - val_accuracy: 0.4918\n",
            "Epoch 1681/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0832 - accuracy: 0.9837 - val_loss: 2.2251 - val_accuracy: 0.5137\n",
            "Epoch 1682/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0873 - accuracy: 0.9824 - val_loss: 2.2610 - val_accuracy: 0.5082\n",
            "Epoch 1683/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0928 - accuracy: 0.9756 - val_loss: 2.2770 - val_accuracy: 0.5027\n",
            "Epoch 1684/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0800 - accuracy: 0.9878 - val_loss: 2.3122 - val_accuracy: 0.5137\n",
            "Epoch 1685/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0961 - accuracy: 0.9729 - val_loss: 2.2670 - val_accuracy: 0.4863\n",
            "Epoch 1686/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0811 - accuracy: 0.9824 - val_loss: 2.2824 - val_accuracy: 0.4973\n",
            "Epoch 1687/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0817 - accuracy: 0.9864 - val_loss: 2.3421 - val_accuracy: 0.5027\n",
            "Epoch 1688/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0728 - accuracy: 0.9824 - val_loss: 2.2983 - val_accuracy: 0.5027\n",
            "Epoch 1689/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0730 - accuracy: 0.9837 - val_loss: 2.2711 - val_accuracy: 0.5027\n",
            "Epoch 1690/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0960 - accuracy: 0.9810 - val_loss: 2.2661 - val_accuracy: 0.4973\n",
            "Epoch 1691/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0713 - accuracy: 0.9878 - val_loss: 2.2587 - val_accuracy: 0.5082\n",
            "Epoch 1692/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0823 - accuracy: 0.9810 - val_loss: 2.2585 - val_accuracy: 0.4918\n",
            "Epoch 1693/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0729 - accuracy: 0.9851 - val_loss: 2.2902 - val_accuracy: 0.5082\n",
            "Epoch 1694/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0772 - accuracy: 0.9864 - val_loss: 2.2684 - val_accuracy: 0.4918\n",
            "Epoch 1695/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0786 - accuracy: 0.9878 - val_loss: 2.2972 - val_accuracy: 0.5137\n",
            "Epoch 1696/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0802 - accuracy: 0.9837 - val_loss: 2.3736 - val_accuracy: 0.5137\n",
            "Epoch 1697/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0911 - accuracy: 0.9837 - val_loss: 2.3423 - val_accuracy: 0.5246\n",
            "Epoch 1698/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0603 - accuracy: 0.9932 - val_loss: 2.3322 - val_accuracy: 0.5082\n",
            "Epoch 1699/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0695 - accuracy: 0.9891 - val_loss: 2.3523 - val_accuracy: 0.5082\n",
            "Epoch 1700/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0794 - accuracy: 0.9810 - val_loss: 2.3252 - val_accuracy: 0.5027\n",
            "Epoch 1701/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0745 - accuracy: 0.9864 - val_loss: 2.2833 - val_accuracy: 0.5082\n",
            "Epoch 1702/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0763 - accuracy: 0.9864 - val_loss: 2.3376 - val_accuracy: 0.5137\n",
            "Epoch 1703/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0736 - accuracy: 0.9837 - val_loss: 2.3277 - val_accuracy: 0.4863\n",
            "Epoch 1704/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0696 - accuracy: 0.9878 - val_loss: 2.2906 - val_accuracy: 0.5082\n",
            "Epoch 1705/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0631 - accuracy: 0.9878 - val_loss: 2.2988 - val_accuracy: 0.5191\n",
            "Epoch 1706/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0642 - accuracy: 0.9891 - val_loss: 2.4802 - val_accuracy: 0.4973\n",
            "Epoch 1707/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0901 - accuracy: 0.9756 - val_loss: 2.2938 - val_accuracy: 0.5082\n",
            "Epoch 1708/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0734 - accuracy: 0.9878 - val_loss: 2.3008 - val_accuracy: 0.5082\n",
            "Epoch 1709/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0825 - accuracy: 0.9810 - val_loss: 2.2994 - val_accuracy: 0.5246\n",
            "Epoch 1710/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0721 - accuracy: 0.9878 - val_loss: 2.3073 - val_accuracy: 0.5027\n",
            "Epoch 1711/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0777 - accuracy: 0.9864 - val_loss: 2.3401 - val_accuracy: 0.5137\n",
            "Epoch 1712/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0848 - accuracy: 0.9796 - val_loss: 2.3416 - val_accuracy: 0.5027\n",
            "Epoch 1713/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0762 - accuracy: 0.9769 - val_loss: 2.3060 - val_accuracy: 0.5082\n",
            "Epoch 1714/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0672 - accuracy: 0.9891 - val_loss: 2.3139 - val_accuracy: 0.5082\n",
            "Epoch 1715/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0589 - accuracy: 0.9932 - val_loss: 2.3205 - val_accuracy: 0.5137\n",
            "Epoch 1716/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0891 - accuracy: 0.9729 - val_loss: 2.3472 - val_accuracy: 0.5246\n",
            "Epoch 1717/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0688 - accuracy: 0.9932 - val_loss: 2.3311 - val_accuracy: 0.5082\n",
            "Epoch 1718/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0785 - accuracy: 0.9810 - val_loss: 2.4261 - val_accuracy: 0.5082\n",
            "Epoch 1719/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0705 - accuracy: 0.9824 - val_loss: 2.3418 - val_accuracy: 0.4973\n",
            "Epoch 1720/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0712 - accuracy: 0.9878 - val_loss: 2.3851 - val_accuracy: 0.5082\n",
            "Epoch 1721/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0782 - accuracy: 0.9837 - val_loss: 2.3932 - val_accuracy: 0.5082\n",
            "Epoch 1722/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0674 - accuracy: 0.9946 - val_loss: 2.3503 - val_accuracy: 0.4973\n",
            "Epoch 1723/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0777 - accuracy: 0.9851 - val_loss: 2.3754 - val_accuracy: 0.5191\n",
            "Epoch 1724/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0691 - accuracy: 0.9891 - val_loss: 2.3199 - val_accuracy: 0.4973\n",
            "Epoch 1725/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0798 - accuracy: 0.9796 - val_loss: 2.3140 - val_accuracy: 0.5082\n",
            "Epoch 1726/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0802 - accuracy: 0.9837 - val_loss: 2.3315 - val_accuracy: 0.5191\n",
            "Epoch 1727/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0673 - accuracy: 0.9837 - val_loss: 2.3235 - val_accuracy: 0.4973\n",
            "Epoch 1728/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0677 - accuracy: 0.9919 - val_loss: 2.3774 - val_accuracy: 0.4918\n",
            "Epoch 1729/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0666 - accuracy: 0.9864 - val_loss: 2.3742 - val_accuracy: 0.5137\n",
            "Epoch 1730/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0606 - accuracy: 0.9932 - val_loss: 2.3167 - val_accuracy: 0.5082\n",
            "Epoch 1731/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0781 - accuracy: 0.9824 - val_loss: 2.5508 - val_accuracy: 0.5082\n",
            "Epoch 1732/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0771 - accuracy: 0.9864 - val_loss: 2.3113 - val_accuracy: 0.5082\n",
            "Epoch 1733/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0722 - accuracy: 0.9837 - val_loss: 2.3640 - val_accuracy: 0.5191\n",
            "Epoch 1734/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0738 - accuracy: 0.9864 - val_loss: 2.3340 - val_accuracy: 0.5027\n",
            "Epoch 1735/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0607 - accuracy: 0.9959 - val_loss: 2.3565 - val_accuracy: 0.5027\n",
            "Epoch 1736/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0785 - accuracy: 0.9810 - val_loss: 2.4266 - val_accuracy: 0.4973\n",
            "Epoch 1737/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0664 - accuracy: 0.9919 - val_loss: 2.3456 - val_accuracy: 0.5082\n",
            "Epoch 1738/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0652 - accuracy: 0.9878 - val_loss: 2.3917 - val_accuracy: 0.5191\n",
            "Epoch 1739/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0616 - accuracy: 0.9878 - val_loss: 2.3435 - val_accuracy: 0.5082\n",
            "Epoch 1740/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0676 - accuracy: 0.9891 - val_loss: 2.3521 - val_accuracy: 0.5027\n",
            "Epoch 1741/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0562 - accuracy: 0.9959 - val_loss: 2.4064 - val_accuracy: 0.5082\n",
            "Epoch 1742/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0624 - accuracy: 0.9905 - val_loss: 2.3842 - val_accuracy: 0.5082\n",
            "Epoch 1743/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0695 - accuracy: 0.9864 - val_loss: 2.3983 - val_accuracy: 0.5191\n",
            "Epoch 1744/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0677 - accuracy: 0.9905 - val_loss: 2.4292 - val_accuracy: 0.5191\n",
            "Epoch 1745/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0745 - accuracy: 0.9824 - val_loss: 2.3719 - val_accuracy: 0.5137\n",
            "Epoch 1746/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0667 - accuracy: 0.9878 - val_loss: 2.4052 - val_accuracy: 0.5082\n",
            "Epoch 1747/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.0701 - accuracy: 0.9891 - val_loss: 2.3416 - val_accuracy: 0.5082\n",
            "Epoch 1748/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0575 - accuracy: 0.9919 - val_loss: 2.3737 - val_accuracy: 0.5027\n",
            "Epoch 1749/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0751 - accuracy: 0.9837 - val_loss: 2.4383 - val_accuracy: 0.5027\n",
            "Epoch 1750/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0786 - accuracy: 0.9796 - val_loss: 2.4271 - val_accuracy: 0.4973\n",
            "Epoch 1751/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0743 - accuracy: 0.9851 - val_loss: 2.4097 - val_accuracy: 0.5027\n",
            "Epoch 1752/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0686 - accuracy: 0.9878 - val_loss: 2.3447 - val_accuracy: 0.4973\n",
            "Epoch 1753/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0730 - accuracy: 0.9905 - val_loss: 2.3963 - val_accuracy: 0.4863\n",
            "Epoch 1754/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0780 - accuracy: 0.9837 - val_loss: 2.4972 - val_accuracy: 0.5082\n",
            "Epoch 1755/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0677 - accuracy: 0.9878 - val_loss: 2.3948 - val_accuracy: 0.5082\n",
            "Epoch 1756/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0532 - accuracy: 0.9959 - val_loss: 2.4046 - val_accuracy: 0.5137\n",
            "Epoch 1757/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0495 - accuracy: 0.9959 - val_loss: 2.4092 - val_accuracy: 0.4973\n",
            "Epoch 1758/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0657 - accuracy: 0.9878 - val_loss: 2.5259 - val_accuracy: 0.5191\n",
            "Epoch 1759/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0750 - accuracy: 0.9878 - val_loss: 2.3785 - val_accuracy: 0.5191\n",
            "Epoch 1760/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0786 - accuracy: 0.9810 - val_loss: 2.3632 - val_accuracy: 0.5082\n",
            "Epoch 1761/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0627 - accuracy: 0.9905 - val_loss: 2.3688 - val_accuracy: 0.5027\n",
            "Epoch 1762/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0612 - accuracy: 0.9851 - val_loss: 2.3380 - val_accuracy: 0.5082\n",
            "Epoch 1763/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0710 - accuracy: 0.9864 - val_loss: 2.3994 - val_accuracy: 0.5137\n",
            "Epoch 1764/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0597 - accuracy: 0.9932 - val_loss: 2.3781 - val_accuracy: 0.5082\n",
            "Epoch 1765/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0674 - accuracy: 0.9810 - val_loss: 2.3896 - val_accuracy: 0.5137\n",
            "Epoch 1766/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0543 - accuracy: 0.9932 - val_loss: 2.4058 - val_accuracy: 0.5137\n",
            "Epoch 1767/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0577 - accuracy: 0.9891 - val_loss: 2.4893 - val_accuracy: 0.4973\n",
            "Epoch 1768/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0689 - accuracy: 0.9878 - val_loss: 2.4155 - val_accuracy: 0.5137\n",
            "Epoch 1769/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0643 - accuracy: 0.9905 - val_loss: 2.4377 - val_accuracy: 0.5137\n",
            "Epoch 1770/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0675 - accuracy: 0.9905 - val_loss: 2.3670 - val_accuracy: 0.5082\n",
            "Epoch 1771/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0561 - accuracy: 0.9919 - val_loss: 2.4215 - val_accuracy: 0.5137\n",
            "Epoch 1772/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0572 - accuracy: 0.9919 - val_loss: 2.3723 - val_accuracy: 0.5137\n",
            "Epoch 1773/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0644 - accuracy: 0.9864 - val_loss: 2.4492 - val_accuracy: 0.5027\n",
            "Epoch 1774/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0608 - accuracy: 0.9891 - val_loss: 2.3788 - val_accuracy: 0.5082\n",
            "Epoch 1775/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0757 - accuracy: 0.9824 - val_loss: 2.4334 - val_accuracy: 0.5191\n",
            "Epoch 1776/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0653 - accuracy: 0.9891 - val_loss: 2.4217 - val_accuracy: 0.5082\n",
            "Epoch 1777/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0596 - accuracy: 0.9891 - val_loss: 2.3792 - val_accuracy: 0.4973\n",
            "Epoch 1778/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0700 - accuracy: 0.9837 - val_loss: 2.3389 - val_accuracy: 0.5191\n",
            "Epoch 1779/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0577 - accuracy: 0.9919 - val_loss: 2.3820 - val_accuracy: 0.5191\n",
            "Epoch 1780/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0539 - accuracy: 0.9891 - val_loss: 2.3921 - val_accuracy: 0.4973\n",
            "Epoch 1781/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0647 - accuracy: 0.9878 - val_loss: 2.4028 - val_accuracy: 0.5027\n",
            "Epoch 1782/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0653 - accuracy: 0.9851 - val_loss: 2.4357 - val_accuracy: 0.5082\n",
            "Epoch 1783/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0649 - accuracy: 0.9864 - val_loss: 2.4503 - val_accuracy: 0.4973\n",
            "Epoch 1784/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0549 - accuracy: 0.9905 - val_loss: 2.3883 - val_accuracy: 0.5027\n",
            "Epoch 1785/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0772 - accuracy: 0.9783 - val_loss: 2.4779 - val_accuracy: 0.5191\n",
            "Epoch 1786/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0515 - accuracy: 0.9905 - val_loss: 2.4345 - val_accuracy: 0.5191\n",
            "Epoch 1787/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0684 - accuracy: 0.9851 - val_loss: 2.4268 - val_accuracy: 0.5027\n",
            "Epoch 1788/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0592 - accuracy: 0.9837 - val_loss: 2.4060 - val_accuracy: 0.5082\n",
            "Epoch 1789/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0673 - accuracy: 0.9837 - val_loss: 2.4100 - val_accuracy: 0.5082\n",
            "Epoch 1790/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0523 - accuracy: 0.9878 - val_loss: 2.4115 - val_accuracy: 0.5246\n",
            "Epoch 1791/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0569 - accuracy: 0.9891 - val_loss: 2.4406 - val_accuracy: 0.4863\n",
            "Epoch 1792/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0568 - accuracy: 0.9946 - val_loss: 2.4505 - val_accuracy: 0.5137\n",
            "Epoch 1793/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0617 - accuracy: 0.9864 - val_loss: 2.4461 - val_accuracy: 0.5137\n",
            "Epoch 1794/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0639 - accuracy: 0.9864 - val_loss: 2.4300 - val_accuracy: 0.5191\n",
            "Epoch 1795/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0557 - accuracy: 0.9919 - val_loss: 2.4340 - val_accuracy: 0.5191\n",
            "Epoch 1796/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0655 - accuracy: 0.9864 - val_loss: 2.4246 - val_accuracy: 0.5137\n",
            "Epoch 1797/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0557 - accuracy: 0.9946 - val_loss: 2.4117 - val_accuracy: 0.5027\n",
            "Epoch 1798/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0563 - accuracy: 0.9932 - val_loss: 2.4121 - val_accuracy: 0.5246\n",
            "Epoch 1799/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0500 - accuracy: 0.9905 - val_loss: 2.4274 - val_accuracy: 0.5191\n",
            "Epoch 1800/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0612 - accuracy: 0.9878 - val_loss: 2.4470 - val_accuracy: 0.5191\n",
            "Epoch 1801/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0547 - accuracy: 0.9905 - val_loss: 2.4150 - val_accuracy: 0.5082\n",
            "Epoch 1802/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0537 - accuracy: 0.9905 - val_loss: 2.4340 - val_accuracy: 0.5137\n",
            "Epoch 1803/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0522 - accuracy: 0.9932 - val_loss: 2.4300 - val_accuracy: 0.5137\n",
            "Epoch 1804/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0693 - accuracy: 0.9837 - val_loss: 2.4187 - val_accuracy: 0.5137\n",
            "Epoch 1805/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0546 - accuracy: 0.9878 - val_loss: 2.5012 - val_accuracy: 0.5191\n",
            "Epoch 1806/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0585 - accuracy: 0.9878 - val_loss: 2.4587 - val_accuracy: 0.5082\n",
            "Epoch 1807/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0570 - accuracy: 0.9905 - val_loss: 2.4547 - val_accuracy: 0.5082\n",
            "Epoch 1808/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0603 - accuracy: 0.9932 - val_loss: 2.4567 - val_accuracy: 0.5137\n",
            "Epoch 1809/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0549 - accuracy: 0.9932 - val_loss: 2.4382 - val_accuracy: 0.5137\n",
            "Epoch 1810/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0697 - accuracy: 0.9824 - val_loss: 2.4834 - val_accuracy: 0.4973\n",
            "Epoch 1811/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0553 - accuracy: 0.9905 - val_loss: 2.4851 - val_accuracy: 0.5137\n",
            "Epoch 1812/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0573 - accuracy: 0.9878 - val_loss: 2.4167 - val_accuracy: 0.5027\n",
            "Epoch 1813/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0586 - accuracy: 0.9878 - val_loss: 2.4540 - val_accuracy: 0.4918\n",
            "Epoch 1814/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0526 - accuracy: 0.9905 - val_loss: 2.4564 - val_accuracy: 0.4973\n",
            "Epoch 1815/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0599 - accuracy: 0.9851 - val_loss: 2.4451 - val_accuracy: 0.5137\n",
            "Epoch 1816/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0456 - accuracy: 0.9959 - val_loss: 2.5048 - val_accuracy: 0.5191\n",
            "Epoch 1817/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0499 - accuracy: 0.9959 - val_loss: 2.5884 - val_accuracy: 0.4973\n",
            "Epoch 1818/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0561 - accuracy: 0.9919 - val_loss: 2.4916 - val_accuracy: 0.5027\n",
            "Epoch 1819/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0490 - accuracy: 0.9932 - val_loss: 2.4485 - val_accuracy: 0.4918\n",
            "Epoch 1820/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0445 - accuracy: 0.9946 - val_loss: 2.5277 - val_accuracy: 0.4973\n",
            "Epoch 1821/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0515 - accuracy: 0.9891 - val_loss: 2.4783 - val_accuracy: 0.4973\n",
            "Epoch 1822/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0618 - accuracy: 0.9864 - val_loss: 2.4948 - val_accuracy: 0.4918\n",
            "Epoch 1823/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0613 - accuracy: 0.9864 - val_loss: 2.4504 - val_accuracy: 0.5137\n",
            "Epoch 1824/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0668 - accuracy: 0.9837 - val_loss: 2.4197 - val_accuracy: 0.5137\n",
            "Epoch 1825/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0523 - accuracy: 0.9946 - val_loss: 2.4490 - val_accuracy: 0.4973\n",
            "Epoch 1826/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0533 - accuracy: 0.9891 - val_loss: 2.4850 - val_accuracy: 0.5191\n",
            "Epoch 1827/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0509 - accuracy: 0.9919 - val_loss: 2.4947 - val_accuracy: 0.5027\n",
            "Epoch 1828/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0626 - accuracy: 0.9851 - val_loss: 2.4770 - val_accuracy: 0.5027\n",
            "Epoch 1829/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0421 - accuracy: 0.9932 - val_loss: 2.6502 - val_accuracy: 0.4699\n",
            "Epoch 1830/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0603 - accuracy: 0.9919 - val_loss: 2.4771 - val_accuracy: 0.5137\n",
            "Epoch 1831/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0647 - accuracy: 0.9851 - val_loss: 2.4540 - val_accuracy: 0.5191\n",
            "Epoch 1832/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0655 - accuracy: 0.9837 - val_loss: 2.4646 - val_accuracy: 0.5191\n",
            "Epoch 1833/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0531 - accuracy: 0.9878 - val_loss: 2.5011 - val_accuracy: 0.5027\n",
            "Epoch 1834/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0499 - accuracy: 0.9919 - val_loss: 2.5275 - val_accuracy: 0.5027\n",
            "Epoch 1835/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0517 - accuracy: 0.9932 - val_loss: 2.4289 - val_accuracy: 0.5137\n",
            "Epoch 1836/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0491 - accuracy: 0.9946 - val_loss: 2.4491 - val_accuracy: 0.4973\n",
            "Epoch 1837/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0576 - accuracy: 0.9891 - val_loss: 2.4610 - val_accuracy: 0.5137\n",
            "Epoch 1838/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0551 - accuracy: 0.9932 - val_loss: 2.4624 - val_accuracy: 0.4863\n",
            "Epoch 1839/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0542 - accuracy: 0.9878 - val_loss: 2.4741 - val_accuracy: 0.4973\n",
            "Epoch 1840/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0497 - accuracy: 0.9905 - val_loss: 2.4702 - val_accuracy: 0.5027\n",
            "Epoch 1841/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0477 - accuracy: 0.9946 - val_loss: 2.4867 - val_accuracy: 0.5191\n",
            "Epoch 1842/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0559 - accuracy: 0.9919 - val_loss: 2.5190 - val_accuracy: 0.5137\n",
            "Epoch 1843/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0530 - accuracy: 0.9864 - val_loss: 2.5621 - val_accuracy: 0.5137\n",
            "Epoch 1844/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0490 - accuracy: 0.9905 - val_loss: 2.5358 - val_accuracy: 0.5082\n",
            "Epoch 1845/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0459 - accuracy: 0.9946 - val_loss: 2.6420 - val_accuracy: 0.4973\n",
            "Epoch 1846/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0674 - accuracy: 0.9878 - val_loss: 2.4922 - val_accuracy: 0.5082\n",
            "Epoch 1847/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0677 - accuracy: 0.9810 - val_loss: 2.4743 - val_accuracy: 0.5137\n",
            "Epoch 1848/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0503 - accuracy: 0.9919 - val_loss: 2.4921 - val_accuracy: 0.5191\n",
            "Epoch 1849/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0474 - accuracy: 0.9905 - val_loss: 2.4678 - val_accuracy: 0.5137\n",
            "Epoch 1850/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0474 - accuracy: 0.9919 - val_loss: 2.4565 - val_accuracy: 0.5137\n",
            "Epoch 1851/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0509 - accuracy: 0.9878 - val_loss: 2.4914 - val_accuracy: 0.5301\n",
            "Epoch 1852/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0463 - accuracy: 0.9932 - val_loss: 2.5147 - val_accuracy: 0.5191\n",
            "Epoch 1853/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0605 - accuracy: 0.9851 - val_loss: 2.4884 - val_accuracy: 0.5191\n",
            "Epoch 1854/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0464 - accuracy: 0.9932 - val_loss: 2.5367 - val_accuracy: 0.4973\n",
            "Epoch 1855/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0492 - accuracy: 0.9919 - val_loss: 2.5177 - val_accuracy: 0.4973\n",
            "Epoch 1856/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0481 - accuracy: 0.9891 - val_loss: 2.5004 - val_accuracy: 0.4863\n",
            "Epoch 1857/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0536 - accuracy: 0.9905 - val_loss: 2.5584 - val_accuracy: 0.5191\n",
            "Epoch 1858/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0436 - accuracy: 0.9919 - val_loss: 2.4991 - val_accuracy: 0.5027\n",
            "Epoch 1859/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0435 - accuracy: 0.9959 - val_loss: 2.5632 - val_accuracy: 0.4973\n",
            "Epoch 1860/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0544 - accuracy: 0.9851 - val_loss: 2.6151 - val_accuracy: 0.4918\n",
            "Epoch 1861/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0487 - accuracy: 0.9932 - val_loss: 2.5019 - val_accuracy: 0.5191\n",
            "Epoch 1862/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0420 - accuracy: 0.9919 - val_loss: 2.5334 - val_accuracy: 0.5191\n",
            "Epoch 1863/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0444 - accuracy: 0.9891 - val_loss: 2.5361 - val_accuracy: 0.5191\n",
            "Epoch 1864/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0412 - accuracy: 0.9946 - val_loss: 2.5560 - val_accuracy: 0.4918\n",
            "Epoch 1865/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0505 - accuracy: 0.9905 - val_loss: 2.4689 - val_accuracy: 0.5246\n",
            "Epoch 1866/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0408 - accuracy: 0.9973 - val_loss: 2.5157 - val_accuracy: 0.5246\n",
            "Epoch 1867/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0442 - accuracy: 0.9946 - val_loss: 2.5925 - val_accuracy: 0.5137\n",
            "Epoch 1868/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0442 - accuracy: 0.9919 - val_loss: 2.5243 - val_accuracy: 0.5191\n",
            "Epoch 1869/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0508 - accuracy: 0.9891 - val_loss: 2.5905 - val_accuracy: 0.5027\n",
            "Epoch 1870/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0479 - accuracy: 0.9919 - val_loss: 2.5822 - val_accuracy: 0.5027\n",
            "Epoch 1871/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0424 - accuracy: 0.9932 - val_loss: 2.5250 - val_accuracy: 0.5191\n",
            "Epoch 1872/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0489 - accuracy: 0.9919 - val_loss: 2.5779 - val_accuracy: 0.5027\n",
            "Epoch 1873/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0493 - accuracy: 0.9905 - val_loss: 2.5376 - val_accuracy: 0.5082\n",
            "Epoch 1874/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0478 - accuracy: 0.9919 - val_loss: 2.5681 - val_accuracy: 0.5027\n",
            "Epoch 1875/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0471 - accuracy: 0.9878 - val_loss: 2.5043 - val_accuracy: 0.5246\n",
            "Epoch 1876/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0412 - accuracy: 0.9946 - val_loss: 2.5428 - val_accuracy: 0.5191\n",
            "Epoch 1877/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0469 - accuracy: 0.9919 - val_loss: 2.6676 - val_accuracy: 0.4973\n",
            "Epoch 1878/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0456 - accuracy: 0.9932 - val_loss: 2.5616 - val_accuracy: 0.5191\n",
            "Epoch 1879/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0485 - accuracy: 0.9905 - val_loss: 2.5648 - val_accuracy: 0.5027\n",
            "Epoch 1880/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0419 - accuracy: 0.9946 - val_loss: 2.5623 - val_accuracy: 0.5027\n",
            "Epoch 1881/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0501 - accuracy: 0.9878 - val_loss: 2.5615 - val_accuracy: 0.5137\n",
            "Epoch 1882/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0397 - accuracy: 0.9959 - val_loss: 2.5744 - val_accuracy: 0.5191\n",
            "Epoch 1883/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0408 - accuracy: 0.9959 - val_loss: 2.5467 - val_accuracy: 0.5137\n",
            "Epoch 1884/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0434 - accuracy: 0.9905 - val_loss: 2.6121 - val_accuracy: 0.5191\n",
            "Epoch 1885/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0491 - accuracy: 0.9864 - val_loss: 2.5999 - val_accuracy: 0.5027\n",
            "Epoch 1886/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0527 - accuracy: 0.9905 - val_loss: 2.5396 - val_accuracy: 0.5137\n",
            "Epoch 1887/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0393 - accuracy: 0.9932 - val_loss: 2.6225 - val_accuracy: 0.5082\n",
            "Epoch 1888/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0500 - accuracy: 0.9905 - val_loss: 2.6179 - val_accuracy: 0.5082\n",
            "Epoch 1889/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0482 - accuracy: 0.9878 - val_loss: 2.5392 - val_accuracy: 0.5082\n",
            "Epoch 1890/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0544 - accuracy: 0.9864 - val_loss: 2.5642 - val_accuracy: 0.5191\n",
            "Epoch 1891/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0414 - accuracy: 0.9973 - val_loss: 2.5623 - val_accuracy: 0.4973\n",
            "Epoch 1892/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0472 - accuracy: 0.9932 - val_loss: 2.5602 - val_accuracy: 0.5191\n",
            "Epoch 1893/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0477 - accuracy: 0.9891 - val_loss: 2.5581 - val_accuracy: 0.5082\n",
            "Epoch 1894/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0459 - accuracy: 0.9932 - val_loss: 2.5921 - val_accuracy: 0.5191\n",
            "Epoch 1895/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0383 - accuracy: 0.9946 - val_loss: 2.5561 - val_accuracy: 0.5137\n",
            "Epoch 1896/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0515 - accuracy: 0.9905 - val_loss: 2.6280 - val_accuracy: 0.5137\n",
            "Epoch 1897/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0397 - accuracy: 0.9959 - val_loss: 2.5683 - val_accuracy: 0.5027\n",
            "Epoch 1898/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0480 - accuracy: 0.9878 - val_loss: 2.6029 - val_accuracy: 0.4918\n",
            "Epoch 1899/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0406 - accuracy: 0.9946 - val_loss: 2.5673 - val_accuracy: 0.5137\n",
            "Epoch 1900/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0358 - accuracy: 0.9973 - val_loss: 2.5964 - val_accuracy: 0.5246\n",
            "Epoch 1901/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0432 - accuracy: 0.9946 - val_loss: 2.6343 - val_accuracy: 0.5027\n",
            "Epoch 1902/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0441 - accuracy: 0.9905 - val_loss: 2.5854 - val_accuracy: 0.5137\n",
            "Epoch 1903/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0451 - accuracy: 0.9891 - val_loss: 2.5906 - val_accuracy: 0.5137\n",
            "Epoch 1904/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0349 - accuracy: 0.9959 - val_loss: 2.6259 - val_accuracy: 0.5082\n",
            "Epoch 1905/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0369 - accuracy: 0.9946 - val_loss: 2.6356 - val_accuracy: 0.5027\n",
            "Epoch 1906/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0421 - accuracy: 0.9946 - val_loss: 2.5687 - val_accuracy: 0.4973\n",
            "Epoch 1907/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0431 - accuracy: 0.9919 - val_loss: 2.6112 - val_accuracy: 0.5082\n",
            "Epoch 1908/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0595 - accuracy: 0.9878 - val_loss: 2.5490 - val_accuracy: 0.5137\n",
            "Epoch 1909/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0470 - accuracy: 0.9878 - val_loss: 2.6221 - val_accuracy: 0.4973\n",
            "Epoch 1910/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0408 - accuracy: 0.9986 - val_loss: 2.6099 - val_accuracy: 0.4918\n",
            "Epoch 1911/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0540 - accuracy: 0.9905 - val_loss: 2.5945 - val_accuracy: 0.5027\n",
            "Epoch 1912/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0375 - accuracy: 0.9946 - val_loss: 2.5826 - val_accuracy: 0.5027\n",
            "Epoch 1913/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0507 - accuracy: 0.9932 - val_loss: 2.6025 - val_accuracy: 0.5191\n",
            "Epoch 1914/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0410 - accuracy: 0.9946 - val_loss: 2.5949 - val_accuracy: 0.5082\n",
            "Epoch 1915/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0319 - accuracy: 0.9986 - val_loss: 2.6384 - val_accuracy: 0.5137\n",
            "Epoch 1916/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0419 - accuracy: 0.9919 - val_loss: 2.5573 - val_accuracy: 0.4973\n",
            "Epoch 1917/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0445 - accuracy: 0.9891 - val_loss: 2.5647 - val_accuracy: 0.5137\n",
            "Epoch 1918/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0401 - accuracy: 0.9932 - val_loss: 2.6317 - val_accuracy: 0.5246\n",
            "Epoch 1919/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0365 - accuracy: 0.9932 - val_loss: 2.5718 - val_accuracy: 0.5246\n",
            "Epoch 1920/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0364 - accuracy: 0.9973 - val_loss: 2.6170 - val_accuracy: 0.5082\n",
            "Epoch 1921/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0411 - accuracy: 0.9919 - val_loss: 2.6500 - val_accuracy: 0.5082\n",
            "Epoch 1922/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0579 - accuracy: 0.9851 - val_loss: 2.5906 - val_accuracy: 0.5191\n",
            "Epoch 1923/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0347 - accuracy: 0.9959 - val_loss: 2.6770 - val_accuracy: 0.5137\n",
            "Epoch 1924/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0432 - accuracy: 0.9946 - val_loss: 2.6471 - val_accuracy: 0.4973\n",
            "Epoch 1925/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0383 - accuracy: 0.9932 - val_loss: 2.5930 - val_accuracy: 0.5137\n",
            "Epoch 1926/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0397 - accuracy: 0.9946 - val_loss: 2.6396 - val_accuracy: 0.5082\n",
            "Epoch 1927/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0346 - accuracy: 0.9946 - val_loss: 2.6126 - val_accuracy: 0.5082\n",
            "Epoch 1928/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0438 - accuracy: 0.9891 - val_loss: 2.6243 - val_accuracy: 0.5027\n",
            "Epoch 1929/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0416 - accuracy: 0.9946 - val_loss: 2.6584 - val_accuracy: 0.5246\n",
            "Epoch 1930/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0450 - accuracy: 0.9946 - val_loss: 2.7193 - val_accuracy: 0.5082\n",
            "Epoch 1931/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9932 - val_loss: 2.6176 - val_accuracy: 0.5246\n",
            "Epoch 1932/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0386 - accuracy: 0.9919 - val_loss: 2.6495 - val_accuracy: 0.5082\n",
            "Epoch 1933/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0402 - accuracy: 0.9946 - val_loss: 2.6346 - val_accuracy: 0.5191\n",
            "Epoch 1934/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0365 - accuracy: 0.9932 - val_loss: 2.6753 - val_accuracy: 0.4973\n",
            "Epoch 1935/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0422 - accuracy: 0.9932 - val_loss: 2.6250 - val_accuracy: 0.5137\n",
            "Epoch 1936/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0413 - accuracy: 0.9919 - val_loss: 2.6009 - val_accuracy: 0.5191\n",
            "Epoch 1937/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0415 - accuracy: 0.9905 - val_loss: 2.8187 - val_accuracy: 0.5027\n",
            "Epoch 1938/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0475 - accuracy: 0.9891 - val_loss: 2.6244 - val_accuracy: 0.5082\n",
            "Epoch 1939/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0437 - accuracy: 0.9919 - val_loss: 2.6477 - val_accuracy: 0.5027\n",
            "Epoch 1940/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0377 - accuracy: 0.9946 - val_loss: 2.5982 - val_accuracy: 0.5082\n",
            "Epoch 1941/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0424 - accuracy: 0.9959 - val_loss: 2.6496 - val_accuracy: 0.4918\n",
            "Epoch 1942/2018\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0372 - accuracy: 0.9946 - val_loss: 2.6443 - val_accuracy: 0.5082\n",
            "Epoch 1943/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0402 - accuracy: 0.9919 - val_loss: 2.8661 - val_accuracy: 0.4973\n",
            "Epoch 1944/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0421 - accuracy: 0.9891 - val_loss: 2.6630 - val_accuracy: 0.4973\n",
            "Epoch 1945/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0338 - accuracy: 0.9946 - val_loss: 2.7333 - val_accuracy: 0.5137\n",
            "Epoch 1946/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0391 - accuracy: 0.9919 - val_loss: 2.6263 - val_accuracy: 0.5355\n",
            "Epoch 1947/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.0354 - accuracy: 0.9959 - val_loss: 2.7106 - val_accuracy: 0.5137\n",
            "Epoch 1948/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0436 - accuracy: 0.9932 - val_loss: 2.7080 - val_accuracy: 0.4973\n",
            "Epoch 1949/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0303 - accuracy: 0.9973 - val_loss: 2.7027 - val_accuracy: 0.5137\n",
            "Epoch 1950/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0425 - accuracy: 0.9932 - val_loss: 2.6365 - val_accuracy: 0.5191\n",
            "Epoch 1951/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0441 - accuracy: 0.9919 - val_loss: 2.6124 - val_accuracy: 0.5355\n",
            "Epoch 1952/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0360 - accuracy: 0.9946 - val_loss: 2.7401 - val_accuracy: 0.5246\n",
            "Epoch 1953/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0279 - accuracy: 0.9986 - val_loss: 2.6594 - val_accuracy: 0.5027\n",
            "Epoch 1954/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0379 - accuracy: 0.9919 - val_loss: 2.6623 - val_accuracy: 0.5027\n",
            "Epoch 1955/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0386 - accuracy: 0.9946 - val_loss: 2.6391 - val_accuracy: 0.4973\n",
            "Epoch 1956/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0354 - accuracy: 0.9946 - val_loss: 2.7204 - val_accuracy: 0.5082\n",
            "Epoch 1957/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0426 - accuracy: 0.9919 - val_loss: 2.6657 - val_accuracy: 0.5191\n",
            "Epoch 1958/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0370 - accuracy: 0.9932 - val_loss: 2.6409 - val_accuracy: 0.5191\n",
            "Epoch 1959/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0363 - accuracy: 0.9919 - val_loss: 2.7808 - val_accuracy: 0.5027\n",
            "Epoch 1960/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0309 - accuracy: 0.9973 - val_loss: 2.7200 - val_accuracy: 0.5027\n",
            "Epoch 1961/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0382 - accuracy: 0.9919 - val_loss: 2.7270 - val_accuracy: 0.5137\n",
            "Epoch 1962/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0393 - accuracy: 0.9932 - val_loss: 2.7317 - val_accuracy: 0.5137\n",
            "Epoch 1963/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0312 - accuracy: 0.9959 - val_loss: 2.6936 - val_accuracy: 0.5301\n",
            "Epoch 1964/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0474 - accuracy: 0.9891 - val_loss: 2.6562 - val_accuracy: 0.5191\n",
            "Epoch 1965/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0313 - accuracy: 0.9973 - val_loss: 2.6637 - val_accuracy: 0.5027\n",
            "Epoch 1966/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0459 - accuracy: 0.9919 - val_loss: 2.6712 - val_accuracy: 0.5137\n",
            "Epoch 1967/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0368 - accuracy: 0.9946 - val_loss: 2.7288 - val_accuracy: 0.4918\n",
            "Epoch 1968/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0356 - accuracy: 0.9959 - val_loss: 2.7383 - val_accuracy: 0.5137\n",
            "Epoch 1969/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0361 - accuracy: 0.9959 - val_loss: 2.7451 - val_accuracy: 0.5191\n",
            "Epoch 1970/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0432 - accuracy: 0.9905 - val_loss: 2.6510 - val_accuracy: 0.5246\n",
            "Epoch 1971/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0408 - accuracy: 0.9878 - val_loss: 2.6478 - val_accuracy: 0.5301\n",
            "Epoch 1972/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0340 - accuracy: 0.9973 - val_loss: 2.6679 - val_accuracy: 0.5027\n",
            "Epoch 1973/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0408 - accuracy: 0.9919 - val_loss: 2.6945 - val_accuracy: 0.5082\n",
            "Epoch 1974/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0333 - accuracy: 0.9932 - val_loss: 2.7456 - val_accuracy: 0.5191\n",
            "Epoch 1975/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0384 - accuracy: 0.9919 - val_loss: 2.6735 - val_accuracy: 0.5082\n",
            "Epoch 1976/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0267 - accuracy: 0.9986 - val_loss: 2.6705 - val_accuracy: 0.5191\n",
            "Epoch 1977/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0350 - accuracy: 0.9946 - val_loss: 2.6791 - val_accuracy: 0.5191\n",
            "Epoch 1978/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0418 - accuracy: 0.9946 - val_loss: 2.6667 - val_accuracy: 0.5191\n",
            "Epoch 1979/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0394 - accuracy: 0.9932 - val_loss: 2.7402 - val_accuracy: 0.5191\n",
            "Epoch 1980/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.9919 - val_loss: 2.6696 - val_accuracy: 0.5301\n",
            "Epoch 1981/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0352 - accuracy: 0.9932 - val_loss: 2.6715 - val_accuracy: 0.5191\n",
            "Epoch 1982/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0481 - accuracy: 0.9864 - val_loss: 2.6688 - val_accuracy: 0.5191\n",
            "Epoch 1983/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0359 - accuracy: 0.9946 - val_loss: 2.6692 - val_accuracy: 0.5355\n",
            "Epoch 1984/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0314 - accuracy: 0.9959 - val_loss: 2.6890 - val_accuracy: 0.5137\n",
            "Epoch 1985/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0443 - accuracy: 0.9891 - val_loss: 2.6628 - val_accuracy: 0.5137\n",
            "Epoch 1986/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0298 - accuracy: 0.9959 - val_loss: 2.7441 - val_accuracy: 0.5137\n",
            "Epoch 1987/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0400 - accuracy: 0.9932 - val_loss: 2.6899 - val_accuracy: 0.5137\n",
            "Epoch 1988/2018\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0356 - accuracy: 0.9959 - val_loss: 2.7222 - val_accuracy: 0.5137\n",
            "Epoch 1989/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0361 - accuracy: 0.9919 - val_loss: 2.7524 - val_accuracy: 0.4973\n",
            "Epoch 1990/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0319 - accuracy: 0.9959 - val_loss: 2.7244 - val_accuracy: 0.4973\n",
            "Epoch 1991/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0326 - accuracy: 0.9959 - val_loss: 2.6791 - val_accuracy: 0.5137\n",
            "Epoch 1992/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0357 - accuracy: 0.9919 - val_loss: 2.7967 - val_accuracy: 0.5082\n",
            "Epoch 1993/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0342 - accuracy: 0.9905 - val_loss: 2.6997 - val_accuracy: 0.5191\n",
            "Epoch 1994/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0345 - accuracy: 0.9946 - val_loss: 2.7742 - val_accuracy: 0.5082\n",
            "Epoch 1995/2018\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.0305 - accuracy: 0.9973 - val_loss: 2.7074 - val_accuracy: 0.5027\n",
            "Epoch 1996/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0314 - accuracy: 0.9959 - val_loss: 2.7460 - val_accuracy: 0.5191\n",
            "Epoch 1997/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.0323 - accuracy: 0.9946 - val_loss: 2.7234 - val_accuracy: 0.5191\n",
            "Epoch 1998/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0311 - accuracy: 0.9946 - val_loss: 2.6745 - val_accuracy: 0.5191\n",
            "Epoch 1999/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0353 - accuracy: 0.9932 - val_loss: 2.7103 - val_accuracy: 0.5246\n",
            "Epoch 2000/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0252 - accuracy: 0.9986 - val_loss: 2.7111 - val_accuracy: 0.5137\n",
            "Epoch 2001/2018\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.0362 - accuracy: 0.9946 - val_loss: 2.7240 - val_accuracy: 0.5246\n",
            "Epoch 2002/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0235 - accuracy: 0.9973 - val_loss: 2.7678 - val_accuracy: 0.5191\n",
            "Epoch 2003/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0327 - accuracy: 0.9973 - val_loss: 2.8120 - val_accuracy: 0.5137\n",
            "Epoch 2004/2018\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0375 - accuracy: 0.9946 - val_loss: 2.8491 - val_accuracy: 0.5082\n",
            "Epoch 2005/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0376 - accuracy: 0.9891 - val_loss: 2.7268 - val_accuracy: 0.5082\n",
            "Epoch 2006/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0281 - accuracy: 0.9973 - val_loss: 2.8060 - val_accuracy: 0.5082\n",
            "Epoch 2007/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0303 - accuracy: 0.9919 - val_loss: 2.7212 - val_accuracy: 0.5301\n",
            "Epoch 2008/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0357 - accuracy: 0.9946 - val_loss: 2.7112 - val_accuracy: 0.5246\n",
            "Epoch 2009/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0311 - accuracy: 0.9932 - val_loss: 2.7297 - val_accuracy: 0.5082\n",
            "Epoch 2010/2018\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.0286 - accuracy: 0.9959 - val_loss: 2.7741 - val_accuracy: 0.5246\n",
            "Epoch 2011/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0321 - accuracy: 0.9946 - val_loss: 2.7719 - val_accuracy: 0.5027\n",
            "Epoch 2012/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0314 - accuracy: 0.9959 - val_loss: 2.7453 - val_accuracy: 0.5191\n",
            "Epoch 2013/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0399 - accuracy: 0.9932 - val_loss: 2.7520 - val_accuracy: 0.5027\n",
            "Epoch 2014/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0323 - accuracy: 0.9919 - val_loss: 2.7215 - val_accuracy: 0.5082\n",
            "Epoch 2015/2018\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0324 - accuracy: 0.9932 - val_loss: 2.7107 - val_accuracy: 0.5246\n",
            "Epoch 2016/2018\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0293 - accuracy: 0.9959 - val_loss: 3.0328 - val_accuracy: 0.4809\n",
            "Epoch 2017/2018\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0369 - accuracy: 0.9919 - val_loss: 2.6969 - val_accuracy: 0.5191\n",
            "Epoch 2018/2018\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0329 - accuracy: 0.9905 - val_loss: 2.7506 - val_accuracy: 0.5027\n"
          ]
        }
      ],
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=64, epochs=2018, validation_data=(x_testcnn, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uzzUPv_iQQD"
      },
      "source": [
        "## Plotting the accuracy and loss graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "pzyYy5DqiSD6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "4d773698-3fc9-452c-dd30-386161e67da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [2.450349807739258, 2.3376247882843018, 2.325579881668091, 2.3036417961120605, 2.285687208175659, 2.2611961364746094, 2.2628014087677, 2.2319328784942627, 2.231583595275879, 2.2216086387634277, 2.2262487411499023, 2.21321702003479, 2.2158079147338867, 2.196388006210327, 2.177358865737915, 2.1733415126800537, 2.1554527282714844, 2.1735641956329346, 2.1501097679138184, 2.1388936042785645, 2.1266391277313232, 2.1209311485290527, 2.111388683319092, 2.1231441497802734, 2.1185925006866455, 2.086848497390747, 2.090898275375366, 2.0983645915985107, 2.086467742919922, 2.0546905994415283, 2.041877508163452, 2.0656790733337402, 2.052549362182617, 2.0428144931793213, 2.029296636581421, 2.0308330059051514, 2.0156514644622803, 2.0059850215911865, 2.0091989040374756, 1.9992153644561768, 1.9791933298110962, 1.9815682172775269, 1.974239706993103, 1.9745886325836182, 1.9723435640335083, 1.9752949476242065, 1.9577374458312988, 1.9678176641464233, 1.9420381784439087, 1.9410544633865356, 1.930316686630249, 1.9140743017196655, 1.932024598121643, 1.9241446256637573, 1.8842952251434326, 1.889283537864685, 1.8986726999282837, 1.9154945611953735, 1.8770058155059814, 1.8988463878631592, 1.8823943138122559, 1.8800626993179321, 1.842844843864441, 1.8529435396194458, 1.8364146947860718, 1.8695290088653564, 1.8616479635238647, 1.8236526250839233, 1.8128398656845093, 1.8309181928634644, 1.8216004371643066, 1.7908225059509277, 1.8131585121154785, 1.8117684125900269, 1.8248951435089111, 1.7953001260757446, 1.7989604473114014, 1.800389051437378, 1.7845534086227417, 1.7870591878890991, 1.7871063947677612, 1.7849503755569458, 1.7989578247070312, 1.7861782312393188, 1.7496776580810547, 1.7592568397521973, 1.7868098020553589, 1.771216869354248, 1.7713379859924316, 1.7495759725570679, 1.7615966796875, 1.7229841947555542, 1.7544080018997192, 1.7471520900726318, 1.731377363204956, 1.7201998233795166, 1.732409954071045, 1.7269831895828247, 1.7289353609085083, 1.7339569330215454, 1.711668610572815, 1.7252026796340942, 1.6910642385482788, 1.6959410905838013, 1.6990846395492554, 1.6949880123138428, 1.698284387588501, 1.6913104057312012, 1.6784873008728027, 1.7006117105484009, 1.6681698560714722, 1.6818304061889648, 1.689566969871521, 1.6603988409042358, 1.6738948822021484, 1.6565545797348022, 1.6713612079620361, 1.6537525653839111, 1.6829197406768799, 1.642826795578003, 1.659200668334961, 1.645082950592041, 1.6669633388519287, 1.6466528177261353, 1.6307964324951172, 1.6548690795898438, 1.6455551385879517, 1.6322919130325317, 1.6490488052368164, 1.6280971765518188, 1.6291500329971313, 1.6432368755340576, 1.6154611110687256, 1.6195977926254272, 1.6203277111053467, 1.594156265258789, 1.5857149362564087, 1.605739712715149, 1.616486668586731, 1.5734007358551025, 1.6068603992462158, 1.5819355249404907, 1.580409049987793, 1.5926355123519897, 1.5955671072006226, 1.5735586881637573, 1.5627552270889282, 1.5664888620376587, 1.560760259628296, 1.5717055797576904, 1.561100721359253, 1.5705698728561401, 1.5831729173660278, 1.5320371389389038, 1.533799409866333, 1.5585895776748657, 1.5398472547531128, 1.5553641319274902, 1.5382386445999146, 1.528245449066162, 1.5427759885787964, 1.5191478729248047, 1.4985275268554688, 1.5028196573257446, 1.5060540437698364, 1.5340147018432617, 1.5258307456970215, 1.5022172927856445, 1.5138071775436401, 1.5043466091156006, 1.4954582452774048, 1.5058903694152832, 1.506684422492981, 1.49313485622406, 1.466357946395874, 1.4807742834091187, 1.4831185340881348, 1.4814292192459106, 1.476749300956726, 1.4583693742752075, 1.4747897386550903, 1.4796909093856812, 1.476324200630188, 1.45487642288208, 1.4673352241516113, 1.427094578742981, 1.4470133781433105, 1.4715290069580078, 1.435412883758545, 1.44062077999115, 1.4525669813156128, 1.4246127605438232, 1.4409353733062744, 1.431628704071045, 1.4202684164047241, 1.4486831426620483, 1.4052958488464355, 1.428424596786499, 1.422808289527893, 1.4275420904159546, 1.4408316612243652, 1.4368042945861816, 1.4155783653259277, 1.4290512800216675, 1.4028970003128052, 1.4143635034561157, 1.3796366453170776, 1.4123603105545044, 1.3997682332992554, 1.3972588777542114, 1.3904105424880981, 1.4100738763809204, 1.401191234588623, 1.3912328481674194, 1.3849579095840454, 1.3912348747253418, 1.3802404403686523, 1.3827662467956543, 1.387795090675354, 1.3733493089675903, 1.3489570617675781, 1.3714842796325684, 1.4060531854629517, 1.3552722930908203, 1.3574721813201904, 1.3555654287338257, 1.3622045516967773, 1.3694771528244019, 1.3805562257766724, 1.3605118989944458, 1.3298134803771973, 1.3780609369277954, 1.3673479557037354, 1.3758009672164917, 1.3491742610931396, 1.3774183988571167, 1.3495813608169556, 1.3417788743972778, 1.3283694982528687, 1.3501800298690796, 1.3280749320983887, 1.372830867767334, 1.347426176071167, 1.339472770690918, 1.3301113843917847, 1.331985592842102, 1.3283566236495972, 1.337622046470642, 1.330177664756775, 1.3359153270721436, 1.3467313051223755, 1.32935631275177, 1.300326943397522, 1.3194377422332764, 1.3368699550628662, 1.3321661949157715, 1.3288624286651611, 1.3062732219696045, 1.3112291097640991, 1.3137229681015015, 1.3289457559585571, 1.3008780479431152, 1.3049166202545166, 1.2812131643295288, 1.2943580150604248, 1.290899395942688, 1.295822262763977, 1.3107751607894897, 1.3066617250442505, 1.3018203973770142, 1.2668811082839966, 1.2953810691833496, 1.2923113107681274, 1.2633578777313232, 1.3026269674301147, 1.272801399230957, 1.3058639764785767, 1.2696560621261597, 1.2627211809158325, 1.2963626384735107, 1.2881523370742798, 1.2839504480361938, 1.2768930196762085, 1.2874114513397217, 1.2750976085662842, 1.2617288827896118, 1.2733567953109741, 1.2884390354156494, 1.2503812313079834, 1.2553961277008057, 1.269944190979004, 1.2862826585769653, 1.2762126922607422, 1.2661172151565552, 1.2555512189865112, 1.248623251914978, 1.2730209827423096, 1.256476640701294, 1.2472960948944092, 1.2434252500534058, 1.2566204071044922, 1.2508114576339722, 1.261888027191162, 1.2477195262908936, 1.2501741647720337, 1.2578949928283691, 1.2572609186172485, 1.2358217239379883, 1.2074700593948364, 1.2401496171951294, 1.2453112602233887, 1.2878103256225586, 1.2490205764770508, 1.2246544361114502, 1.2259799242019653, 1.222033143043518, 1.2212306261062622, 1.2164608240127563, 1.2246311902999878, 1.224374532699585, 1.2403154373168945, 1.2413502931594849, 1.226529598236084, 1.2195056676864624, 1.2082183361053467, 1.2152612209320068, 1.2011475563049316, 1.2258092164993286, 1.201250672340393, 1.2211824655532837, 1.2214107513427734, 1.2259061336517334, 1.2406190633773804, 1.2190805673599243, 1.2267640829086304, 1.1848702430725098, 1.2030662298202515, 1.1948491334915161, 1.2115007638931274, 1.205552339553833, 1.1938996315002441, 1.180986762046814, 1.2040579319000244, 1.2011899948120117, 1.178652048110962, 1.2005891799926758, 1.1752405166625977, 1.1734918355941772, 1.198664903640747, 1.1654689311981201, 1.1883397102355957, 1.201766848564148, 1.177977204322815, 1.1619234085083008, 1.1984713077545166, 1.1939157247543335, 1.1680887937545776, 1.1712992191314697, 1.1799880266189575, 1.177903413772583, 1.1848859786987305, 1.1692262887954712, 1.1770449876785278, 1.1875052452087402, 1.1934642791748047, 1.2061883211135864, 1.1567158699035645, 1.158498764038086, 1.1507411003112793, 1.1776604652404785, 1.1702824831008911, 1.1801702976226807, 1.1797248125076294, 1.1500755548477173, 1.1632674932479858, 1.1632623672485352, 1.14482581615448, 1.1552549600601196, 1.1374132633209229, 1.1403067111968994, 1.166560411453247, 1.132957935333252, 1.152960181236267, 1.1638246774673462, 1.1401289701461792, 1.155754566192627, 1.147054672241211, 1.1213234663009644, 1.1353703737258911, 1.1417196989059448, 1.1601769924163818, 1.1375277042388916, 1.1453982591629028, 1.1403682231903076, 1.1091829538345337, 1.1207486391067505, 1.1215530633926392, 1.1279574632644653, 1.105302095413208, 1.127087950706482, 1.1284843683242798, 1.1306493282318115, 1.1328682899475098, 1.1122074127197266, 1.1312520503997803, 1.152407169342041, 1.1113468408584595, 1.1198272705078125, 1.1238362789154053, 1.124677300453186, 1.0997707843780518, 1.1194403171539307, 1.1313577890396118, 1.1163722276687622, 1.1240026950836182, 1.116360068321228, 1.1151789426803589, 1.1235895156860352, 1.1059547662734985, 1.1031897068023682, 1.08932363986969, 1.0998330116271973, 1.1234595775604248, 1.1055119037628174, 1.0933218002319336, 1.09412682056427, 1.1194074153900146, 1.1160287857055664, 1.1036776304244995, 1.1072115898132324, 1.1163756847381592, 1.0671641826629639, 1.0837303400039673, 1.0846086740493774, 1.0987430810928345, 1.1097931861877441, 1.0900123119354248, 1.0862640142440796, 1.0940345525741577, 1.0739917755126953, 1.0679998397827148, 1.0979934930801392, 1.0830767154693604, 1.0832469463348389, 1.0760263204574585, 1.0921564102172852, 1.0500614643096924, 1.0604150295257568, 1.0516053438186646, 1.0449062585830688, 1.0720351934432983, 1.0835368633270264, 1.0726791620254517, 1.0690265893936157, 1.0659542083740234, 1.0714327096939087, 1.052998423576355, 1.0705409049987793, 1.0571630001068115, 1.05290949344635, 1.067488431930542, 1.070908784866333, 1.048088550567627, 1.0455787181854248, 1.0435360670089722, 1.0379279851913452, 1.0672974586486816, 1.049599528312683, 1.0623255968093872, 1.02056086063385, 1.033198595046997, 1.0448355674743652, 1.053481936454773, 1.0508328676223755, 1.0487103462219238, 1.04573392868042, 1.0529718399047852, 1.0479682683944702, 1.055046558380127, 1.0238065719604492, 1.0255613327026367, 1.0578123331069946, 1.0376853942871094, 1.0413095951080322, 1.0238922834396362, 1.022688865661621, 1.0158579349517822, 1.0241104364395142, 1.0251442193984985, 1.0300904512405396, 1.0071672201156616, 1.0298579931259155, 1.0321599245071411, 1.0137302875518799, 1.0290861129760742, 1.027420163154602, 0.9918935298919678, 1.047230839729309, 1.012225866317749, 1.0172500610351562, 1.0046650171279907, 1.0041252374649048, 1.0357420444488525, 1.0291849374771118, 1.009711503982544, 1.0096551179885864, 1.0024489164352417, 1.0131670236587524, 1.0090875625610352, 1.0089023113250732, 1.0250062942504883, 1.0092368125915527, 0.9945451021194458, 0.9891951084136963, 1.0125852823257446, 1.0038738250732422, 0.996224045753479, 0.9914293885231018, 0.9832810759544373, 0.9982104301452637, 0.999564528465271, 0.9737693071365356, 0.9833729863166809, 0.9999479651451111, 0.9974828362464905, 1.0031001567840576, 0.9750364422798157, 0.9768997430801392, 0.9975558519363403, 0.9635877013206482, 0.9807273745536804, 0.9811715483665466, 0.9835923910140991, 0.9669021368026733, 0.9824533462524414, 0.9800331592559814, 0.9707005620002747, 0.9577673077583313, 0.9749566316604614, 0.9796577095985413, 0.9566609263420105, 0.9724919199943542, 1.0063775777816772, 0.9427222609519958, 0.9548025131225586, 0.9527443647384644, 0.9875065088272095, 0.9832661747932434, 0.9713261723518372, 0.9715465903282166, 0.9521284699440002, 0.9896037578582764, 0.9409834146499634, 0.9374291896820068, 0.9694260358810425, 0.955539882183075, 0.9321897029876709, 0.9860493540763855, 0.9370150566101074, 0.9417462944984436, 0.9414617419242859, 0.9533271789550781, 0.9406449794769287, 0.939735472202301, 0.9687267541885376, 0.9218705296516418, 0.9246441721916199, 0.9423968195915222, 0.9340370893478394, 0.9333335161209106, 0.9368039965629578, 0.9184096455574036, 0.9351115226745605, 0.924700915813446, 0.9320099949836731, 0.9193204045295715, 0.9228156208992004, 0.925565242767334, 0.9416571855545044, 0.9048259258270264, 0.9036909341812134, 0.9328177571296692, 0.9264434576034546, 0.8972060084342957, 0.9054586887359619, 0.9127516746520996, 0.9059975743293762, 0.9128005504608154, 0.9114116430282593, 0.9230466485023499, 0.9245626926422119, 0.9238213300704956, 0.9206791520118713, 0.9164095520973206, 0.8878762722015381, 0.9097500443458557, 0.9185413122177124, 0.9260033369064331, 0.9170597791671753, 0.8883217573165894, 0.8998398184776306, 0.9003317356109619, 0.9314630627632141, 0.9041211605072021, 0.8830078840255737, 0.8985051512718201, 0.873929500579834, 0.883601725101471, 0.8969640731811523, 0.9121099710464478, 0.8843603134155273, 0.8887777924537659, 0.8828509449958801, 0.8874486684799194, 0.8970134258270264, 0.8851363658905029, 0.8936984539031982, 0.8679647445678711, 0.8931858539581299, 0.8862631916999817, 0.8708445429801941, 0.9050995111465454, 0.870011031627655, 0.8903385400772095, 0.8510406613349915, 0.8725690245628357, 0.8709796071052551, 0.8637604117393494, 0.8574386239051819, 0.888174831867218, 0.8646442890167236, 0.861239492893219, 0.8698149919509888, 0.8892016410827637, 0.8839721083641052, 0.8571116328239441, 0.8401677012443542, 0.8805897235870361, 0.8571968078613281, 0.857852041721344, 0.8628697991371155, 0.868766725063324, 0.864474892616272, 0.8712651133537292, 0.8509736061096191, 0.8473398089408875, 0.8465670347213745, 0.8459647297859192, 0.8609839677810669, 0.854734480381012, 0.8431838154792786, 0.8420829176902771, 0.8419970273971558, 0.8500509262084961, 0.8376734256744385, 0.8529585003852844, 0.825935959815979, 0.8702316284179688, 0.8436528444290161, 0.8275501728057861, 0.8241113424301147, 0.8532230257987976, 0.8334790468215942, 0.846872091293335, 0.8422404527664185, 0.8482413291931152, 0.8426461219787598, 0.8222745656967163, 0.8423869609832764, 0.8330531120300293, 0.8297280073165894, 0.8328263759613037, 0.8349252343177795, 0.8112025260925293, 0.8032221794128418, 0.8102820515632629, 0.8480914235115051, 0.8089979887008667, 0.8177345395088196, 0.8240320682525635, 0.8148987293243408, 0.7870995998382568, 0.821186363697052, 0.8375633358955383, 0.7984181046485901, 0.8044148087501526, 0.8160588145256042, 0.809694230556488, 0.803839385509491, 0.824788510799408, 0.7598060369491577, 0.7893695831298828, 0.7944335341453552, 0.8008406162261963, 0.7887113690376282, 0.7768871188163757, 0.7749049067497253, 0.7927645444869995, 0.8090289235115051, 0.7921596169471741, 0.776106595993042, 0.819116473197937, 0.7894266247749329, 0.7871904373168945, 0.7911723256111145, 0.7935099601745605, 0.7665596604347229, 0.7504300475120544, 0.788200318813324, 0.7796545028686523, 0.7963815927505493, 0.777366042137146, 0.785240650177002, 0.7788310647010803, 0.791618287563324, 0.7664757370948792, 0.7709541320800781, 0.7813640236854553, 0.7896615862846375, 0.7623502016067505, 0.7763348817825317, 0.7716308832168579, 0.7606446146965027, 0.762409508228302, 0.7670124769210815, 0.7850741744041443, 0.7713820934295654, 0.7502641081809998, 0.7624452114105225, 0.7194530367851257, 0.7603055238723755, 0.7490655183792114, 0.7631025314331055, 0.7562946677207947, 0.7636366486549377, 0.7471838593482971, 0.741172730922699, 0.7508608102798462, 0.7570163011550903, 0.7525456547737122, 0.7608811259269714, 0.7452899217605591, 0.7373533248901367, 0.7588490843772888, 0.7481153011322021, 0.7504867911338806, 0.7488850355148315, 0.7710113525390625, 0.7442578673362732, 0.742422878742218, 0.7241527438163757, 0.7409902215003967, 0.7140583992004395, 0.73992919921875, 0.7306428551673889, 0.7223189473152161, 0.7531002759933472, 0.7315916419029236, 0.7101337909698486, 0.7357190251350403, 0.70289146900177, 0.7386218905448914, 0.7352716326713562, 0.717473566532135, 0.7148852944374084, 0.7023753523826599, 0.7198449969291687, 0.7365041971206665, 0.7095251679420471, 0.7207967638969421, 0.7141344547271729, 0.6891604661941528, 0.7150562405586243, 0.7281773090362549, 0.7268192172050476, 0.7089043855667114, 0.7013479471206665, 0.7056859731674194, 0.7163294553756714, 0.6670862436294556, 0.730586588382721, 0.7182647585868835, 0.7009989023208618, 0.7136582732200623, 0.7055206298828125, 0.6731526255607605, 0.6810446381568909, 0.6904405355453491, 0.6980323195457458, 0.7030759453773499, 0.684861958026886, 0.6909939050674438, 0.6941859722137451, 0.6992828249931335, 0.6896222233772278, 0.7033465504646301, 0.6787405014038086, 0.6951967477798462, 0.6840800046920776, 0.678876519203186, 0.6587333679199219, 0.6768800616264343, 0.6749815940856934, 0.6685354709625244, 0.6812819242477417, 0.6666637659072876, 0.670650839805603, 0.678280234336853, 0.6673982739448547, 0.6702117919921875, 0.6876349449157715, 0.6860859394073486, 0.6700615286827087, 0.6747683882713318, 0.6493180990219116, 0.6745269298553467, 0.6690500974655151, 0.6941074728965759, 0.6900129318237305, 0.6803408265113831, 0.6552414298057556, 0.6458066701889038, 0.6618063449859619, 0.6557672023773193, 0.6473624110221863, 0.6634374856948853, 0.6773409843444824, 0.6556218266487122, 0.6524171829223633, 0.6581475734710693, 0.6366525292396545, 0.6429340243339539, 0.657879114151001, 0.6742270588874817, 0.6337159872055054, 0.6308813095092773, 0.6479148268699646, 0.6398127675056458, 0.649508535861969, 0.6481521725654602, 0.6428335309028625, 0.669309675693512, 0.6455931663513184, 0.6195030808448792, 0.6224750876426697, 0.6289682984352112, 0.6463795900344849, 0.6350144743919373, 0.6195283532142639, 0.6182172894477844, 0.6309629082679749, 0.6252514719963074, 0.6360068321228027, 0.6386464238166809, 0.6176316738128662, 0.6150295734405518, 0.6381963491439819, 0.6459829807281494, 0.6308979392051697, 0.6219496726989746, 0.609927773475647, 0.6291334629058838, 0.618228554725647, 0.6045679450035095, 0.6271705031394958, 0.621859610080719, 0.6039213538169861, 0.6450941562652588, 0.6111156344413757, 0.6234515309333801, 0.6132470965385437, 0.5989235639572144, 0.6048957705497742, 0.6222429871559143, 0.5979719758033752, 0.5974946022033691, 0.6170825958251953, 0.6031520366668701, 0.6150465607643127, 0.6167460680007935, 0.5974218249320984, 0.591866135597229, 0.6033821105957031, 0.5907783508300781, 0.5932510495185852, 0.5884249210357666, 0.5879614949226379, 0.5929247736930847, 0.5971359014511108, 0.5898954272270203, 0.6177228689193726, 0.6224525570869446, 0.5881989598274231, 0.5870516300201416, 0.5877355337142944, 0.5879058837890625, 0.5815791487693787, 0.5644744634628296, 0.5835272669792175, 0.6038891077041626, 0.5853723883628845, 0.5706973075866699, 0.5611203908920288, 0.5956658720970154, 0.5800585150718689, 0.5617295503616333, 0.6011903285980225, 0.5830005407333374, 0.5664185285568237, 0.5578607320785522, 0.5495802760124207, 0.579601526260376, 0.6007336378097534, 0.574564516544342, 0.5726308226585388, 0.5638440251350403, 0.5934774279594421, 0.5667726397514343, 0.5873885750770569, 0.5772081017494202, 0.5570935606956482, 0.5505825877189636, 0.55769944190979, 0.5527622699737549, 0.5476192831993103, 0.5412344932556152, 0.5439205169677734, 0.5674260258674622, 0.551919162273407, 0.5836169719696045, 0.5312812328338623, 0.5429826378822327, 0.5684410929679871, 0.5441825985908508, 0.5344237685203552, 0.5382094383239746, 0.5348406434059143, 0.5317961573600769, 0.5342376828193665, 0.5586729049682617, 0.5490450859069824, 0.5485029220581055, 0.5500613451004028, 0.537449061870575, 0.5393185019493103, 0.5513683557510376, 0.534182071685791, 0.5418011546134949, 0.5473906993865967, 0.5486627221107483, 0.5433785319328308, 0.5355453491210938, 0.5279157161712646, 0.5141682028770447, 0.5364567637443542, 0.5440679788589478, 0.5078650712966919, 0.5094119310379028, 0.5391164422035217, 0.5208346843719482, 0.5304046869277954, 0.5260195732116699, 0.5283095240592957, 0.506152868270874, 0.5410550236701965, 0.5346771478652954, 0.5140568017959595, 0.5175642371177673, 0.516713559627533, 0.5171585083007812, 0.5177320241928101, 0.5111912488937378, 0.5131734013557434, 0.5348190069198608, 0.5228176116943359, 0.5206538438796997, 0.5072416663169861, 0.517136812210083, 0.5297569036483765, 0.4948400855064392, 0.508321225643158, 0.49658912420272827, 0.48870542645454407, 0.4984498918056488, 0.5046624541282654, 0.500998854637146, 0.4842130243778229, 0.515223503112793, 0.46653419733047485, 0.49463993310928345, 0.5175678730010986, 0.4837254285812378, 0.4835807979106903, 0.4854000508785248, 0.4576081335544586, 0.5028008818626404, 0.4932718873023987, 0.47226038575172424, 0.5029178857803345, 0.483335942029953, 0.45797279477119446, 0.49227777123451233, 0.48607102036476135, 0.46853479743003845, 0.4523763358592987, 0.4732597768306732, 0.4618282616138458, 0.480838805437088, 0.4726370871067047, 0.47378936409950256, 0.506872832775116, 0.47891512513160706, 0.48741036653518677, 0.4726947247982025, 0.46528106927871704, 0.4555808901786804, 0.45158764719963074, 0.4618462324142456, 0.465785950422287, 0.4849506914615631, 0.48539140820503235, 0.4549309313297272, 0.4944915473461151, 0.4561344385147095, 0.4619176983833313, 0.447975218296051, 0.480803519487381, 0.4601879417896271, 0.4534432590007782, 0.46998700499534607, 0.4545426070690155, 0.4831554591655731, 0.46881476044654846, 0.4568852186203003, 0.4499266445636749, 0.4656912088394165, 0.4503163993358612, 0.452376127243042, 0.46100690960884094, 0.4475756883621216, 0.4362645447254181, 0.454198956489563, 0.4627213478088379, 0.43765580654144287, 0.44145622849464417, 0.43605613708496094, 0.4196503162384033, 0.4413890540599823, 0.4308495819568634, 0.41862422227859497, 0.4420619010925293, 0.4465542435646057, 0.429204523563385, 0.4403510093688965, 0.43384087085723877, 0.447803795337677, 0.43738487362861633, 0.42830413579940796, 0.4230206608772278, 0.44614318013191223, 0.4250507652759552, 0.4259606599807739, 0.42511048913002014, 0.41071799397468567, 0.4351496994495392, 0.4339853823184967, 0.41631466150283813, 0.4402901530265808, 0.39457571506500244, 0.42787253856658936, 0.4133654832839966, 0.40521135926246643, 0.4124457836151123, 0.4335647523403168, 0.44282689690589905, 0.40209293365478516, 0.41788560152053833, 0.4180397689342499, 0.41988298296928406, 0.4086122214794159, 0.4131576418876648, 0.39846867322921753, 0.41990774869918823, 0.39717021584510803, 0.40815070271492004, 0.42697423696517944, 0.4006967544555664, 0.39238622784614563, 0.393913209438324, 0.40127915143966675, 0.41053488850593567, 0.38169705867767334, 0.39769303798675537, 0.3871994912624359, 0.40618258714675903, 0.38912728428840637, 0.38864433765411377, 0.4020882248878479, 0.40274569392204285, 0.394189715385437, 0.38393327593803406, 0.38006845116615295, 0.3822053074836731, 0.3774550259113312, 0.40103092789649963, 0.41025617718696594, 0.3820677101612091, 0.37701722979545593, 0.3904528319835663, 0.3803383708000183, 0.3709999918937683, 0.3758997619152069, 0.3878953158855438, 0.3659491240978241, 0.3816424012184143, 0.38681915402412415, 0.38492152094841003, 0.38927698135375977, 0.3652726411819458, 0.35975682735443115, 0.3673895299434662, 0.3660808205604553, 0.3771745264530182, 0.3605600893497467, 0.3726803660392761, 0.3841579258441925, 0.3552378714084625, 0.38369911909103394, 0.3646673858165741, 0.3691462576389313, 0.3663657605648041, 0.380176305770874, 0.3843706250190735, 0.3689676523208618, 0.3569372892379761, 0.32971277832984924, 0.35991227626800537, 0.3360939919948578, 0.3647440969944, 0.3695482909679413, 0.34967634081840515, 0.3462381660938263, 0.35124221444129944, 0.3559395670890808, 0.35295340418815613, 0.3345932960510254, 0.34473127126693726, 0.34506702423095703, 0.36411619186401367, 0.36124497652053833, 0.3502854108810425, 0.36947867274284363, 0.3544376492500305, 0.3510388135910034, 0.3602907359600067, 0.33080753684043884, 0.345516562461853, 0.34090444445610046, 0.350136399269104, 0.34879589080810547, 0.3425568640232086, 0.3509150743484497, 0.3532774746417999, 0.3271099627017975, 0.338608980178833, 0.3496944308280945, 0.3379463255405426, 0.34837067127227783, 0.3314362168312073, 0.32092973589897156, 0.34247085452079773, 0.34686750173568726, 0.3317474126815796, 0.30817243456840515, 0.34045279026031494, 0.3346025049686432, 0.35503819584846497, 0.3427755534648895, 0.32941463589668274, 0.3485846221446991, 0.3204617202281952, 0.33662149310112, 0.31090736389160156, 0.33297014236450195, 0.3491229712963104, 0.3253868520259857, 0.3033734858036041, 0.3012249171733856, 0.2950134575366974, 0.33664989471435547, 0.32981881499290466, 0.3155530095100403, 0.33660057187080383, 0.3108394145965576, 0.32052138447761536, 0.31775155663490295, 0.3114226758480072, 0.33548104763031006, 0.3243485689163208, 0.31175696849823, 0.32816532254219055, 0.3305605947971344, 0.3134373426437378, 0.31614264845848083, 0.3106865882873535, 0.32867467403411865, 0.30550917983055115, 0.33147501945495605, 0.29173940420150757, 0.2946396768093109, 0.29922255873680115, 0.2933329641819, 0.2935335040092468, 0.2784534692764282, 0.2993473410606384, 0.3184458315372467, 0.31535980105400085, 0.29246053099632263, 0.28857046365737915, 0.29053789377212524, 0.2887779772281647, 0.3200661540031433, 0.3055585026741028, 0.2846696674823761, 0.30453547835350037, 0.2824150621891022, 0.30075564980506897, 0.2922665774822235, 0.2789579927921295, 0.29021674394607544, 0.2934291660785675, 0.2964125871658325, 0.282610148191452, 0.29527485370635986, 0.2677658796310425, 0.2806238830089569, 0.2826734781265259, 0.27729934453964233, 0.2722640335559845, 0.27823495864868164, 0.2665393352508545, 0.2740556299686432, 0.2711511254310608, 0.26527538895606995, 0.28873300552368164, 0.278747022151947, 0.2867635190486908, 0.2680732309818268, 0.24858368933200836, 0.2900942862033844, 0.26363658905029297, 0.27240750193595886, 0.2686093747615814, 0.27128562331199646, 0.2770117521286011, 0.28247347474098206, 0.25156742334365845, 0.26021096110343933, 0.2800474464893341, 0.2582731246948242, 0.25178274512290955, 0.3066149652004242, 0.2607457637786865, 0.26515302062034607, 0.2604444921016693, 0.2535812258720398, 0.2620420455932617, 0.2604314088821411, 0.257469117641449, 0.27170446515083313, 0.2645004987716675, 0.24587054550647736, 0.2705162465572357, 0.23861658573150635, 0.26310011744499207, 0.23846766352653503, 0.25711822509765625, 0.2548869848251343, 0.26639482378959656, 0.26322612166404724, 0.2371489256620407, 0.2583443224430084, 0.23814891278743744, 0.2620035707950592, 0.25645965337753296, 0.25221696496009827, 0.25290849804878235, 0.24345679581165314, 0.23880425095558167, 0.27075421810150146, 0.25361987948417664, 0.24957768619060516, 0.2418663650751114, 0.26088911294937134, 0.2633441388607025, 0.2378200888633728, 0.24042102694511414, 0.24593771994113922, 0.2568964958190918, 0.237945556640625, 0.22907228767871857, 0.24149680137634277, 0.2578144669532776, 0.2389078140258789, 0.25309300422668457, 0.23360562324523926, 0.2423877865076065, 0.24169737100601196, 0.2462545782327652, 0.260284423828125, 0.22641365230083466, 0.2244003415107727, 0.22435668110847473, 0.22749750316143036, 0.2344861775636673, 0.2210417240858078, 0.23623955249786377, 0.22445827722549438, 0.2574646472930908, 0.24492818117141724, 0.2561202347278595, 0.2547707259654999, 0.21834348142147064, 0.22590269148349762, 0.21973629295825958, 0.20134827494621277, 0.21862424910068512, 0.239142507314682, 0.2117900252342224, 0.2320593297481537, 0.23075930774211884, 0.2208402305841446, 0.21638761460781097, 0.22471149265766144, 0.20732419192790985, 0.20623376965522766, 0.20318669080734253, 0.20633496344089508, 0.21567964553833008, 0.2272816151380539, 0.22504214942455292, 0.21951378881931305, 0.22452743351459503, 0.20783782005310059, 0.20116396248340607, 0.23732851445674896, 0.218808114528656, 0.2019306719303131, 0.2169417440891266, 0.1983523964881897, 0.2115987241268158, 0.2159135639667511, 0.20455700159072876, 0.20064841210842133, 0.1981353908777237, 0.20516979694366455, 0.18552641570568085, 0.20708677172660828, 0.20320045948028564, 0.204007089138031, 0.184769406914711, 0.2338908165693283, 0.22212614119052887, 0.20965832471847534, 0.21762637794017792, 0.2026999294757843, 0.21960724890232086, 0.20829710364341736, 0.1873849630355835, 0.19885824620723724, 0.19524921476840973, 0.201808899641037, 0.21500001847743988, 0.18530261516571045, 0.1773875206708908, 0.19628548622131348, 0.20612004399299622, 0.1856873333454132, 0.2133493274450302, 0.20279301702976227, 0.19661730527877808, 0.2050292193889618, 0.19220225512981415, 0.18039488792419434, 0.2062724083662033, 0.2038457691669464, 0.19772864878177643, 0.2089701145887375, 0.20680460333824158, 0.1885140836238861, 0.1904142051935196, 0.20119914412498474, 0.19797605276107788, 0.2033671885728836, 0.16286107897758484, 0.18662072718143463, 0.17273558676242828, 0.18077917397022247, 0.18652664124965668, 0.18430499732494354, 0.20533592998981476, 0.1834733635187149, 0.1750262975692749, 0.1836087852716446, 0.19123105704784393, 0.1722022145986557, 0.16709774732589722, 0.1797221153974533, 0.17261506617069244, 0.1667882204055786, 0.19626039266586304, 0.18395096063613892, 0.1659909188747406, 0.17320962250232697, 0.18788449466228485, 0.17524239420890808, 0.15183639526367188, 0.1762136071920395, 0.17232459783554077, 0.1915842443704605, 0.1694050133228302, 0.1929253190755844, 0.19121672213077545, 0.16065458953380585, 0.18859261274337769, 0.16884906589984894, 0.18410217761993408, 0.173971027135849, 0.18502646684646606, 0.17581988871097565, 0.17188674211502075, 0.15494918823242188, 0.16412314772605896, 0.15211625397205353, 0.18346625566482544, 0.1524038463830948, 0.1676451414823532, 0.1643056720495224, 0.15604066848754883, 0.18027503788471222, 0.16270536184310913, 0.15283676981925964, 0.17005349695682526, 0.18215587735176086, 0.1612614244222641, 0.16425618529319763, 0.14998431503772736, 0.16809968650341034, 0.15708814561367035, 0.17027947306632996, 0.17442601919174194, 0.16946151852607727, 0.15373624861240387, 0.16001300513744354, 0.1720769703388214, 0.15127497911453247, 0.1597473919391632, 0.18162396550178528, 0.15574118494987488, 0.14825260639190674, 0.14662662148475647, 0.17915353178977966, 0.14299365878105164, 0.14581875503063202, 0.1519155353307724, 0.1413806676864624, 0.16160665452480316, 0.14659497141838074, 0.148845836520195, 0.16827692091464996, 0.1588422805070877, 0.15002384781837463, 0.16306744515895844, 0.14704178273677826, 0.15389707684516907, 0.14631205797195435, 0.1452152580022812, 0.15654225647449493, 0.15747971832752228, 0.1440969705581665, 0.1406572312116623, 0.13747282326221466, 0.13078686594963074, 0.162488654255867, 0.14043140411376953, 0.14444048702716827, 0.1470927745103836, 0.13305607438087463, 0.14958298206329346, 0.13536620140075684, 0.14104855060577393, 0.13339683413505554, 0.15049603581428528, 0.13521605730056763, 0.1342109739780426, 0.1366388201713562, 0.13184158504009247, 0.15824459493160248, 0.14434942603111267, 0.14833524823188782, 0.14031167328357697, 0.12719585001468658, 0.13757529854774475, 0.14447137713432312, 0.14020085334777832, 0.1379772126674652, 0.13314414024353027, 0.13139809668064117, 0.13657818734645844, 0.1401621401309967, 0.14179927110671997, 0.1410561352968216, 0.12909914553165436, 0.1338588148355484, 0.14667700231075287, 0.1462879627943039, 0.14024557173252106, 0.15121378004550934, 0.14160634577274323, 0.13675208389759064, 0.11810564994812012, 0.12755310535430908, 0.12841635942459106, 0.12349280714988708, 0.1261112093925476, 0.11497018486261368, 0.1285473108291626, 0.13203011453151703, 0.1208108514547348, 0.13859699666500092, 0.12014292925596237, 0.11544223129749298, 0.11644898355007172, 0.13089966773986816, 0.1228407770395279, 0.11955409497022629, 0.13524378836154938, 0.1262047439813614, 0.12705367803573608, 0.11909782141447067, 0.12593767046928406, 0.11967355757951736, 0.12129391729831696, 0.12049209326505661, 0.119603730738163, 0.1193084791302681, 0.13381415605545044, 0.1277109682559967, 0.13507962226867676, 0.11746576428413391, 0.11901190131902695, 0.12903986871242523, 0.12647494673728943, 0.12001567333936691, 0.12498854845762253, 0.12168825417757034, 0.10476703196763992, 0.11557856947183609, 0.12055420875549316, 0.1266198605298996, 0.11572406440973282, 0.12984807789325714, 0.1026732474565506, 0.1394185721874237, 0.11592482775449753, 0.11001530289649963, 0.10936781764030457, 0.10507962107658386, 0.10647274553775787, 0.12183523923158646, 0.11619589477777481, 0.11460267752408981, 0.11061994731426239, 0.11045873910188675, 0.09556669741868973, 0.11797954887151718, 0.12738066911697388, 0.11349380761384964, 0.11150028556585312, 0.1222013384103775, 0.1075507178902626, 0.10664249956607819, 0.09622689336538315, 0.11069007217884064, 0.11205793917179108, 0.10221051424741745, 0.12260671705007553, 0.10272639244794846, 0.10317888855934143, 0.1178072839975357, 0.10746021568775177, 0.1045374795794487, 0.10719729214906693, 0.1106884554028511, 0.10787744075059891, 0.10981118679046631, 0.11407124251127243, 0.09540896117687225, 0.10413479804992676, 0.09733331203460693, 0.10298541933298111, 0.0966939628124237, 0.1252969354391098, 0.10340631008148193, 0.08866683393716812, 0.11262533068656921, 0.09110813587903976, 0.10410650819540024, 0.09560106694698334, 0.11778336018323898, 0.09739647805690765, 0.11174144595861435, 0.10469214618206024, 0.09057452529668808, 0.10810419172048569, 0.08771486580371857, 0.1225922703742981, 0.09848590940237045, 0.08896461874246597, 0.09556438773870468, 0.09280240535736084, 0.10810726881027222, 0.07936829328536987, 0.09669957309961319, 0.10613930970430374, 0.10035738348960876, 0.08971919864416122, 0.0970187783241272, 0.09299668669700623, 0.09787436574697495, 0.09844021499156952, 0.1006438359618187, 0.10973446816205978, 0.08866851031780243, 0.11183105409145355, 0.1017376258969307, 0.09415273368358612, 0.09233792871236801, 0.08013671636581421, 0.10213654488325119, 0.09516395628452301, 0.0933244600892067, 0.08961383253335953, 0.09170911461114883, 0.08386753499507904, 0.09112544357776642, 0.10212952643632889, 0.09516579657793045, 0.08526409417390823, 0.09661433845758438, 0.09351088106632233, 0.08275134116411209, 0.09321395307779312, 0.0982750877737999, 0.08659607917070389, 0.0826980471611023, 0.08736808598041534, 0.07959666103124619, 0.09206517040729523, 0.08047185838222504, 0.08313171565532684, 0.08003316074609756, 0.09118252992630005, 0.07956843078136444, 0.07636493444442749, 0.08489043265581131, 0.07918696850538254, 0.08390985429286957, 0.0933256521821022, 0.08001858741044998, 0.09784594923257828, 0.0879054069519043, 0.09450218081474304, 0.09625283628702164, 0.0745079442858696, 0.08173606544733047, 0.08038494735956192, 0.08364930003881454, 0.07701878994703293, 0.08624710887670517, 0.08317847549915314, 0.08728083223104477, 0.09282950311899185, 0.07998688519001007, 0.09611521661281586, 0.08105338364839554, 0.0816538855433464, 0.07283037155866623, 0.07295435667037964, 0.09596068412065506, 0.07125205546617508, 0.08234759420156479, 0.07288943231105804, 0.07719936221837997, 0.07858573645353317, 0.08016988635063171, 0.09108336269855499, 0.06026311218738556, 0.06945367157459259, 0.07936577498912811, 0.07452331483364105, 0.07625242322683334, 0.07356259971857071, 0.06955860555171967, 0.06310670077800751, 0.06417164206504822, 0.09012673795223236, 0.07342160493135452, 0.08251087367534637, 0.07214778661727905, 0.07767516374588013, 0.08482231199741364, 0.07624693959951401, 0.06722668558359146, 0.05889901891350746, 0.08908238261938095, 0.06876974552869797, 0.07845129817724228, 0.07050637155771255, 0.07118885219097137, 0.07821585237979889, 0.06738497316837311, 0.07772909104824066, 0.06914974004030228, 0.07983772456645966, 0.0801658034324646, 0.06730079650878906, 0.06768082827329636, 0.06661298871040344, 0.060643941164016724, 0.07811127603054047, 0.07712370902299881, 0.07220315933227539, 0.07380449771881104, 0.06072142347693443, 0.07852811366319656, 0.06638813763856888, 0.06521817296743393, 0.06156031787395477, 0.06761113554239273, 0.05615999922156334, 0.0623587928712368, 0.06950501352548599, 0.06765853613615036, 0.07446595281362534, 0.06673099100589752, 0.07007016241550446, 0.057453472167253494, 0.07512665539979935, 0.07862619310617447, 0.07426337897777557, 0.0685746818780899, 0.07299058139324188, 0.07804213464260101, 0.06771238893270493, 0.05320807918906212, 0.049473777413368225, 0.06565684825181961, 0.07500570267438889, 0.07856003195047379, 0.0627426728606224, 0.0611736960709095, 0.07100000232458115, 0.05970605090260506, 0.06739311665296555, 0.05428725853562355, 0.05770179256796837, 0.06890720874071121, 0.06433288007974625, 0.06753159314393997, 0.05607784539461136, 0.05717691406607628, 0.0643744021654129, 0.060837339609861374, 0.07569260150194168, 0.06525865942239761, 0.059599682688713074, 0.06996308267116547, 0.05767833814024925, 0.05390220135450363, 0.06471651792526245, 0.06526167690753937, 0.06486574560403824, 0.054903365671634674, 0.07723874598741531, 0.051489319652318954, 0.06836386024951935, 0.05915252864360809, 0.06731315702199936, 0.0522909089922905, 0.05693987384438515, 0.056830890476703644, 0.06174831837415695, 0.06385280191898346, 0.05574136972427368, 0.06547536700963974, 0.05569177493453026, 0.05633784458041191, 0.050031524151563644, 0.061243340373039246, 0.054741743952035904, 0.05366826802492142, 0.05221381038427353, 0.06930817663669586, 0.05458967387676239, 0.058452244848012924, 0.05696368217468262, 0.060261666774749756, 0.05489069223403931, 0.06972767412662506, 0.055279526859521866, 0.05726545304059982, 0.058577243238687515, 0.05259176716208458, 0.059904925525188446, 0.045619815587997437, 0.04987086355686188, 0.05607490614056587, 0.048974957317113876, 0.04454472288489342, 0.05150391906499863, 0.06180708482861519, 0.061328619718551636, 0.06682972609996796, 0.05234384909272194, 0.053281206637620926, 0.0509183406829834, 0.06262887269258499, 0.042145635932683945, 0.060280073434114456, 0.06465457379817963, 0.06550780683755875, 0.0530993677675724, 0.04988079518079758, 0.051658712327480316, 0.049071650952100754, 0.0575859360396862, 0.05513112246990204, 0.054165150970220566, 0.04972294345498085, 0.04771053418517113, 0.05586811527609825, 0.05296337231993675, 0.048993948847055435, 0.04590873420238495, 0.06741831451654434, 0.0677160918712616, 0.05025101080536842, 0.04741669073700905, 0.04736701399087906, 0.05093350261449814, 0.0462697371840477, 0.06051452085375786, 0.04644531011581421, 0.04919097200036049, 0.048132337629795074, 0.05358865484595299, 0.04355962574481964, 0.04353616014122963, 0.0544249527156353, 0.04865527153015137, 0.04204872250556946, 0.04441754147410393, 0.041191067546606064, 0.05048952251672745, 0.040807485580444336, 0.044165898114442825, 0.0441717766225338, 0.05083936080336571, 0.047876615077257156, 0.04239943251013756, 0.0488763228058815, 0.04932722821831703, 0.047776028513908386, 0.04710448160767555, 0.04115328937768936, 0.04688844829797745, 0.045625053346157074, 0.04850594699382782, 0.04186062887310982, 0.050072744488716125, 0.039664216339588165, 0.040770672261714935, 0.04338691011071205, 0.049098361283540726, 0.052699461579322815, 0.03929756581783295, 0.050031088292598724, 0.04823647439479828, 0.05443127825856209, 0.04137970507144928, 0.04724675044417381, 0.04765494167804718, 0.045919474214315414, 0.03830654174089432, 0.05153433978557587, 0.039689622819423676, 0.048013146966695786, 0.040586430579423904, 0.035780586302280426, 0.04322356730699539, 0.0440731942653656, 0.045135367661714554, 0.034910667687654495, 0.036912478506565094, 0.042069535702466965, 0.04308253154158592, 0.05946808680891991, 0.04701194539666176, 0.040833670645952225, 0.0540035218000412, 0.037537623196840286, 0.05070701986551285, 0.04099135100841522, 0.03193262964487076, 0.041940219700336456, 0.04446396231651306, 0.04013923183083534, 0.03653009608387947, 0.03641948476433754, 0.0411340706050396, 0.0578792579472065, 0.034689996391534805, 0.04319532960653305, 0.038342587649822235, 0.039699893444776535, 0.0346435084939003, 0.04380681365728378, 0.04164770245552063, 0.045031093060970306, 0.040276430547237396, 0.03856377676129341, 0.040216442197561264, 0.0365104153752327, 0.04217756539583206, 0.04133985564112663, 0.041453637182712555, 0.04751010239124298, 0.04366360977292061, 0.03765568509697914, 0.04244916886091232, 0.03723299503326416, 0.040213510394096375, 0.04214952886104584, 0.03377972170710564, 0.03908836841583252, 0.03544355183839798, 0.043599385768175125, 0.030315140262246132, 0.042499441653490067, 0.044073764234781265, 0.036042433232069016, 0.027930131182074547, 0.037926141172647476, 0.038603220134973526, 0.035445261746644974, 0.04259030148386955, 0.03697012737393379, 0.03625575453042984, 0.030948475003242493, 0.03821185231208801, 0.039260923862457275, 0.031245645135641098, 0.04739365354180336, 0.03132280707359314, 0.04586373269557953, 0.03682159259915352, 0.03560059145092964, 0.03609522059559822, 0.04323084279894829, 0.04084572196006775, 0.03400278463959694, 0.04078492149710655, 0.0332927405834198, 0.03838010132312775, 0.026691284030675888, 0.035023290663957596, 0.041822582483291626, 0.03943115845322609, 0.03743625059723854, 0.035232894122600555, 0.048057425767183304, 0.03591255098581314, 0.03137712553143501, 0.04431867226958275, 0.02975286915898323, 0.03998919576406479, 0.03564729541540146, 0.03606536239385605, 0.031932175159454346, 0.03264238312840462, 0.03566691279411316, 0.03420345485210419, 0.03454847261309624, 0.030545298010110855, 0.03135683014988899, 0.03233008086681366, 0.031060542911291122, 0.035302918404340744, 0.02518193982541561, 0.036196473985910416, 0.023483743891119957, 0.032712340354919434, 0.03752127289772034, 0.03762165084481239, 0.02811839058995247, 0.03027384914457798, 0.035739749670028687, 0.03109404444694519, 0.02864420786499977, 0.03205481171607971, 0.03137892484664917, 0.03987615555524826, 0.03232572227716446, 0.03241332620382309, 0.02934369258582592, 0.03692774474620819, 0.032910481095314026], 'accuracy': [0.10447761416435242, 0.1166892796754837, 0.12483039498329163, 0.12211669236421585, 0.1383989155292511, 0.1601085513830185, 0.16417910158634186, 0.17774762213230133, 0.15739484131336212, 0.18453188240528107, 0.16282224655151367, 0.17774762213230133, 0.17503392696380615, 0.19538670778274536, 0.19810040295124054, 0.20759837329387665, 0.20488466322422028, 0.17774762213230133, 0.19402985274791718, 0.18995928764343262, 0.2130257785320282, 0.20488466322422028, 0.21573948860168457, 0.2130257785320282, 0.2130257785320282, 0.22659429907798767, 0.2333785593509674, 0.19810040295124054, 0.23066486418247223, 0.255088210105896, 0.2645861506462097, 0.22523744404315948, 0.2347354143857956, 0.22659429907798767, 0.24016281962394714, 0.24016281962394714, 0.22659429907798767, 0.2523745000362396, 0.2442333847284317, 0.23880596458911896, 0.26865673065185547, 0.24016281962394714, 0.2442333847284317, 0.25101763010025024, 0.26051560044288635, 0.2442333847284317, 0.2781546711921692, 0.24694707989692688, 0.27951154112815857, 0.2659430205821991, 0.2632293105125427, 0.26865673065185547, 0.26187247037887573, 0.26187247037887573, 0.29172319173812866, 0.2971506118774414, 0.2767978310585022, 0.2537313401699066, 0.29443690180778503, 0.28086838126182556, 0.2659430205821991, 0.2890095114707947, 0.3297150731086731, 0.29036635160446167, 0.3080054223537445, 0.30529171228408813, 0.26051560044288635, 0.321573942899704, 0.31207597255706787, 0.30393487215042114, 0.3175033926963806, 0.32564449310302734, 0.2985074520111084, 0.32564449310302734, 0.2998643219470978, 0.3188602328300476, 0.3175033926963806, 0.3093622922897339, 0.3107191324234009, 0.3093622922897339, 0.34464043378829956, 0.32564449310302734, 0.3161465525627136, 0.3107191324234009, 0.3514246940612793, 0.3310719132423401, 0.31343284249305725, 0.321573942899704, 0.31207597255706787, 0.3378561735153198, 0.320217102766037, 0.3622795045375824, 0.3283582031726837, 0.3283582031726837, 0.3487109839916229, 0.3419267237186432, 0.34599727392196655, 0.35685211420059204, 0.3419267237186432, 0.3270013630390167, 0.34599727392196655, 0.34735414385795593, 0.385345995426178, 0.35413840413093567, 0.359565794467926, 0.3527815341949463, 0.3419267237186432, 0.359565794467926, 0.36906376481056213, 0.3514246940612793, 0.3826322853565216, 0.3487109839916229, 0.3934870958328247, 0.38805970549583435, 0.36770692467689514, 0.3731343150138855, 0.36906376481056213, 0.3921302556991577, 0.36499321460723877, 0.37720489501953125, 0.4016282260417938, 0.3744911849498749, 0.36906376481056213, 0.3731343150138855, 0.3704206347465515, 0.3744911849498749, 0.37991857528686523, 0.38941654562950134, 0.3704206347465515, 0.385345995426178, 0.3934870958328247, 0.383989155292511, 0.4151967465877533, 0.3812754452228546, 0.37720489501953125, 0.4043419361114502, 0.4043419361114502, 0.4056987762451172, 0.40976932644844055, 0.42062416672706604, 0.3934870958328247, 0.4043419361114502, 0.4138398766517639, 0.39891451597213745, 0.4070556163787842, 0.4138398766517639, 0.4450474977493286, 0.41926729679107666, 0.4165535867214203, 0.4274084270000458, 0.40841248631477356, 0.4056987762451172, 0.39755767583847046, 0.44097694754600525, 0.42198100686073303, 0.4233378469944, 0.4246947169303894, 0.4233378469944, 0.4124830365180969, 0.46132972836494446, 0.4233378469944, 0.4572591483592987, 0.45183175802230835, 0.4586160182952881, 0.43147897720336914, 0.42876526713371277, 0.4382632374763489, 0.4369063675403595, 0.42198100686073303, 0.43012210726737976, 0.42198100686073303, 0.4355495274066925, 0.4341926872730255, 0.449118047952652, 0.4667571187019348, 0.447761207818985, 0.4559023082256317, 0.46268656849861145, 0.447761207818985, 0.4559023082256317, 0.43962007761001587, 0.42198100686073303, 0.4586160182952881, 0.4355495274066925, 0.449118047952652, 0.47354137897491455, 0.4572591483592987, 0.4559023082256317, 0.4572591483592987, 0.47489824891090393, 0.43012210726737976, 0.4803256392478943, 0.4464043378829956, 0.4586160182952881, 0.47218453884124756, 0.447761207818985, 0.47354137897491455, 0.4776119291782379, 0.4708276689052582, 0.4776119291782379, 0.4599728584289551, 0.46132972836494446, 0.4572591483592987, 0.47489824891090393, 0.48168250918388367, 0.48575305938720703, 0.49525102972984314, 0.449118047952652, 0.5061058402061462, 0.4708276689052582, 0.4776119291782379, 0.4694708287715912, 0.4884667694568634, 0.4694708287715912, 0.4681139886379242, 0.4789687991142273, 0.4789687991142273, 0.4776119291782379, 0.4762550890445709, 0.4993215799331665, 0.4993215799331665, 0.49253731966018677, 0.47489824891090393, 0.5251017808914185, 0.5020352602005005, 0.4803256392478943, 0.49389415979385376, 0.48439618945121765, 0.5006784200668335, 0.4708276689052582, 0.4979647099971771, 0.48303934931755066, 0.49253731966018677, 0.47489824891090393, 0.49660786986351013, 0.4776119291782379, 0.4993215799331665, 0.49389415979385376, 0.5237449407577515, 0.5305292010307312, 0.4911804497241974, 0.49389415979385376, 0.5047490000724792, 0.49253731966018677, 0.5033921599388123, 0.51560378074646, 0.49525102972984314, 0.5006784200668335, 0.4993215799331665, 0.487109899520874, 0.4776119291782379, 0.514246940612793, 0.5183175206184387, 0.5223880410194397, 0.49389415979385376, 0.487109899520874, 0.5047490000724792, 0.5006784200668335, 0.5033921599388123, 0.514246940612793, 0.49660786986351013, 0.5088195204734802, 0.5237449407577515, 0.5318860411643982, 0.5305292010307312, 0.5183175206184387, 0.5061058402061462, 0.5033921599388123, 0.5183175206184387, 0.511533260345459, 0.5359565615653992, 0.5278154611587524, 0.5183175206184387, 0.5508819818496704, 0.5210312008857727, 0.5345997214317322, 0.5061058402061462, 0.5332428812980652, 0.5291723012924194, 0.511533260345459, 0.5332428812980652, 0.5291723012924194, 0.5373134613037109, 0.5251017808914185, 0.5237449407577515, 0.5196743607521057, 0.5223880410194397, 0.510176420211792, 0.5345997214317322, 0.5400271415710449, 0.5291723012924194, 0.5332428812980652, 0.5318860411643982, 0.5386703014373779, 0.5468114018440247, 0.5427408218383789, 0.5359565615653992, 0.5332428812980652, 0.5468114018440247, 0.5373134613037109, 0.5278154611587524, 0.5359565615653992, 0.5359565615653992, 0.5440977215766907, 0.5359565615653992, 0.5495250821113586, 0.5196743607521057, 0.5468114018440247, 0.5820895433425903, 0.5563093423843384, 0.5359565615653992, 0.5169606804847717, 0.5440977215766907, 0.5495250821113586, 0.5413839817047119, 0.5291723012924194, 0.5468114018440247, 0.5563093423843384, 0.5549525022506714, 0.5440977215766907, 0.5318860411643982, 0.5427408218383789, 0.5440977215766907, 0.5508819818496704, 0.5495250821113586, 0.5807327032089233, 0.5712347626686096, 0.5332428812980652, 0.5454545617103577, 0.5481682419776917, 0.5576662421226501, 0.5373134613037109, 0.5278154611587524, 0.5359565615653992, 0.5508819818496704, 0.5698778629302979, 0.5725916028022766, 0.5400271415710449, 0.5535956621170044, 0.5535956621170044, 0.5481682419776917, 0.5617367625236511, 0.5603799223899841, 0.5522388219833374, 0.5671641826629639, 0.5685210227966309, 0.5508819818496704, 0.5671641826629639, 0.5590230822563171, 0.5712347626686096, 0.5522388219833374, 0.5603799223899841, 0.5780190229415894, 0.5725916028022766, 0.5739484429359436, 0.5630936026573181, 0.5603799223899841, 0.5698778629302979, 0.5549525022506714, 0.5644505023956299, 0.5590230822563171, 0.5848032832145691, 0.5780190229415894, 0.5617367625236511, 0.5535956621170044, 0.5413839817047119, 0.5780190229415894, 0.5848032832145691, 0.5820895433425903, 0.5630936026573181, 0.5725916028022766, 0.5725916028022766, 0.5807327032089233, 0.5793758630752563, 0.5780190229415894, 0.5766621232032776, 0.5698778629302979, 0.5915875434875488, 0.5834463834762573, 0.5888738036155701, 0.5820895433425903, 0.5848032832145691, 0.5820895433425903, 0.5766621232032776, 0.5739484429359436, 0.5807327032089233, 0.5658073425292969, 0.5956580638885498, 0.5820895433425903, 0.5820895433425903, 0.5725916028022766, 0.5793758630752563, 0.5725916028022766, 0.5875169634819031, 0.6010854840278625, 0.5915875434875488, 0.5943012237548828, 0.5888738036155701, 0.6037991642951965, 0.5997286438941956, 0.5820895433425903, 0.5861601233482361, 0.6078697443008423, 0.6065129041671753, 0.6078697443008423, 0.5753052830696106, 0.6024423241615295, 0.5807327032089233, 0.5698778629302979, 0.5780190229415894, 0.611940324306488, 0.5820895433425903, 0.5807327032089233, 0.5902306437492371, 0.5712347626686096, 0.5820895433425903, 0.5766621232032776, 0.6037991642951965, 0.5983718037605286, 0.5970149040222168, 0.6241519451141357, 0.6078697443008423, 0.5943012237548828, 0.5834463834762573, 0.5970149040222168, 0.611940324306488, 0.5875169634819031, 0.5807327032089233, 0.6105834245681763, 0.5617367625236511, 0.5902306437492371, 0.6187245845794678, 0.6010854840278625, 0.6309362053871155, 0.6051560640335083, 0.5956580638885498, 0.5956580638885498, 0.6200814247131348, 0.5929443836212158, 0.6309362053871155, 0.6065129041671753, 0.5915875434875488, 0.617367684841156, 0.6282225251197815, 0.6037991642951965, 0.6268656849861145, 0.617367684841156, 0.613297164440155, 0.6282225251197815, 0.6295793652534485, 0.611940324306488, 0.5915875434875488, 0.6227951049804688, 0.613297164440155, 0.617367684841156, 0.6214382648468018, 0.6092265844345093, 0.613297164440155, 0.6282225251197815, 0.6065129041671753, 0.614654004573822, 0.6309362053871155, 0.614654004573822, 0.6268656849861145, 0.641791045665741, 0.6105834245681763, 0.6187245845794678, 0.6268656849861145, 0.6092265844345093, 0.643147885799408, 0.6214382648468018, 0.6214382648468018, 0.5983718037605286, 0.6078697443008423, 0.6227951049804688, 0.6295793652534485, 0.6065129041671753, 0.6214382648468018, 0.6255088448524475, 0.641791045665741, 0.6255088448524475, 0.6065129041671753, 0.6268656849861145, 0.611940324306488, 0.6255088448524475, 0.6268656849861145, 0.6227951049804688, 0.640434205532074, 0.644504725933075, 0.6214382648468018, 0.644504725933075, 0.6187245845794678, 0.6295793652534485, 0.611940324306488, 0.6282225251197815, 0.6227951049804688, 0.6363636255264282, 0.613297164440155, 0.640434205532074, 0.6336499452590942, 0.6214382648468018, 0.640434205532074, 0.6187245845794678, 0.6214382648468018, 0.6350067853927612, 0.6512889862060547, 0.639077365398407, 0.6350067853927612, 0.639077365398407, 0.6377204656600952, 0.6255088448524475, 0.6295793652534485, 0.6472184658050537, 0.6485753059387207, 0.639077365398407, 0.6526458859443665, 0.6553595662117004, 0.643147885799408, 0.6472184658050537, 0.6499321460723877, 0.6363636255264282, 0.6635006666183472, 0.6485753059387207, 0.639077365398407, 0.6526458859443665, 0.643147885799408, 0.6567164063453674, 0.6607869863510132, 0.6363636255264282, 0.6580732464790344, 0.641791045665741, 0.644504725933075, 0.6512889862060547, 0.6512889862060547, 0.644504725933075, 0.6635006666183472, 0.6540027260780334, 0.6580732464790344, 0.6458616256713867, 0.6553595662117004, 0.6512889862060547, 0.640434205532074, 0.643147885799408, 0.6499321460723877, 0.6580732464790344, 0.6540027260780334, 0.644504725933075, 0.6377204656600952, 0.6567164063453674, 0.6458616256713867, 0.6594301462173462, 0.6485753059387207, 0.6594301462173462, 0.6689280867576599, 0.6607869863510132, 0.6540027260780334, 0.6526458859443665, 0.6458616256713867, 0.6838534474372864, 0.6702849268913269, 0.6580732464790344, 0.6770691871643066, 0.6594301462173462, 0.6662144064903259, 0.6512889862060547, 0.6648575067520142, 0.6743555068969727, 0.6716417670249939, 0.6675712466239929, 0.6567164063453674, 0.6784260272979736, 0.6621438264846802, 0.6702849268913269, 0.6553595662117004, 0.6635006666183472, 0.6729986667633057, 0.6662144064903259, 0.6743555068969727, 0.6635006666183472, 0.6879240274429321, 0.6811397671699524, 0.6635006666183472, 0.6662144064903259, 0.6879240274429321, 0.6824966073036194, 0.6702849268913269, 0.6824966073036194, 0.6716417670249939, 0.6662144064903259, 0.6675712466239929, 0.6662144064903259, 0.6621438264846802, 0.6675712466239929, 0.6648575067520142, 0.6879240274429321, 0.6770691871643066, 0.6797829270362854, 0.6784260272979736, 0.6675712466239929, 0.6797829270362854, 0.6716417670249939, 0.6797829270362854, 0.6743555068969727, 0.6797829270362854, 0.6960651278495789, 0.6770691871643066, 0.6865671873092651, 0.6879240274429321, 0.6757123470306396, 0.6757123470306396, 0.6797829270362854, 0.6757123470306396, 0.6879240274429321, 0.6824966073036194, 0.6648575067520142, 0.6770691871643066, 0.6729986667633057, 0.7014925479888916, 0.6797829270362854, 0.6797829270362854, 0.7014925479888916, 0.6716417670249939, 0.6987788081169128, 0.6974219679832458, 0.7232021689414978, 0.6960651278495789, 0.6919945478439331, 0.6906377077102661, 0.7096336483955383, 0.6729986667633057, 0.7014925479888916, 0.6960651278495789, 0.6947082877159119, 0.6635006666183472, 0.7014925479888916, 0.6933514475822449, 0.7042062282562256, 0.6702849268913269, 0.7055630683898926, 0.7069199681282043, 0.7042062282562256, 0.7028493881225586, 0.6933514475822449, 0.6933514475822449, 0.7109904885292053, 0.7164179086685181, 0.7069199681282043, 0.6919945478439331, 0.6892808675765991, 0.6879240274429321, 0.7028493881225586, 0.6987788081169128, 0.7014925479888916, 0.7069199681282043, 0.7069199681282043, 0.7082768082618713, 0.6974219679832458, 0.6689280867576599, 0.6838534474372864, 0.7096336483955383, 0.7164179086685181, 0.7014925479888916, 0.7150610685348511, 0.6933514475822449, 0.7082768082618713, 0.7123473286628723, 0.719131588935852, 0.6892808675765991, 0.7028493881225586, 0.6933514475822449, 0.6987788081169128, 0.6987788081169128, 0.7218453288078308, 0.7123473286628723, 0.7137042284011841, 0.7204884886741638, 0.6879240274429321, 0.6987788081169128, 0.7299864292144775, 0.7096336483955383, 0.7327001094818115, 0.7286295890808105, 0.7164179086685181, 0.719131588935852, 0.7245590090751648, 0.7137042284011841, 0.7232021689414978, 0.7042062282562256, 0.7340570092201233, 0.6947082877159119, 0.7272727489471436, 0.7096336483955383, 0.7272727489471436, 0.7259158492088318, 0.7245590090751648, 0.7327001094818115, 0.7096336483955383, 0.7204884886741638, 0.7164179086685181, 0.7272727489471436, 0.7164179086685181, 0.6960651278495789, 0.7150610685348511, 0.719131588935852, 0.7123473286628723, 0.7354138493537903, 0.7367706894874573, 0.7367706894874573, 0.7164179086685181, 0.7204884886741638, 0.7177747488021851, 0.7150610685348511, 0.7204884886741638, 0.7245590090751648, 0.7245590090751648, 0.7245590090751648, 0.7272727489471436, 0.7313432693481445, 0.7137042284011841, 0.7259158492088318, 0.7313432693481445, 0.7218453288078308, 0.7272727489471436, 0.7109904885292053, 0.719131588935852, 0.7137042284011841, 0.7123473286628723, 0.7394843697547913, 0.7394843697547913, 0.7611940503120422, 0.740841269493103, 0.7327001094818115, 0.740841269493103, 0.7327001094818115, 0.7367706894874573, 0.7340570092201233, 0.7354138493537903, 0.7354138493537903, 0.74219810962677, 0.7354138493537903, 0.7340570092201233, 0.740841269493103, 0.743554949760437, 0.7354138493537903, 0.7313432693481445, 0.7354138493537903, 0.7232021689414978, 0.7327001094818115, 0.743554949760437, 0.7299864292144775, 0.7611940503120422, 0.740841269493103, 0.7544097900390625, 0.746268630027771, 0.7503392100334167, 0.7313432693481445, 0.7286295890808105, 0.7354138493537903, 0.7598371505737305, 0.7340570092201233, 0.7639077305793762, 0.7476255297660828, 0.7286295890808105, 0.7503392100334167, 0.7503392100334167, 0.767978310585022, 0.7476255297660828, 0.7354138493537903, 0.7530528903007507, 0.7476255297660828, 0.7544097900390625, 0.7625508904457092, 0.7544097900390625, 0.746268630027771, 0.740841269493103, 0.7489823698997498, 0.772048830986023, 0.7354138493537903, 0.7476255297660828, 0.770691990852356, 0.7204884886741638, 0.7489823698997498, 0.7530528903007507, 0.744911789894104, 0.7666214108467102, 0.7788330912590027, 0.767978310585022, 0.7557666301727295, 0.767978310585022, 0.7557666301727295, 0.7557666301727295, 0.7639077305793762, 0.7611940503120422, 0.7476255297660828, 0.7625508904457092, 0.7544097900390625, 0.769335150718689, 0.7489823698997498, 0.7611940503120422, 0.770691990852356, 0.7774762511253357, 0.7747625708580017, 0.7734056711196899, 0.769335150718689, 0.7652645707130432, 0.770691990852356, 0.769335150718689, 0.7544097900390625, 0.7598371505737305, 0.7747625708580017, 0.7530528903007507, 0.7557666301727295, 0.7652645707130432, 0.7666214108467102, 0.7734056711196899, 0.7666214108467102, 0.767978310585022, 0.7544097900390625, 0.767978310585022, 0.7571234703063965, 0.7856173515319824, 0.7815468311309814, 0.7639077305793762, 0.767978310585022, 0.772048830986023, 0.7530528903007507, 0.7489823698997498, 0.7774762511253357, 0.7788330912590027, 0.7747625708580017, 0.7856173515319824, 0.7829036712646484, 0.7666214108467102, 0.743554949760437, 0.7869741916656494, 0.7951153516769409, 0.7910447716712952, 0.7625508904457092, 0.770691990852356, 0.7842605113983154, 0.7761194109916687, 0.770691990852356, 0.7910447716712952, 0.7856173515319824, 0.7747625708580017, 0.7801899313926697, 0.7761194109916687, 0.7734056711196899, 0.8018996119499207, 0.8018996119499207, 0.7896879315376282, 0.7951153516769409, 0.7869741916656494, 0.7869741916656494, 0.7910447716712952, 0.7896879315376282, 0.7747625708580017, 0.769335150718689, 0.7815468311309814, 0.7842605113983154, 0.7964721918106079, 0.7924016118049622, 0.7883310914039612, 0.7937584519386292, 0.7788330912590027, 0.7788330912590027, 0.7937584519386292, 0.7734056711196899, 0.8018996119499207, 0.7747625708580017, 0.7842605113983154, 0.8073269724845886, 0.7924016118049622, 0.7856173515319824, 0.8073269724845886, 0.8046132922172546, 0.7896879315376282, 0.8018996119499207, 0.7869741916656494, 0.7761194109916687, 0.7869741916656494, 0.7937584519386292, 0.7829036712646484, 0.8100407123565674, 0.7910447716712952, 0.8018996119499207, 0.8127543926239014, 0.8018996119499207, 0.7991858720779419, 0.8073269724845886, 0.7951153516769409, 0.7842605113983154, 0.8032564520835876, 0.7978290319442749, 0.8005427122116089, 0.8018996119499207, 0.7978290319442749, 0.8303934931755066, 0.7924016118049622, 0.7991858720779419, 0.8073269724845886, 0.8046132922172546, 0.8181818127632141, 0.8073269724845886, 0.7978290319442749, 0.8127543926239014, 0.7951153516769409, 0.8005427122116089, 0.8100407123565674, 0.8236092329025269, 0.8208954930305481, 0.7978290319442749, 0.7951153516769409, 0.8005427122116089, 0.7964721918106079, 0.8018996119499207, 0.7991858720779419, 0.8059701323509216, 0.8018996119499207, 0.7896879315376282, 0.8073269724845886, 0.8141112327575684, 0.8113975524902344, 0.8100407123565674, 0.8168249726295471, 0.8141112327575684, 0.8195386528968811, 0.8073269724845886, 0.8222523927688599, 0.8141112327575684, 0.8263229131698608, 0.8141112327575684, 0.7951153516769409, 0.8181818127632141, 0.8371777534484863, 0.8168249726295471, 0.8276797533035278, 0.8290366530418396, 0.8208954930305481, 0.8141112327575684, 0.8086838722229004, 0.8127543926239014, 0.8073269724845886, 0.8222523927688599, 0.8032564520835876, 0.8086838722229004, 0.8303934931755066, 0.8249660730361938, 0.8032564520835876, 0.8073269724845886, 0.8168249726295471, 0.8208954930305481, 0.8181818127632141, 0.8331071734428406, 0.8181818127632141, 0.8290366530418396, 0.8453188538551331, 0.8412483334541321, 0.8127543926239014, 0.8236092329025269, 0.8100407123565674, 0.8331071734428406, 0.8317503333091736, 0.8507462739944458, 0.8303934931755066, 0.8141112327575684, 0.8331071734428406, 0.8303934931755066, 0.8331071734428406, 0.8317503333091736, 0.8344640731811523, 0.8168249726295471, 0.8195386528968811, 0.8276797533035278, 0.8290366530418396, 0.8222523927688599, 0.8303934931755066, 0.8249660730361938, 0.8276797533035278, 0.8276797533035278, 0.8249660730361938, 0.8331071734428406, 0.8453188538551331, 0.8439620137214661, 0.8331071734428406, 0.8385345935821533, 0.8466756939888, 0.8276797533035278, 0.8561736941337585, 0.8331071734428406, 0.8371777534484863, 0.8466756939888, 0.8453188538551331, 0.8331071734428406, 0.868385374546051, 0.8344640731811523, 0.8249660730361938, 0.8561736941337585, 0.8358209133148193, 0.8263229131698608, 0.8493894338607788, 0.8317503333091736, 0.8344640731811523, 0.8602442145347595, 0.8588873744010925, 0.8439620137214661, 0.8493894338607788, 0.8426051735877991, 0.8602442145347595, 0.8507462739944458, 0.8236092329025269, 0.8426051735877991, 0.8344640731811523, 0.8426051735877991, 0.8371777534484863, 0.8643147945404053, 0.8629579544067383, 0.8439620137214661, 0.8466756939888, 0.8398914337158203, 0.8371777534484863, 0.8521031141281128, 0.8521031141281128, 0.8548168540000916, 0.8616011142730713, 0.8426051735877991, 0.8466756939888, 0.8493894338607788, 0.8507462739944458, 0.8385345935821533, 0.8534599542617798, 0.8412483334541321, 0.8426051735877991, 0.8412483334541321, 0.868385374546051, 0.8358209133148193, 0.8534599542617798, 0.8507462739944458, 0.8575305342674255, 0.8493894338607788, 0.8616011142730713, 0.8602442145347595, 0.8521031141281128, 0.8521031141281128, 0.8548168540000916, 0.8575305342674255, 0.8751696348190308, 0.8643147945404053, 0.8643147945404053, 0.8765264749526978, 0.8534599542617798, 0.8439620137214661, 0.8765264749526978, 0.8466756939888, 0.8521031141281128, 0.8534599542617798, 0.8643147945404053, 0.8670284748077393, 0.8561736941337585, 0.8548168540000916, 0.8493894338607788, 0.871099054813385, 0.8643147945404053, 0.8778833150863647, 0.8602442145347595, 0.8507462739944458, 0.8778833150863647, 0.8670284748077393, 0.8900949954986572, 0.8588873744010925, 0.8751696348190308, 0.8805969953536987, 0.8602442145347595, 0.869742214679718, 0.8534599542617798, 0.8616011142730713, 0.873812735080719, 0.8602442145347595, 0.8792401552200317, 0.871099054813385, 0.869742214679718, 0.873812735080719, 0.8860244154930115, 0.872455894947052, 0.8670284748077393, 0.8656716346740723, 0.8656716346740723, 0.8805969953536987, 0.8833107352256775, 0.8887381553649902, 0.8643147945404053, 0.8805969953536987, 0.8792401552200317, 0.8873812556266785, 0.8751696348190308, 0.8778833150863647, 0.8792401552200317, 0.8792401552200317, 0.8656716346740723, 0.872455894947052, 0.8860244154930115, 0.8833107352256775, 0.8914518356323242, 0.898236095905304, 0.8670284748077393, 0.872455894947052, 0.872455894947052, 0.8887381553649902, 0.8860244154930115, 0.8656716346740723, 0.8833107352256775, 0.8792401552200317, 0.869742214679718, 0.8805969953536987, 0.8819538950920105, 0.8860244154930115, 0.8765264749526978, 0.8860244154930115, 0.9036635160446167, 0.896879255771637, 0.8887381553649902, 0.8928086757659912, 0.8833107352256775, 0.8914518356323242, 0.873812735080719, 0.8873812556266785, 0.899592936038971, 0.8833107352256775, 0.8792401552200317, 0.898236095905304, 0.8914518356323242, 0.89552241563797, 0.8751696348190308, 0.8792401552200317, 0.8941655158996582, 0.896879255771637, 0.8914518356323242, 0.899592936038971, 0.8860244154930115, 0.8778833150863647, 0.8887381553649902, 0.8900949954986572, 0.9023066759109497, 0.899592936038971, 0.899592936038971, 0.9077340364456177, 0.899592936038971, 0.8900949954986572, 0.8887381553649902, 0.8819538950920105, 0.8860244154930115, 0.8765264749526978, 0.8914518356323242, 0.8900949954986572, 0.8873812556266785, 0.9023066759109497, 0.898236095905304, 0.8805969953536987, 0.8900949954986572, 0.8792401552200317, 0.9036635160446167, 0.8887381553649902, 0.8873812556266785, 0.9063771963119507, 0.9063771963119507, 0.8900949954986572, 0.9009497761726379, 0.8928086757659912, 0.8887381553649902, 0.899592936038971, 0.89552241563797, 0.8900949954986572, 0.9077340364456177, 0.9050203561782837, 0.8941655158996582, 0.8846675753593445, 0.8819538950920105, 0.9036635160446167, 0.8928086757659912, 0.8860244154930115, 0.89552241563797, 0.8914518356323242, 0.9063771963119507, 0.89552241563797, 0.8778833150863647, 0.9131614565849304, 0.9172320365905762, 0.9145182967185974, 0.9213025569915771, 0.89552241563797, 0.8846675753593445, 0.9077340364456177, 0.9023066759109497, 0.9077340364456177, 0.9050203561782837, 0.8941655158996582, 0.9063771963119507, 0.899592936038971, 0.9023066759109497, 0.898236095905304, 0.9023066759109497, 0.8887381553649902, 0.8941655158996582, 0.898236095905304, 0.896879255771637, 0.8928086757659912, 0.9185888767242432, 0.8914518356323242, 0.9131614565849304, 0.9199457168579102, 0.9185888767242432, 0.9199457168579102, 0.9090909361839294, 0.9213025569915771, 0.9009497761726379, 0.9009497761726379, 0.9077340364456177, 0.9253731369972229, 0.9090909361839294, 0.9145182967185974, 0.9172320365905762, 0.8914518356323242, 0.9090909361839294, 0.9172320365905762, 0.899592936038971, 0.9185888767242432, 0.9063771963119507, 0.9118046164512634, 0.9267299771308899, 0.9172320365905762, 0.9023066759109497, 0.9104477763175964, 0.9240162968635559, 0.898236095905304, 0.9280868172645569, 0.9199457168579102, 0.9185888767242432, 0.9185888767242432, 0.9199457168579102, 0.9213025569915771, 0.9172320365905762, 0.9131614565849304, 0.9226594567298889, 0.9172320365905762, 0.9104477763175964, 0.9145182967185974, 0.896879255771637, 0.9199457168579102, 0.9226594567298889, 0.9118046164512634, 0.9185888767242432, 0.9226594567298889, 0.9104477763175964, 0.9145182967185974, 0.9063771963119507, 0.9158751964569092, 0.9348710775375366, 0.9158751964569092, 0.9213025569915771, 0.9308005571365356, 0.9267299771308899, 0.9050203561782837, 0.9240162968635559, 0.9131614565849304, 0.9253731369972229, 0.9267299771308899, 0.9213025569915771, 0.9240162968635559, 0.9267299771308899, 0.9158751964569092, 0.9199457168579102, 0.9321573972702026, 0.9145182967185974, 0.9280868172645569, 0.9185888767242432, 0.9375848174095154, 0.9158751964569092, 0.9226594567298889, 0.9267299771308899, 0.9172320365905762, 0.9348710775375366, 0.9172320365905762, 0.9280868172645569, 0.9226594567298889, 0.9253731369972229, 0.9267299771308899, 0.9240162968635559, 0.9280868172645569, 0.9362279772758484, 0.9172320365905762, 0.9199457168579102, 0.9280868172645569, 0.9280868172645569, 0.9240162968635559, 0.9090909361839294, 0.9280868172645569, 0.9294437170028687, 0.9294437170028687, 0.9226594567298889, 0.9294437170028687, 0.9348710775375366, 0.9253731369972229, 0.9158751964569092, 0.9240162968635559, 0.9158751964569092, 0.9362279772758484, 0.9294437170028687, 0.9335142374038696, 0.9267299771308899, 0.9335142374038696, 0.9348710775375366, 0.9389416575431824, 0.9402984976768494, 0.9362279772758484, 0.9375848174095154, 0.9375848174095154, 0.9294437170028687, 0.9389416575431824, 0.9199457168579102, 0.9348710775375366, 0.9131614565849304, 0.9145182967185974, 0.9416553378105164, 0.9430122375488281, 0.9375848174095154, 0.9565807580947876, 0.9484395980834961, 0.9335142374038696, 0.9525101780891418, 0.9267299771308899, 0.9294437170028687, 0.9321573972702026, 0.9470827579498291, 0.9430122375488281, 0.9457259178161621, 0.9416553378105164, 0.9443690776824951, 0.9430122375488281, 0.9389416575431824, 0.9375848174095154, 0.9321573972702026, 0.9321573972702026, 0.9362279772758484, 0.9457259178161621, 0.9497964978218079, 0.9308005571365356, 0.9375848174095154, 0.9348710775375366, 0.9402984976768494, 0.9497964978218079, 0.9362279772758484, 0.9294437170028687, 0.9402984976768494, 0.9525101780891418, 0.9565807580947876, 0.9470827579498291, 0.9552238583564758, 0.9443690776824951, 0.9443690776824951, 0.9443690776824951, 0.9552238583564758, 0.9280868172645569, 0.9375848174095154, 0.9430122375488281, 0.9308005571365356, 0.9443690776824951, 0.9294437170028687, 0.9470827579498291, 0.9552238583564758, 0.9470827579498291, 0.9552238583564758, 0.9470827579498291, 0.9362279772758484, 0.9484395980834961, 0.9538670182228088, 0.9511533379554749, 0.9416553378105164, 0.9525101780891418, 0.9416553378105164, 0.9430122375488281, 0.9375848174095154, 0.9402984976768494, 0.9497964978218079, 0.9538670182228088, 0.9430122375488281, 0.9497964978218079, 0.9538670182228088, 0.9389416575431824, 0.9362279772758484, 0.9484395980834961, 0.9497964978218079, 0.9443690776824951, 0.9457259178161621, 0.9389416575431824, 0.9633650183677673, 0.9457259178161621, 0.9633650183677673, 0.9620081186294556, 0.9457259178161621, 0.9484395980834961, 0.9484395980834961, 0.9497964978218079, 0.9592944383621216, 0.9430122375488281, 0.9457259178161621, 0.9552238583564758, 0.9606512784957886, 0.9457259178161621, 0.9579375982284546, 0.9620081186294556, 0.9335142374038696, 0.9525101780891418, 0.9592944383621216, 0.9660786986351013, 0.9457259178161621, 0.9579375982284546, 0.9701492786407471, 0.9565807580947876, 0.9538670182228088, 0.9484395980834961, 0.9579375982284546, 0.9416553378105164, 0.9457259178161621, 0.9647218585014343, 0.9470827579498291, 0.9565807580947876, 0.9470827579498291, 0.9620081186294556, 0.9430122375488281, 0.9525101780891418, 0.9579375982284546, 0.9647218585014343, 0.9633650183677673, 0.9660786986351013, 0.9362279772758484, 0.9660786986351013, 0.9538670182228088, 0.9538670182228088, 0.9633650183677673, 0.9457259178161621, 0.9525101780891418, 0.9633650183677673, 0.9511533379554749, 0.9511533379554749, 0.9647218585014343, 0.9633650183677673, 0.9647218585014343, 0.9430122375488281, 0.9592944383621216, 0.9633650183677673, 0.9402984976768494, 0.9633650183677673, 0.9579375982284546, 0.9538670182228088, 0.9457259178161621, 0.9620081186294556, 0.9620081186294556, 0.9470827579498291, 0.9592944383621216, 0.9620081186294556, 0.9620081186294556, 0.9511533379554749, 0.9660786986351013, 0.9715061187744141, 0.9620081186294556, 0.9687923789024353, 0.9538670182228088, 0.9633650183677673, 0.9647218585014343, 0.9552238583564758, 0.9552238583564758, 0.9606512784957886, 0.9552238583564758, 0.9620081186294556, 0.9660786986351013, 0.9579375982284546, 0.9674355387687683, 0.9620081186294556, 0.9579375982284546, 0.972862958908081, 0.9592944383621216, 0.9633650183677673, 0.9715061187744141, 0.9538670182228088, 0.9715061187744141, 0.9660786986351013, 0.9606512784957886, 0.9701492786407471, 0.9674355387687683, 0.9687923789024353, 0.9660786986351013, 0.9674355387687683, 0.9633650183677673, 0.9674355387687683, 0.9647218585014343, 0.9606512784957886, 0.9674355387687683, 0.9511533379554749, 0.9647218585014343, 0.9606512784957886, 0.9715061187744141, 0.9715061187744141, 0.9715061187744141, 0.9633650183677673, 0.9606512784957886, 0.9552238583564758, 0.972862958908081, 0.974219799041748, 0.9674355387687683, 0.9674355387687683, 0.9633650183677673, 0.9674355387687683, 0.9715061187744141, 0.9660786986351013, 0.9633650183677673, 0.9565807580947876, 0.9701492786407471, 0.9565807580947876, 0.9620081186294556, 0.974219799041748, 0.9769335389137268, 0.9701492786407471, 0.9674355387687683, 0.972862958908081, 0.9782903790473938, 0.9796472191810608, 0.9701492786407471, 0.9782903790473938, 0.972862958908081, 0.9715061187744141, 0.9715061187744141, 0.9810040593147278, 0.9769335389137268, 0.9715061187744141, 0.9701492786407471, 0.9782903790473938, 0.9687923789024353, 0.972862958908081, 0.9687923789024353, 0.9782903790473938, 0.9660786986351013, 0.9769335389137268, 0.9633650183677673, 0.9660786986351013, 0.9715061187744141, 0.974219799041748, 0.9633650183677673, 0.9674355387687683, 0.9633650183677673, 0.9715061187744141, 0.972862958908081, 0.9715061187744141, 0.9674355387687683, 0.972862958908081, 0.9620081186294556, 0.972862958908081, 0.9823608994483948, 0.975576639175415, 0.975576639175415, 0.9647218585014343, 0.9769335389137268, 0.9674355387687683, 0.974219799041748, 0.9579375982284546, 0.9769335389137268, 0.974219799041748, 0.975576639175415, 0.9837177991867065, 0.9810040593147278, 0.9782903790473938, 0.9647218585014343, 0.9823608994483948, 0.975576639175415, 0.9782903790473938, 0.9864314794540405, 0.9660786986351013, 0.9660786986351013, 0.9769335389137268, 0.972862958908081, 0.974219799041748, 0.972862958908081, 0.9796472191810608, 0.9782903790473938, 0.975576639175415, 0.975576639175415, 0.9769335389137268, 0.975576639175415, 0.974219799041748, 0.975576639175415, 0.9620081186294556, 0.9715061187744141, 0.9796472191810608, 0.9796472191810608, 0.974219799041748, 0.972862958908081, 0.974219799041748, 0.974219799041748, 0.9877883195877075, 0.9782903790473938, 0.9810040593147278, 0.9769335389137268, 0.9837177991867065, 0.9660786986351013, 0.9715061187744141, 0.9769335389137268, 0.9701492786407471, 0.9769335389137268, 0.975576639175415, 0.9810040593147278, 0.9687923789024353, 0.9837177991867065, 0.974219799041748, 0.9769335389137268, 0.9769335389137268, 0.9715061187744141, 0.9864314794540405, 0.9715061187744141, 0.9769335389137268, 0.9850746393203735, 0.9769335389137268, 0.9823608994483948, 0.9687923789024353, 0.9877883195877075, 0.9769335389137268, 0.9647218585014343, 0.9810040593147278, 0.9823608994483948, 0.9769335389137268, 0.9823608994483948, 0.9796472191810608, 0.975576639175415, 0.972862958908081, 0.9674355387687683, 0.9850746393203735, 0.9715061187744141, 0.9796472191810608, 0.9782903790473938, 0.9782903790473938, 0.9891451597213745, 0.9782903790473938, 0.9782903790473938, 0.975576639175415, 0.9864314794540405, 0.9796472191810608, 0.9823608994483948, 0.975576639175415, 0.974219799041748, 0.9810040593147278, 0.9837177991867065, 0.9796472191810608, 0.9810040593147278, 0.9837177991867065, 0.974219799041748, 0.9715061187744141, 0.9837177991867065, 0.9796472191810608, 0.9782903790473938, 0.9864314794540405, 0.9796472191810608, 0.9823608994483948, 0.9891451597213745, 0.9877883195877075, 0.975576639175415, 0.9850746393203735, 0.9877883195877075, 0.9850746393203735, 0.9864314794540405, 0.9837177991867065, 0.9782903790473938, 0.9864314794540405, 0.974219799041748, 0.9782903790473938, 0.9837177991867065, 0.9796472191810608, 0.9823608994483948, 0.9918588995933533, 0.9864314794540405, 0.9837177991867065, 0.9905020594596863, 0.9782903790473938, 0.9837177991867065, 0.9823608994483948, 0.975576639175415, 0.9877883195877075, 0.972862958908081, 0.9823608994483948, 0.9864314794540405, 0.9823608994483948, 0.9837177991867065, 0.9810040593147278, 0.9877883195877075, 0.9810040593147278, 0.9850746393203735, 0.9864314794540405, 0.9877883195877075, 0.9837177991867065, 0.9837177991867065, 0.9932157397270203, 0.9891451597213745, 0.9810040593147278, 0.9864314794540405, 0.9864314794540405, 0.9837177991867065, 0.9877883195877075, 0.9877883195877075, 0.9891451597213745, 0.975576639175415, 0.9877883195877075, 0.9810040593147278, 0.9877883195877075, 0.9864314794540405, 0.9796472191810608, 0.9769335389137268, 0.9891451597213745, 0.9932157397270203, 0.972862958908081, 0.9932157397270203, 0.9810040593147278, 0.9823608994483948, 0.9877883195877075, 0.9837177991867065, 0.9945725798606873, 0.9850746393203735, 0.9891451597213745, 0.9796472191810608, 0.9837177991867065, 0.9837177991867065, 0.9918588995933533, 0.9864314794540405, 0.9932157397270203, 0.9823608994483948, 0.9864314794540405, 0.9837177991867065, 0.9864314794540405, 0.9959294199943542, 0.9810040593147278, 0.9918588995933533, 0.9877883195877075, 0.9877883195877075, 0.9891451597213745, 0.9959294199943542, 0.9905020594596863, 0.9864314794540405, 0.9905020594596863, 0.9823608994483948, 0.9877883195877075, 0.9891451597213745, 0.9918588995933533, 0.9837177991867065, 0.9796472191810608, 0.9850746393203735, 0.9877883195877075, 0.9905020594596863, 0.9837177991867065, 0.9877883195877075, 0.9959294199943542, 0.9959294199943542, 0.9877883195877075, 0.9877883195877075, 0.9810040593147278, 0.9905020594596863, 0.9850746393203735, 0.9864314794540405, 0.9932157397270203, 0.9810040593147278, 0.9932157397270203, 0.9891451597213745, 0.9877883195877075, 0.9905020594596863, 0.9905020594596863, 0.9918588995933533, 0.9918588995933533, 0.9864314794540405, 0.9891451597213745, 0.9823608994483948, 0.9891451597213745, 0.9891451597213745, 0.9837177991867065, 0.9918588995933533, 0.9891451597213745, 0.9877883195877075, 0.9850746393203735, 0.9864314794540405, 0.9905020594596863, 0.9782903790473938, 0.9905020594596863, 0.9850746393203735, 0.9837177991867065, 0.9837177991867065, 0.9877883195877075, 0.9891451597213745, 0.9945725798606873, 0.9864314794540405, 0.9864314794540405, 0.9918588995933533, 0.9864314794540405, 0.9945725798606873, 0.9932157397270203, 0.9905020594596863, 0.9877883195877075, 0.9905020594596863, 0.9905020594596863, 0.9932157397270203, 0.9837177991867065, 0.9877883195877075, 0.9877883195877075, 0.9905020594596863, 0.9932157397270203, 0.9932157397270203, 0.9823608994483948, 0.9905020594596863, 0.9877883195877075, 0.9877883195877075, 0.9905020594596863, 0.9850746393203735, 0.9959294199943542, 0.9959294199943542, 0.9918588995933533, 0.9932157397270203, 0.9945725798606873, 0.9891451597213745, 0.9864314794540405, 0.9864314794540405, 0.9837177991867065, 0.9945725798606873, 0.9891451597213745, 0.9918588995933533, 0.9850746393203735, 0.9932157397270203, 0.9918588995933533, 0.9850746393203735, 0.9837177991867065, 0.9877883195877075, 0.9918588995933533, 0.9932157397270203, 0.9945725798606873, 0.9891451597213745, 0.9932157397270203, 0.9877883195877075, 0.9905020594596863, 0.9945725798606873, 0.9918588995933533, 0.9864314794540405, 0.9905020594596863, 0.9945725798606873, 0.9877883195877075, 0.9810040593147278, 0.9918588995933533, 0.9905020594596863, 0.9918588995933533, 0.9877883195877075, 0.9932157397270203, 0.9850746393203735, 0.9932157397270203, 0.9918588995933533, 0.9891451597213745, 0.9905020594596863, 0.9918588995933533, 0.9959294199943542, 0.9850746393203735, 0.9932157397270203, 0.9918588995933533, 0.9891451597213745, 0.9945725798606873, 0.9905020594596863, 0.997286319732666, 0.9945725798606873, 0.9918588995933533, 0.9891451597213745, 0.9918588995933533, 0.9932157397270203, 0.9918588995933533, 0.9905020594596863, 0.9918588995933533, 0.9877883195877075, 0.9945725798606873, 0.9918588995933533, 0.9932157397270203, 0.9905020594596863, 0.9945725798606873, 0.9877883195877075, 0.9959294199943542, 0.9959294199943542, 0.9905020594596863, 0.9864314794540405, 0.9905020594596863, 0.9932157397270203, 0.9905020594596863, 0.9877883195877075, 0.9864314794540405, 0.997286319732666, 0.9932157397270203, 0.9891451597213745, 0.9932157397270203, 0.9945725798606873, 0.9905020594596863, 0.9959294199943542, 0.9877883195877075, 0.9945725798606873, 0.997286319732666, 0.9945725798606873, 0.9905020594596863, 0.9891451597213745, 0.9959294199943542, 0.9945725798606873, 0.9945725798606873, 0.9918588995933533, 0.9877883195877075, 0.9877883195877075, 0.998643159866333, 0.9905020594596863, 0.9945725798606873, 0.9932157397270203, 0.9945725798606873, 0.998643159866333, 0.9918588995933533, 0.9891451597213745, 0.9932157397270203, 0.9932157397270203, 0.997286319732666, 0.9918588995933533, 0.9850746393203735, 0.9959294199943542, 0.9945725798606873, 0.9932157397270203, 0.9945725798606873, 0.9945725798606873, 0.9891451597213745, 0.9945725798606873, 0.9945725798606873, 0.9932157397270203, 0.9918588995933533, 0.9945725798606873, 0.9932157397270203, 0.9932157397270203, 0.9918588995933533, 0.9905020594596863, 0.9891451597213745, 0.9918588995933533, 0.9945725798606873, 0.9959294199943542, 0.9945725798606873, 0.9918588995933533, 0.9891451597213745, 0.9945725798606873, 0.9918588995933533, 0.9959294199943542, 0.9932157397270203, 0.997286319732666, 0.9932157397270203, 0.9918588995933533, 0.9945725798606873, 0.998643159866333, 0.9918588995933533, 0.9945725798606873, 0.9945725798606873, 0.9918588995933533, 0.9932157397270203, 0.9918588995933533, 0.997286319732666, 0.9918588995933533, 0.9932157397270203, 0.9959294199943542, 0.9891451597213745, 0.997286319732666, 0.9918588995933533, 0.9945725798606873, 0.9959294199943542, 0.9959294199943542, 0.9905020594596863, 0.9877883195877075, 0.997286319732666, 0.9918588995933533, 0.9932157397270203, 0.9918588995933533, 0.998643159866333, 0.9945725798606873, 0.9945725798606873, 0.9932157397270203, 0.9918588995933533, 0.9932157397270203, 0.9864314794540405, 0.9945725798606873, 0.9959294199943542, 0.9891451597213745, 0.9959294199943542, 0.9932157397270203, 0.9959294199943542, 0.9918588995933533, 0.9959294199943542, 0.9959294199943542, 0.9918588995933533, 0.9905020594596863, 0.9945725798606873, 0.997286319732666, 0.9959294199943542, 0.9945725798606873, 0.9945725798606873, 0.9932157397270203, 0.998643159866333, 0.9945725798606873, 0.997286319732666, 0.997286319732666, 0.9945725798606873, 0.9891451597213745, 0.997286319732666, 0.9918588995933533, 0.9945725798606873, 0.9932157397270203, 0.9959294199943542, 0.9945725798606873, 0.9959294199943542, 0.9932157397270203, 0.9918588995933533, 0.9932157397270203, 0.9959294199943542, 0.9918588995933533, 0.9905020594596863], 'val_loss': [2.3075475692749023, 2.28119158744812, 2.257012128829956, 2.254160165786743, 2.238717794418335, 2.230532169342041, 2.216188430786133, 2.2116239070892334, 2.2006866931915283, 2.1967525482177734, 2.1856513023376465, 2.1774356365203857, 2.1756181716918945, 2.169583797454834, 2.1664726734161377, 2.1538138389587402, 2.153923273086548, 2.1430959701538086, 2.132913112640381, 2.1264915466308594, 2.1199347972869873, 2.112121820449829, 2.1088457107543945, 2.102379083633423, 2.090294122695923, 2.085005283355713, 2.073394298553467, 2.0757956504821777, 2.058246612548828, 2.0606870651245117, 2.0632336139678955, 2.0460424423217773, 2.0475471019744873, 2.0440468788146973, 2.0337376594543457, 2.0302603244781494, 2.018199920654297, 1.9984585046768188, 2.0046300888061523, 1.999977946281433, 1.9939968585968018, 1.9854134321212769, 1.9776771068572998, 1.9738279581069946, 1.9712938070297241, 1.9696108102798462, 1.9571632146835327, 1.954525113105774, 1.943193793296814, 1.9415308237075806, 1.941127061843872, 1.9352309703826904, 1.9283350706100464, 1.9287430047988892, 1.9237109422683716, 1.9258593320846558, 1.9118773937225342, 1.9086323976516724, 1.8970565795898438, 1.896944284439087, 1.8923527002334595, 1.8930926322937012, 1.8921679258346558, 1.8797050714492798, 1.874916434288025, 1.8761401176452637, 1.8737972974777222, 1.8778810501098633, 1.864969253540039, 1.879824161529541, 1.8495566844940186, 1.8530949354171753, 1.845083475112915, 1.849384069442749, 1.844753623008728, 1.8460345268249512, 1.8466743230819702, 1.8494455814361572, 1.8460851907730103, 1.8465086221694946, 1.840854525566101, 1.8220155239105225, 1.8278181552886963, 1.8292187452316284, 1.8214541673660278, 1.817642331123352, 1.8149327039718628, 1.8131605386734009, 1.8129167556762695, 1.8066579103469849, 1.812032699584961, 1.8076151609420776, 1.8060599565505981, 1.8024362325668335, 1.8112808465957642, 1.7955501079559326, 1.800520420074463, 1.7972575426101685, 1.7955037355422974, 1.7853189706802368, 1.798343539237976, 1.7843924760818481, 1.7797141075134277, 1.7751787900924683, 1.778209924697876, 1.7828994989395142, 1.7864999771118164, 1.7797396183013916, 1.7774057388305664, 1.7702234983444214, 1.757954716682434, 1.770921230316162, 1.7684638500213623, 1.760229229927063, 1.755826711654663, 1.7578485012054443, 1.751722812652588, 1.7440115213394165, 1.753159999847412, 1.7632797956466675, 1.738739013671875, 1.7635685205459595, 1.7360602617263794, 1.7427966594696045, 1.745591163635254, 1.7333669662475586, 1.7282114028930664, 1.730587124824524, 1.7334144115447998, 1.7293671369552612, 1.7310247421264648, 1.73041570186615, 1.7341108322143555, 1.7226876020431519, 1.719754934310913, 1.7403535842895508, 1.7111717462539673, 1.7142817974090576, 1.7134137153625488, 1.7091529369354248, 1.7138174772262573, 1.70587158203125, 1.7066737413406372, 1.7078580856323242, 1.69549560546875, 1.6878515481948853, 1.6865688562393188, 1.6867128610610962, 1.6793159246444702, 1.6856679916381836, 1.6758873462677002, 1.676151156425476, 1.6762068271636963, 1.7091556787490845, 1.6722105741500854, 1.6666516065597534, 1.6652553081512451, 1.6702871322631836, 1.670122504234314, 1.6719459295272827, 1.6577365398406982, 1.662393569946289, 1.653193712234497, 1.660542607307434, 1.6590299606323242, 1.6525957584381104, 1.6710535287857056, 1.6439807415008545, 1.6453453302383423, 1.638384222984314, 1.6396032571792603, 1.6307069063186646, 1.6388828754425049, 1.6481150388717651, 1.6365410089492798, 1.6367723941802979, 1.6353459358215332, 1.620794415473938, 1.6176139116287231, 1.6320546865463257, 1.6153063774108887, 1.6067167520523071, 1.6038296222686768, 1.6111195087432861, 1.613081693649292, 1.6264314651489258, 1.5974689722061157, 1.608067274093628, 1.6031523942947388, 1.606948971748352, 1.601431131362915, 1.5877552032470703, 1.587034821510315, 1.6004570722579956, 1.5972884893417358, 1.590518832206726, 1.584314227104187, 1.6108105182647705, 1.5932676792144775, 1.5896509885787964, 1.588308334350586, 1.5843174457550049, 1.578878402709961, 1.579551100730896, 1.5923194885253906, 1.5772284269332886, 1.5816338062286377, 1.564134955406189, 1.586178183555603, 1.5917190313339233, 1.5590543746948242, 1.5974665880203247, 1.5807090997695923, 1.5980466604232788, 1.5580579042434692, 1.5775330066680908, 1.585243821144104, 1.5691401958465576, 1.5527470111846924, 1.5725687742233276, 1.5522502660751343, 1.5750389099121094, 1.5768169164657593, 1.5740796327590942, 1.559687852859497, 1.589100956916809, 1.5646260976791382, 1.5474860668182373, 1.5518510341644287, 1.5583696365356445, 1.549333930015564, 1.5903218984603882, 1.5474276542663574, 1.5563955307006836, 1.5419141054153442, 1.5520436763763428, 1.5540151596069336, 1.5431897640228271, 1.564073085784912, 1.5654338598251343, 1.5588963031768799, 1.5320221185684204, 1.5324597358703613, 1.5431655645370483, 1.5368582010269165, 1.5348798036575317, 1.5257458686828613, 1.5338473320007324, 1.5302621126174927, 1.5283273458480835, 1.5693681240081787, 1.5246021747589111, 1.5358716249465942, 1.5443812608718872, 1.5233652591705322, 1.5533297061920166, 1.5231448411941528, 1.5258078575134277, 1.5481127500534058, 1.5243805646896362, 1.515842318534851, 1.5333225727081299, 1.5193754434585571, 1.5294239521026611, 1.531530499458313, 1.5229835510253906, 1.5195329189300537, 1.5238614082336426, 1.5172605514526367, 1.5256657600402832, 1.5786449909210205, 1.5240403413772583, 1.5321476459503174, 1.5831172466278076, 1.5081226825714111, 1.5283409357070923, 1.5469465255737305, 1.5171458721160889, 1.51809561252594, 1.536954641342163, 1.554745078086853, 1.5196491479873657, 1.5190294981002808, 1.5235439538955688, 1.5401698350906372, 1.5176254510879517, 1.5790948867797852, 1.5183703899383545, 1.5123661756515503, 1.5105420351028442, 1.5099207162857056, 1.5015895366668701, 1.5098727941513062, 1.5433242321014404, 1.5048531293869019, 1.4992330074310303, 1.5100754499435425, 1.502764105796814, 1.5048331022262573, 1.5038886070251465, 1.5079082250595093, 1.5111184120178223, 1.5044759511947632, 1.5051482915878296, 1.5061498880386353, 1.4986662864685059, 1.4964169263839722, 1.5129364728927612, 1.4968303442001343, 1.5025601387023926, 1.5092464685440063, 1.5103408098220825, 1.496738314628601, 1.494215965270996, 1.4933021068572998, 1.5215729475021362, 1.54153573513031, 1.5076195001602173, 1.5100470781326294, 1.4910778999328613, 1.4983206987380981, 1.487493634223938, 1.5021415948867798, 1.5185917615890503, 1.4915817975997925, 1.4887949228286743, 1.4872043132781982, 1.5301508903503418, 1.541170358657837, 1.491046667098999, 1.5458250045776367, 1.4817371368408203, 1.5556162595748901, 1.4902163743972778, 1.4807525873184204, 1.4930617809295654, 1.5092791318893433, 1.486246109008789, 1.4829294681549072, 1.4978810548782349, 1.4865010976791382, 1.481038212776184, 1.5018603801727295, 1.4777534008026123, 1.4779499769210815, 1.4795656204223633, 1.5102349519729614, 1.5388760566711426, 1.5111277103424072, 1.4813332557678223, 1.5196576118469238, 1.4814453125, 1.4862717390060425, 1.5034551620483398, 1.484945297241211, 1.4734337329864502, 1.5230258703231812, 1.4863941669464111, 1.4715462923049927, 1.4815782308578491, 1.4785585403442383, 1.497964859008789, 1.4812473058700562, 1.4766508340835571, 1.495859980583191, 1.481967568397522, 1.4725351333618164, 1.4641542434692383, 1.4704269170761108, 1.4748278856277466, 1.4747731685638428, 1.4771143198013306, 1.4641121625900269, 1.4901793003082275, 1.467027187347412, 1.5021388530731201, 1.459719181060791, 1.498632550239563, 1.468644618988037, 1.457655668258667, 1.4735256433486938, 1.485799789428711, 1.4959596395492554, 1.4710973501205444, 1.5123486518859863, 1.5221917629241943, 1.4677296876907349, 1.4610239267349243, 1.4666279554367065, 1.5272852182388306, 1.5195835828781128, 1.4653592109680176, 1.4712847471237183, 1.479564905166626, 1.462783694267273, 1.4598844051361084, 1.4705479145050049, 1.46420156955719, 1.5041520595550537, 1.4683514833450317, 1.4586381912231445, 1.4470561742782593, 1.4781330823898315, 1.4957082271575928, 1.4604405164718628, 1.4629279375076294, 1.4733071327209473, 1.470534086227417, 1.4516955614089966, 1.450100302696228, 1.4942994117736816, 1.4563828706741333, 1.4602159261703491, 1.4524402618408203, 1.4655181169509888, 1.4545605182647705, 1.5444315671920776, 1.4475529193878174, 1.45028817653656, 1.4893264770507812, 1.4782748222351074, 1.4479163885116577, 1.4626151323318481, 1.457535982131958, 1.493391990661621, 1.4513373374938965, 1.5295648574829102, 1.4629062414169312, 1.4644616842269897, 1.4505698680877686, 1.4519364833831787, 1.4503403902053833, 1.4450474977493286, 1.4699548482894897, 1.450575590133667, 1.469030499458313, 1.454256534576416, 1.499258041381836, 1.4535318613052368, 1.4640690088272095, 1.4523248672485352, 1.4707533121109009, 1.4479141235351562, 1.4640321731567383, 1.4650325775146484, 1.475162386894226, 1.4476597309112549, 1.4816317558288574, 1.4750967025756836, 1.4812827110290527, 1.4837499856948853, 1.4566010236740112, 1.4640315771102905, 1.4420595169067383, 1.4387661218643188, 1.469590187072754, 1.4405220746994019, 1.437450647354126, 1.4646267890930176, 1.4452093839645386, 1.4567524194717407, 1.4519357681274414, 1.4305286407470703, 1.4914700984954834, 1.4454152584075928, 1.5204404592514038, 1.449973702430725, 1.4393177032470703, 1.477285623550415, 1.4326636791229248, 1.4399057626724243, 1.4571696519851685, 1.4526584148406982, 1.4530004262924194, 1.4346387386322021, 1.4714995622634888, 1.4410144090652466, 1.453054666519165, 1.4401650428771973, 1.4528698921203613, 1.4337221384048462, 1.4501843452453613, 1.4606235027313232, 1.4561750888824463, 1.4346455335617065, 1.4398539066314697, 1.4323056936264038, 1.4416320323944092, 1.4414377212524414, 1.4773585796356201, 1.4654276371002197, 1.4337570667266846, 1.4317392110824585, 1.4757009744644165, 1.435414433479309, 1.4306180477142334, 1.4479951858520508, 1.4377068281173706, 1.4452418088912964, 1.433902621269226, 1.42569100856781, 1.4838286638259888, 1.423707365989685, 1.4224748611450195, 1.4326565265655518, 1.4338592290878296, 1.4477075338363647, 1.4278841018676758, 1.4313838481903076, 1.438704013824463, 1.4420329332351685, 1.422115683555603, 1.4664829969406128, 1.4271713495254517, 1.4463412761688232, 1.4345695972442627, 1.4478850364685059, 1.4495279788970947, 1.4414212703704834, 1.4490721225738525, 1.4258166551589966, 1.4482648372650146, 1.4455077648162842, 1.4442062377929688, 1.5008093118667603, 1.4289854764938354, 1.4413022994995117, 1.4311472177505493, 1.4291281700134277, 1.4258856773376465, 1.4366364479064941, 1.4193099737167358, 1.4606988430023193, 1.4331879615783691, 1.4303112030029297, 1.418826699256897, 1.4174095392227173, 1.4184163808822632, 1.4180728197097778, 1.4148567914962769, 1.4482132196426392, 1.4668636322021484, 1.4242286682128906, 1.4196974039077759, 1.4424388408660889, 1.4790829420089722, 1.4282346963882446, 1.4616535902023315, 1.41730535030365, 1.4230149984359741, 1.534753680229187, 1.4248263835906982, 1.4203014373779297, 1.4241199493408203, 1.424051284790039, 1.4179164171218872, 1.4191434383392334, 1.4417961835861206, 1.4326741695404053, 1.4167537689208984, 1.4682579040527344, 1.4224493503570557, 1.4256386756896973, 1.4762438535690308, 1.4242266416549683, 1.433821439743042, 1.4520727396011353, 1.4186241626739502, 1.4144670963287354, 1.4346702098846436, 1.4141631126403809, 1.4144823551177979, 1.4307007789611816, 1.466939091682434, 1.4102373123168945, 1.4439903497695923, 1.4226020574569702, 1.409720778465271, 1.4085650444030762, 1.4179500341415405, 1.4061195850372314, 1.4175424575805664, 1.4269919395446777, 1.429771900177002, 1.4305592775344849, 1.4263043403625488, 1.4229252338409424, 1.4126026630401611, 1.444359302520752, 1.4300026893615723, 1.4274492263793945, 1.4132468700408936, 1.4230279922485352, 1.4602130651474, 1.4188120365142822, 1.447255253791809, 1.422034740447998, 1.401893973350525, 1.4060084819793701, 1.4584497213363647, 1.4167124032974243, 1.4253036975860596, 1.436099886894226, 1.4206339120864868, 1.4129223823547363, 1.4238440990447998, 1.4134137630462646, 1.4140270948410034, 1.4654103517532349, 1.4042198657989502, 1.4142488241195679, 1.441910982131958, 1.4037792682647705, 1.413167953491211, 1.4144210815429688, 1.4111922979354858, 1.4052187204360962, 1.4481418132781982, 1.4131923913955688, 1.4139810800552368, 1.4080969095230103, 1.4086030721664429, 1.4394004344940186, 1.4001275300979614, 1.4102215766906738, 1.4159133434295654, 1.4162715673446655, 1.4066400527954102, 1.40264093875885, 1.3998421430587769, 1.416945457458496, 1.4204821586608887, 1.5000944137573242, 1.409568190574646, 1.4128211736679077, 1.4327483177185059, 1.408227801322937, 1.4239360094070435, 1.4260050058364868, 1.414233922958374, 1.4146918058395386, 1.4312115907669067, 1.423635721206665, 1.4033946990966797, 1.4285908937454224, 1.414866328239441, 1.4262155294418335, 1.4269382953643799, 1.4074434041976929, 1.4360648393630981, 1.4089351892471313, 1.43971848487854, 1.403756856918335, 1.4095838069915771, 1.4164968729019165, 1.4112907648086548, 1.4079183340072632, 1.4362998008728027, 1.420926809310913, 1.4301478862762451, 1.4509598016738892, 1.4382771253585815, 1.4423084259033203, 1.4252492189407349, 1.434378743171692, 1.4188505411148071, 1.4041746854782104, 1.4118496179580688, 1.4179202318191528, 1.439180850982666, 1.4427837133407593, 1.4181013107299805, 1.4286298751831055, 1.4084986448287964, 1.403873085975647, 1.4203680753707886, 1.4045134782791138, 1.4239137172698975, 1.4465314149856567, 1.4184038639068604, 1.413240671157837, 1.4529999494552612, 1.4066002368927002, 1.4244204759597778, 1.405150055885315, 1.4137543439865112, 1.4332605600357056, 1.418397307395935, 1.4012008905410767, 1.4198276996612549, 1.4128738641738892, 1.4273885488510132, 1.4125465154647827, 1.409299373626709, 1.421958088874817, 1.438233733177185, 1.4219506978988647, 1.4162362813949585, 1.4312095642089844, 1.4199298620224, 1.414034128189087, 1.402182936668396, 1.4119864702224731, 1.4168990850448608, 1.4105275869369507, 1.4109113216400146, 1.41132390499115, 1.4147415161132812, 1.403902292251587, 1.4274704456329346, 1.4892621040344238, 1.4161518812179565, 1.4145019054412842, 1.4242795705795288, 1.4294545650482178, 1.4523799419403076, 1.4338808059692383, 1.423012137413025, 1.4155406951904297, 1.4183733463287354, 1.4328877925872803, 1.437240719795227, 1.4092214107513428, 1.4333323240280151, 1.444301962852478, 1.4169285297393799, 1.4390685558319092, 1.4391963481903076, 1.4346431493759155, 1.4270918369293213, 1.4583793878555298, 1.449660301208496, 1.415236473083496, 1.432210087776184, 1.4273178577423096, 1.4240539073944092, 1.4130641222000122, 1.4356129169464111, 1.413827657699585, 1.4405957460403442, 1.4385769367218018, 1.419601321220398, 1.435006022453308, 1.4173271656036377, 1.4249576330184937, 1.4223262071609497, 1.4177155494689941, 1.4048161506652832, 1.4125137329101562, 1.4153472185134888, 1.4238733053207397, 1.445607304573059, 1.4103777408599854, 1.4076651334762573, 1.401936650276184, 1.4173662662506104, 1.4459443092346191, 1.4122660160064697, 1.4188286066055298, 1.4071226119995117, 1.4646250009536743, 1.5339058637619019, 1.4303301572799683, 1.4196714162826538, 1.4079828262329102, 1.4199721813201904, 1.4411938190460205, 1.4421312808990479, 1.425645351409912, 1.4369657039642334, 1.4165875911712646, 1.4438234567642212, 1.4340381622314453, 1.4122459888458252, 1.4589159488677979, 1.4328725337982178, 1.4337215423583984, 1.4457610845565796, 1.4575319290161133, 1.4418840408325195, 1.4128577709197998, 1.4099656343460083, 1.4114813804626465, 1.422317385673523, 1.4324924945831299, 1.4343425035476685, 1.4237228631973267, 1.4292712211608887, 1.4285154342651367, 1.432436466217041, 1.4193956851959229, 1.419264316558838, 1.4276773929595947, 1.4448705911636353, 1.4328110218048096, 1.419610857963562, 1.4223122596740723, 1.4414761066436768, 1.4624265432357788, 1.425241470336914, 1.472500205039978, 1.448551893234253, 1.4198226928710938, 1.4288698434829712, 1.4290504455566406, 1.4363503456115723, 1.4435609579086304, 1.4696959257125854, 1.4836606979370117, 1.4723197221755981, 1.420254111289978, 1.4350636005401611, 1.4583858251571655, 1.4638278484344482, 1.4542293548583984, 1.4473187923431396, 1.4236524105072021, 1.462459683418274, 1.4545769691467285, 1.4250534772872925, 1.4335728883743286, 1.4394358396530151, 1.4247273206710815, 1.528218388557434, 1.4564809799194336, 1.444406270980835, 1.479050636291504, 1.4463169574737549, 1.4281429052352905, 1.4791078567504883, 1.4547135829925537, 1.4471769332885742, 1.4910215139389038, 1.439918041229248, 1.46840500831604, 1.4908807277679443, 1.474365234375, 1.486489176750183, 1.4382984638214111, 1.4363009929656982, 1.4579811096191406, 1.4656600952148438, 1.44366455078125, 1.5228195190429688, 1.4378125667572021, 1.4449619054794312, 1.4656115770339966, 1.487655758857727, 1.4584602117538452, 1.4290002584457397, 1.4466526508331299, 1.4448918104171753, 1.457345724105835, 1.4507242441177368, 1.4391722679138184, 1.486306071281433, 1.453758716583252, 1.436693549156189, 1.4379104375839233, 1.4695318937301636, 1.453352928161621, 1.442577600479126, 1.4480469226837158, 1.47696852684021, 1.4356576204299927, 1.4411264657974243, 1.4563084840774536, 1.4354501962661743, 1.4929922819137573, 1.438274621963501, 1.4556814432144165, 1.4681326150894165, 1.451339840888977, 1.4682064056396484, 1.4651989936828613, 1.4485136270523071, 1.4583255052566528, 1.4533199071884155, 1.4365588426589966, 1.432682752609253, 1.4314465522766113, 1.4426047801971436, 1.4789178371429443, 1.492032766342163, 1.4511295557022095, 1.4861730337142944, 1.441422939300537, 1.4570964574813843, 1.4549528360366821, 1.4818699359893799, 1.4351991415023804, 1.4482392072677612, 1.4534369707107544, 1.4715509414672852, 1.4664663076400757, 1.4560136795043945, 1.441454291343689, 1.4779670238494873, 1.5048447847366333, 1.499433994293213, 1.4542195796966553, 1.4436172246932983, 1.6543147563934326, 1.4482815265655518, 1.4545106887817383, 1.4623066186904907, 1.4643207788467407, 1.50616455078125, 1.446244478225708, 1.4477732181549072, 1.465030550956726, 1.487149715423584, 1.4511278867721558, 1.4690715074539185, 1.4618760347366333, 1.5479472875595093, 1.4565114974975586, 1.4616819620132446, 1.47161066532135, 1.4629783630371094, 1.520865797996521, 1.511343002319336, 1.458303451538086, 1.4587717056274414, 1.4993740320205688, 1.4940669536590576, 1.4587947130203247, 1.462019681930542, 1.501368761062622, 1.4808512926101685, 1.4686779975891113, 1.4866539239883423, 1.4608677625656128, 1.4618982076644897, 1.4661785364151, 1.4794139862060547, 1.5035063028335571, 1.4613615274429321, 1.4662566184997559, 1.4849311113357544, 1.4650394916534424, 1.475712537765503, 1.4768940210342407, 1.4662058353424072, 1.4864773750305176, 1.4812078475952148, 1.5072518587112427, 1.4692095518112183, 1.4669734239578247, 1.5006710290908813, 1.4706990718841553, 1.4833399057388306, 1.4762306213378906, 1.4792652130126953, 1.4949140548706055, 1.4932966232299805, 1.4857507944107056, 1.4952375888824463, 1.4923715591430664, 1.5020705461502075, 1.5110816955566406, 1.4771955013275146, 1.4890739917755127, 1.4863874912261963, 1.4891867637634277, 1.5200549364089966, 1.4836509227752686, 1.4917954206466675, 1.4865397214889526, 1.5252288579940796, 1.503861427307129, 1.4809414148330688, 1.548520803451538, 1.488908290863037, 1.4931340217590332, 1.4948166608810425, 1.5393619537353516, 1.5193482637405396, 1.4879217147827148, 1.5229976177215576, 1.4997378587722778, 1.5327450037002563, 1.5037168264389038, 1.4854401350021362, 1.5122430324554443, 1.547081470489502, 1.5095715522766113, 1.5047452449798584, 1.5349394083023071, 1.5775363445281982, 1.5002061128616333, 1.5027574300765991, 1.533108115196228, 1.5301927328109741, 1.4967985153198242, 1.5563560724258423, 1.5245164632797241, 1.5157517194747925, 1.4969152212142944, 1.5367457866668701, 1.5022879838943481, 1.523688793182373, 1.5173033475875854, 1.5861207246780396, 1.5045846700668335, 1.5329660177230835, 1.5095261335372925, 1.5732786655426025, 1.5078297853469849, 1.523897409439087, 1.506475567817688, 1.5113321542739868, 1.505353569984436, 1.5062248706817627, 1.4967968463897705, 1.5112805366516113, 1.520500659942627, 1.508436679840088, 1.5426055192947388, 1.5616788864135742, 1.5397560596466064, 1.5123320817947388, 1.4992575645446777, 1.5183871984481812, 1.5210001468658447, 1.5253479480743408, 1.505064606666565, 1.5004812479019165, 1.5220732688903809, 1.5340169668197632, 1.5221525430679321, 1.5162945985794067, 1.5250192880630493, 1.5105153322219849, 1.5055232048034668, 1.508083701133728, 1.5485882759094238, 1.5623449087142944, 1.5201603174209595, 1.519547462463379, 1.569997787475586, 1.5332796573638916, 1.5413124561309814, 1.5103505849838257, 1.5380942821502686, 1.5298799276351929, 1.5674248933792114, 1.5432583093643188, 1.515615463256836, 1.524825096130371, 1.5343984365463257, 1.5194908380508423, 1.5929749011993408, 1.5273085832595825, 1.5309996604919434, 1.6033284664154053, 1.5309388637542725, 1.5171555280685425, 1.5551360845565796, 1.5422372817993164, 1.5746155977249146, 1.5358648300170898, 1.5608264207839966, 1.552248477935791, 1.575534701347351, 1.5863654613494873, 1.5312961339950562, 1.5392906665802002, 1.561405062675476, 1.5393224954605103, 1.608375906944275, 1.545663595199585, 1.5663495063781738, 1.5442324876785278, 1.5382649898529053, 1.5547819137573242, 1.577735424041748, 1.5405521392822266, 1.5822510719299316, 1.5820072889328003, 1.5989346504211426, 1.5906585454940796, 1.5587986707687378, 1.585885763168335, 1.6047074794769287, 1.5779591798782349, 1.5962707996368408, 1.5581260919570923, 1.606651782989502, 1.5937830209732056, 1.5634095668792725, 1.5383213758468628, 1.5652598142623901, 1.6210988759994507, 1.562680721282959, 1.5568020343780518, 1.5708893537521362, 1.5651417970657349, 1.567411184310913, 1.5728570222854614, 1.5636913776397705, 1.5617302656173706, 1.5562795400619507, 1.582693338394165, 1.559900164604187, 1.5755735635757446, 1.5682899951934814, 1.591397762298584, 1.6139088869094849, 1.5716038942337036, 1.6039189100265503, 1.5974063873291016, 1.5553480386734009, 1.5641885995864868, 1.6245487928390503, 1.5727099180221558, 1.6869008541107178, 1.6262975931167603, 1.5663001537322998, 1.6482939720153809, 1.5808974504470825, 1.5767780542373657, 1.6022028923034668, 1.628856897354126, 1.6366547346115112, 1.5920343399047852, 1.630160927772522, 1.60287606716156, 1.5825049877166748, 1.6340006589889526, 1.6108185052871704, 1.5810863971710205, 1.5975112915039062, 1.6038275957107544, 1.579980731010437, 1.6072083711624146, 1.5864516496658325, 1.6276001930236816, 1.6447991132736206, 1.5939929485321045, 1.5929700136184692, 1.6014747619628906, 1.6225934028625488, 1.5955023765563965, 1.6314610242843628, 1.6057859659194946, 1.6113331317901611, 1.7095085382461548, 1.6059740781784058, 1.619004726409912, 1.6456289291381836, 1.607011318206787, 1.6101316213607788, 1.6624804735183716, 1.6232593059539795, 1.635533094406128, 1.629352331161499, 1.6497912406921387, 1.6778769493103027, 1.6364065408706665, 1.6147639751434326, 1.7345197200775146, 1.624035120010376, 1.6527312994003296, 1.627442717552185, 1.6686334609985352, 1.6451153755187988, 1.6303300857543945, 1.6576842069625854, 1.645849347114563, 1.6431941986083984, 1.653501033782959, 1.6476527452468872, 1.6350339651107788, 1.6430623531341553, 1.6109908819198608, 1.6317325830459595, 1.677472710609436, 1.6253535747528076, 1.6249690055847168, 1.6292449235916138, 1.6381412744522095, 1.6332030296325684, 1.6454647779464722, 1.6567944288253784, 1.6334495544433594, 1.637743353843689, 1.6469274759292603, 1.7037633657455444, 1.6888537406921387, 1.628463625907898, 1.641340970993042, 1.6223571300506592, 1.655631184577942, 1.6499403715133667, 1.6489801406860352, 1.645851492881775, 1.7510740756988525, 1.6459226608276367, 1.666074514389038, 1.6286424398422241, 1.642176866531372, 1.6975537538528442, 1.648929476737976, 1.661415696144104, 1.6572072505950928, 1.6586438417434692, 1.637448787689209, 1.6560026407241821, 1.695389747619629, 1.6644086837768555, 1.648797869682312, 1.6800333261489868, 1.7297159433364868, 1.658860683441162, 1.6538742780685425, 1.679221749305725, 1.655282974243164, 1.6550908088684082, 1.6805976629257202, 1.7750297784805298, 1.683605432510376, 1.6901131868362427, 1.6680477857589722, 1.7131987810134888, 1.8375766277313232, 1.6905213594436646, 1.7118984460830688, 1.6893832683563232, 1.7014507055282593, 1.6967170238494873, 1.6948546171188354, 1.674800992012024, 1.6805866956710815, 1.6966615915298462, 1.6865544319152832, 1.6837425231933594, 1.7149542570114136, 1.6896729469299316, 1.7346553802490234, 1.699233889579773, 1.705600380897522, 1.6971410512924194, 1.6915359497070312, 1.7168858051300049, 1.7407561540603638, 1.8139705657958984, 1.6992157697677612, 1.7054221630096436, 1.746360182762146, 1.766555905342102, 1.7300071716308594, 1.735937237739563, 1.7271761894226074, 1.7284826040267944, 1.7202997207641602, 1.7305324077606201, 1.7068201303482056, 1.7162050008773804, 1.7183431386947632, 1.7064965963363647, 1.716201901435852, 1.7160849571228027, 1.7308176755905151, 1.7174748182296753, 1.722825050354004, 1.7153263092041016, 1.7096201181411743, 1.7881451845169067, 1.7371494770050049, 1.7567864656448364, 1.7302361726760864, 1.758402705192566, 1.714005470275879, 1.7496061325073242, 1.7238826751708984, 1.72255539894104, 1.8077194690704346, 1.7507139444351196, 1.7572975158691406, 1.785323143005371, 1.735877513885498, 1.7339084148406982, 1.7242186069488525, 1.7575124502182007, 1.749741792678833, 1.7407935857772827, 1.7432745695114136, 1.7349361181259155, 1.7756162881851196, 1.7531917095184326, 1.7767828702926636, 1.7318081855773926, 1.7548102140426636, 1.7520562410354614, 1.8007758855819702, 1.7762391567230225, 1.753434658050537, 1.7453997135162354, 1.766650915145874, 1.767650842666626, 1.748496651649475, 1.7713773250579834, 1.7597943544387817, 1.7885222434997559, 1.7618972063064575, 1.749199390411377, 1.744958758354187, 1.7514574527740479, 1.7709882259368896, 1.7511290311813354, 1.8018193244934082, 1.7596535682678223, 1.7715070247650146, 1.7505059242248535, 1.846436858177185, 1.8036047220230103, 1.817384123802185, 1.7586055994033813, 1.7741209268569946, 1.7639660835266113, 1.7600914239883423, 1.755415916442871, 1.7805572748184204, 1.7799556255340576, 1.872299075126648, 1.8025001287460327, 1.814366102218628, 1.833780288696289, 1.7764341831207275, 1.7755218744277954, 1.7781695127487183, 1.7943722009658813, 1.8207117319107056, 1.8302478790283203, 1.7901133298873901, 1.8338242769241333, 1.8134514093399048, 1.8098942041397095, 1.822221040725708, 1.8008606433868408, 1.7985893487930298, 1.852402687072754, 1.826554775238037, 1.8116273880004883, 1.8139294385910034, 1.824413537979126, 1.8246417045593262, 1.7951356172561646, 1.8135756254196167, 1.8109190464019775, 1.8270772695541382, 1.8333497047424316, 1.815347671508789, 1.840463399887085, 1.7999216318130493, 1.8274739980697632, 1.907614827156067, 1.8280582427978516, 1.8199697732925415, 1.8260326385498047, 1.8158022165298462, 1.8076080083847046, 1.843665599822998, 1.852451205253601, 1.8345658779144287, 1.847959041595459, 1.8756606578826904, 1.947020173072815, 1.8668444156646729, 1.8409423828125, 1.8393703699111938, 1.9216525554656982, 1.8500053882598877, 1.8650356531143188, 1.853163480758667, 1.8457207679748535, 1.8984633684158325, 1.8719398975372314, 1.854569435119629, 1.852266788482666, 1.9162535667419434, 1.8739373683929443, 1.8461658954620361, 1.8311346769332886, 1.9123603105545044, 1.8521472215652466, 1.8576254844665527, 1.854928731918335, 1.9165029525756836, 1.8889890909194946, 1.8794032335281372, 1.911313533782959, 1.895833969116211, 1.8493025302886963, 1.8392517566680908, 1.8640793561935425, 1.8778493404388428, 1.8609548807144165, 1.9955798387527466, 1.8647232055664062, 1.8371855020523071, 1.884324073791504, 1.8835954666137695, 1.891196608543396, 1.8732399940490723, 1.9717719554901123, 1.857162356376648, 1.8817485570907593, 1.8965967893600464, 1.9082839488983154, 1.8762887716293335, 1.9908250570297241, 1.8832809925079346, 1.8823755979537964, 1.8922312259674072, 1.8978794813156128, 1.9446609020233154, 1.9912171363830566, 1.922074794769287, 1.906510353088379, 1.9119510650634766, 1.8915613889694214, 1.8878544569015503, 1.932712435722351, 1.916782021522522, 1.891903042793274, 1.9005893468856812, 1.9488203525543213, 1.9193867444992065, 1.9036351442337036, 1.9263612031936646, 1.9099178314208984, 1.9155462980270386, 1.9095869064331055, 1.927425503730774, 1.9074891805648804, 1.964669108390808, 1.9329208135604858, 1.9135222434997559, 2.0064070224761963, 1.9347940683364868, 1.9394277334213257, 1.9066095352172852, 1.926438808441162, 1.9568058252334595, 1.9728907346725464, 1.9270076751708984, 1.979014277458191, 1.9388231039047241, 1.9440068006515503, 1.9489519596099854, 1.9487953186035156, 1.941988468170166, 1.9330614805221558, 1.973443865776062, 1.9393796920776367, 1.9662917852401733, 1.9635380506515503, 1.9591054916381836, 1.948997974395752, 1.965318202972412, 1.9854627847671509, 1.9680798053741455, 1.9993232488632202, 1.969562292098999, 1.9578866958618164, 2.0106360912323, 1.956033706665039, 1.9572780132293701, 1.950904369354248, 1.95059072971344, 1.9583929777145386, 1.9735993146896362, 1.9593104124069214, 1.958250641822815, 2.006197452545166, 1.9501839876174927, 1.9855597019195557, 1.9495898485183716, 1.9783082008361816, 2.0003292560577393, 1.972074270248413, 1.987133264541626, 2.0430490970611572, 1.9775466918945312, 2.022663116455078, 1.9702318906784058, 2.0333821773529053, 2.0082578659057617, 2.1100621223449707, 2.0137147903442383, 2.0278725624084473, 1.993174433708191, 2.013032913208008, 1.9751838445663452, 2.0573172569274902, 2.018258571624756, 1.9901341199874878, 1.9853181838989258, 2.064753293991089, 2.022783041000366, 1.9939638376235962, 2.028432607650757, 2.0321028232574463, 2.019068956375122, 2.000075340270996, 2.012939691543579, 2.0164339542388916, 2.0649211406707764, 2.1207737922668457, 2.0523555278778076, 2.0842082500457764, 1.9959757328033447, 2.0306334495544434, 1.9831128120422363, 1.9865450859069824, 2.0024194717407227, 2.034010648727417, 2.006983757019043, 2.0451366901397705, 1.9991768598556519, 2.0652949810028076, 2.009726047515869, 2.0792369842529297, 2.0455923080444336, 2.0508832931518555, 2.0882959365844727, 2.0335352420806885, 2.079881429672241, 2.0585970878601074, 2.054870843887329, 2.0881433486938477, 2.0766217708587646, 2.0479066371917725, 2.194246768951416, 2.0523574352264404, 2.0502231121063232, 2.058300256729126, 2.0716700553894043, 2.0258731842041016, 2.056234836578369, 2.083279609680176, 2.0621941089630127, 2.0736629962921143, 2.0688443183898926, 2.065765619277954, 2.038726568222046, 2.1054279804229736, 2.070364475250244, 2.1019999980926514, 2.035970449447632, 2.0644657611846924, 2.0613186359405518, 2.052485942840576, 2.0578997135162354, 2.044067621231079, 2.1400465965270996, 2.2470788955688477, 2.099670171737671, 2.1043176651000977, 2.126389503479004, 2.0789146423339844, 2.1316940784454346, 2.068653106689453, 2.0915653705596924, 2.098461627960205, 2.103687286376953, 2.107543468475342, 2.1011459827423096, 2.1491124629974365, 2.1205718517303467, 2.1265177726745605, 2.0883712768554688, 2.109076738357544, 2.0917787551879883, 2.121554136276245, 2.1063263416290283, 2.131392240524292, 2.099940061569214, 2.136221170425415, 2.112732410430908, 2.1175484657287598, 2.1965646743774414, 2.1508610248565674, 2.111952304840088, 2.106607437133789, 2.1255578994750977, 2.1441502571105957, 2.1329195499420166, 2.1558117866516113, 2.1298577785491943, 2.099961280822754, 2.0878655910491943, 2.2201311588287354, 2.1310386657714844, 2.158879280090332, 2.1268563270568848, 2.1814489364624023, 2.142580270767212, 2.136134147644043, 2.145341157913208, 2.1540465354919434, 2.184831380844116, 2.2036027908325195, 2.171682357788086, 2.176107168197632, 2.1712610721588135, 2.196183919906616, 2.1486575603485107, 2.1832950115203857, 2.1894516944885254, 2.167717218399048, 2.161512851715088, 2.2213985919952393, 2.155172824859619, 2.15455961227417, 2.1553194522857666, 2.1224849224090576, 2.174354076385498, 2.1512835025787354, 2.111769437789917, 2.2095351219177246, 2.253876209259033, 2.164039373397827, 2.1836328506469727, 2.218127727508545, 2.1548244953155518, 2.168828010559082, 2.143537998199463, 2.1819262504577637, 2.2333731651306152, 2.194477081298828, 2.155888080596924, 2.1751105785369873, 2.2546706199645996, 2.178232431411743, 2.20982027053833, 2.222797393798828, 2.204559803009033, 2.206592559814453, 2.2419121265411377, 2.208101511001587, 2.2032151222229004, 2.198256254196167, 2.2351038455963135, 2.1712887287139893, 2.1574604511260986, 2.180729866027832, 2.1769068241119385, 2.2315056324005127, 2.233478307723999, 2.176530599594116, 2.1895217895507812, 2.230238199234009, 2.2483410835266113, 2.2264037132263184, 2.2405877113342285, 2.18206524848938, 2.2333784103393555, 2.295651435852051, 2.2325332164764404, 2.2067952156066895, 2.19708514213562, 2.2901523113250732, 2.320108413696289, 2.247720718383789, 2.22084379196167, 2.2330846786499023, 2.219188928604126, 2.254373550415039, 2.331510066986084, 2.2403037548065186, 2.222909450531006, 2.2613134384155273, 2.275463104248047, 2.2721030712127686, 2.268298625946045, 2.321977376937866, 2.3539819717407227, 2.225093364715576, 2.2610175609588623, 2.277040958404541, 2.3122358322143555, 2.267007350921631, 2.282433271408081, 2.342052698135376, 2.298278570175171, 2.2711474895477295, 2.2661397457122803, 2.2586889266967773, 2.2584941387176514, 2.290208578109741, 2.2684319019317627, 2.297151803970337, 2.3735504150390625, 2.342278242111206, 2.3322436809539795, 2.3523099422454834, 2.325228214263916, 2.2832517623901367, 2.3376173973083496, 2.3277368545532227, 2.290607452392578, 2.298840045928955, 2.4801833629608154, 2.2938218116760254, 2.300849676132202, 2.2994191646575928, 2.307255268096924, 2.3401336669921875, 2.3415873050689697, 2.306046962738037, 2.3139395713806152, 2.3204503059387207, 2.347189426422119, 2.3311476707458496, 2.426056146621704, 2.3418397903442383, 2.3851428031921387, 2.3931596279144287, 2.350344181060791, 2.37536883354187, 2.319901466369629, 2.3139803409576416, 2.3314857482910156, 2.3235116004943848, 2.3774049282073975, 2.3742387294769287, 2.316664457321167, 2.5507991313934326, 2.3113009929656982, 2.364042282104492, 2.333966016769409, 2.3564770221710205, 2.426605463027954, 2.345604181289673, 2.3916900157928467, 2.3434648513793945, 2.3521318435668945, 2.4064462184906006, 2.384204387664795, 2.3983280658721924, 2.4292287826538086, 2.37186861038208, 2.405241012573242, 2.3416311740875244, 2.373702049255371, 2.438312530517578, 2.427098274230957, 2.4096786975860596, 2.3446953296661377, 2.396284341812134, 2.4971861839294434, 2.3947861194610596, 2.4046123027801514, 2.4091572761535645, 2.5259013175964355, 2.3785135746002197, 2.3631844520568848, 2.3688457012176514, 2.337989568710327, 2.3994264602661133, 2.37807035446167, 2.3896148204803467, 2.4057538509368896, 2.4892520904541016, 2.4155361652374268, 2.4376513957977295, 2.3670167922973633, 2.4215261936187744, 2.3722968101501465, 2.4491724967956543, 2.3788115978240967, 2.433372735977173, 2.4216904640197754, 2.3792123794555664, 2.338890314102173, 2.3820314407348633, 2.392125129699707, 2.4028360843658447, 2.4357357025146484, 2.450335741043091, 2.388258218765259, 2.4779131412506104, 2.4345173835754395, 2.426765203475952, 2.4059574604034424, 2.409956693649292, 2.4114644527435303, 2.4405624866485596, 2.450467109680176, 2.4460787773132324, 2.4299607276916504, 2.4340429306030273, 2.4246389865875244, 2.411731481552124, 2.4120965003967285, 2.4273781776428223, 2.4470131397247314, 2.415034055709839, 2.433971405029297, 2.4299960136413574, 2.418679714202881, 2.5011587142944336, 2.4587061405181885, 2.454688787460327, 2.4566798210144043, 2.4382450580596924, 2.4833931922912598, 2.485072135925293, 2.41672420501709, 2.4539523124694824, 2.456444501876831, 2.4451112747192383, 2.5047764778137207, 2.588391065597534, 2.491595506668091, 2.448516368865967, 2.5276520252227783, 2.478250741958618, 2.494842052459717, 2.4504082202911377, 2.4196853637695312, 2.449014186859131, 2.4850080013275146, 2.494673013687134, 2.4769649505615234, 2.6502087116241455, 2.4770774841308594, 2.4539825916290283, 2.4646003246307373, 2.5011186599731445, 2.5274674892425537, 2.4288835525512695, 2.4491007328033447, 2.4610419273376465, 2.4623820781707764, 2.4741196632385254, 2.470182418823242, 2.4867053031921387, 2.5190420150756836, 2.5621464252471924, 2.5358211994171143, 2.641982078552246, 2.4922468662261963, 2.4743425846099854, 2.4920547008514404, 2.4678471088409424, 2.4564905166625977, 2.491410493850708, 2.514738082885742, 2.4884252548217773, 2.5367178916931152, 2.5177340507507324, 2.50042986869812, 2.558407783508301, 2.4991159439086914, 2.5632150173187256, 2.6150898933410645, 2.501932144165039, 2.5334296226501465, 2.5361225605010986, 2.5559885501861572, 2.4688618183135986, 2.5156962871551514, 2.5925447940826416, 2.524303913116455, 2.5905420780181885, 2.58219575881958, 2.5249907970428467, 2.5779480934143066, 2.537574052810669, 2.568092107772827, 2.5043256282806396, 2.542820453643799, 2.6676435470581055, 2.5616042613983154, 2.5647706985473633, 2.562295436859131, 2.561483860015869, 2.5743982791900635, 2.546673536300659, 2.612093210220337, 2.5998995304107666, 2.539614200592041, 2.622485637664795, 2.6178648471832275, 2.53920316696167, 2.564241647720337, 2.5622777938842773, 2.560203790664673, 2.5581324100494385, 2.5920794010162354, 2.556112766265869, 2.628021478652954, 2.5682950019836426, 2.6029186248779297, 2.5672900676727295, 2.596412420272827, 2.634326696395874, 2.585369348526001, 2.5906407833099365, 2.6258556842803955, 2.6355814933776855, 2.568742036819458, 2.611154317855835, 2.5489649772644043, 2.622127056121826, 2.609896659851074, 2.594454288482666, 2.582639455795288, 2.6024651527404785, 2.594892740249634, 2.638441801071167, 2.5572853088378906, 2.564713716506958, 2.631675958633423, 2.5718257427215576, 2.6170270442962646, 2.649963617324829, 2.5905728340148926, 2.677049398422241, 2.6470751762390137, 2.592978000640869, 2.6396188735961914, 2.6125669479370117, 2.6242923736572266, 2.6583950519561768, 2.7193148136138916, 2.6175544261932373, 2.6494569778442383, 2.634573221206665, 2.675267219543457, 2.6250052452087402, 2.6009256839752197, 2.8187308311462402, 2.6243550777435303, 2.647660970687866, 2.598217487335205, 2.649555206298828, 2.644327402114868, 2.8661088943481445, 2.663011074066162, 2.733268976211548, 2.6262869834899902, 2.7106027603149414, 2.7079975605010986, 2.702676296234131, 2.6364543437957764, 2.612431049346924, 2.74007248878479, 2.659386396408081, 2.6623032093048096, 2.639103412628174, 2.7203588485717773, 2.665696620941162, 2.640855073928833, 2.7807767391204834, 2.7199974060058594, 2.7270405292510986, 2.731694459915161, 2.6936137676239014, 2.6562342643737793, 2.6636974811553955, 2.671151638031006, 2.728782892227173, 2.7383224964141846, 2.745082139968872, 2.6510114669799805, 2.6477956771850586, 2.667945146560669, 2.694463014602661, 2.745598077774048, 2.6734678745269775, 2.6705217361450195, 2.679090738296509, 2.666658639907837, 2.7402236461639404, 2.669614791870117, 2.6715269088745117, 2.668771982192993, 2.669196367263794, 2.688983201980591, 2.66278338432312, 2.74407958984375, 2.6899361610412598, 2.7221860885620117, 2.7523648738861084, 2.724383592605591, 2.679147958755493, 2.7967402935028076, 2.6996707916259766, 2.7741951942443848, 2.707428455352783, 2.7460439205169678, 2.7234063148498535, 2.6744773387908936, 2.710294246673584, 2.711127519607544, 2.7240402698516846, 2.767781972885132, 2.812037467956543, 2.8490912914276123, 2.7268152236938477, 2.805975914001465, 2.7211644649505615, 2.7111573219299316, 2.729682207107544, 2.7740986347198486, 2.771902561187744, 2.745314359664917, 2.7519750595092773, 2.721477746963501, 2.710674285888672, 3.0328116416931152, 2.6968777179718018, 2.7505877017974854], 'val_accuracy': [0.10382513701915741, 0.13661202788352966, 0.13114753365516663, 0.13661202788352966, 0.1420765072107315, 0.13661202788352966, 0.17486338317394257, 0.1530054658651352, 0.19672131538391113, 0.15846994519233704, 0.17486338317394257, 0.22404371201992035, 0.2295081913471222, 0.20765027403831482, 0.16939890384674072, 0.1912568360567093, 0.1803278625011444, 0.1912568360567093, 0.21311475336551666, 0.22404371201992035, 0.19672131538391113, 0.25136610865592957, 0.2185792326927185, 0.21311475336551666, 0.22404371201992035, 0.21311475336551666, 0.2568306028842926, 0.20218579471111298, 0.24590164422988892, 0.23497267067432404, 0.18579235672950745, 0.24590164422988892, 0.22404371201992035, 0.19672131538391113, 0.22404371201992035, 0.21311475336551666, 0.21311475336551666, 0.2568306028842926, 0.24043716490268707, 0.22404371201992035, 0.23497267067432404, 0.25136610865592957, 0.24043716490268707, 0.22404371201992035, 0.2295081913471222, 0.24043716490268707, 0.24590164422988892, 0.2295081913471222, 0.25136610865592957, 0.2677595615386963, 0.24043716490268707, 0.24043716490268707, 0.2568306028842926, 0.25136610865592957, 0.26229506731033325, 0.23497267067432404, 0.2568306028842926, 0.26229506731033325, 0.2950819730758667, 0.2677595615386963, 0.2568306028842926, 0.25136610865592957, 0.23497267067432404, 0.26229506731033325, 0.2568306028842926, 0.2568306028842926, 0.24590164422988892, 0.25136610865592957, 0.2677595615386963, 0.26229506731033325, 0.30054643750190735, 0.284153014421463, 0.2786885201931, 0.2732240557670593, 0.30054643750190735, 0.2677595615386963, 0.2950819730758667, 0.26229506731033325, 0.26229506731033325, 0.2568306028842926, 0.26229506731033325, 0.2732240557670593, 0.2786885201931, 0.2677595615386963, 0.2677595615386963, 0.30054643750190735, 0.2732240557670593, 0.28961747884750366, 0.2732240557670593, 0.2786885201931, 0.30054643750190735, 0.3224043846130371, 0.3060109317302704, 0.3060109317302704, 0.28961747884750366, 0.3224043846130371, 0.2950819730758667, 0.2786885201931, 0.2950819730758667, 0.2786885201931, 0.2786885201931, 0.28961747884750366, 0.3060109317302704, 0.2950819730758667, 0.2786885201931, 0.2950819730758667, 0.2786885201931, 0.284153014421463, 0.2786885201931, 0.3060109317302704, 0.3169398903846741, 0.32786884903907776, 0.2950819730758667, 0.2950819730758667, 0.3060109317302704, 0.32786884903907776, 0.3060109317302704, 0.3224043846130371, 0.30054643750190735, 0.284153014421463, 0.31147539615631104, 0.28961747884750366, 0.31147539615631104, 0.31147539615631104, 0.32786884903907776, 0.3224043846130371, 0.33879780769348145, 0.3169398903846741, 0.31147539615631104, 0.3333333432674408, 0.3060109317302704, 0.3169398903846741, 0.28961747884750366, 0.2950819730758667, 0.3224043846130371, 0.2950819730758667, 0.31147539615631104, 0.32786884903907776, 0.31147539615631104, 0.2950819730758667, 0.3333333432674408, 0.3169398903846741, 0.3442623019218445, 0.284153014421463, 0.3060109317302704, 0.31147539615631104, 0.28961747884750366, 0.3169398903846741, 0.3224043846130371, 0.3224043846130371, 0.3224043846130371, 0.3169398903846741, 0.2950819730758667, 0.28961747884750366, 0.3224043846130371, 0.3169398903846741, 0.3224043846130371, 0.3169398903846741, 0.3169398903846741, 0.3333333432674408, 0.3169398903846741, 0.3224043846130371, 0.3169398903846741, 0.3224043846130371, 0.3169398903846741, 0.3060109317302704, 0.3060109317302704, 0.3333333432674408, 0.31147539615631104, 0.3333333432674408, 0.35519126057624817, 0.3333333432674408, 0.32786884903907776, 0.33879780769348145, 0.34972676634788513, 0.3333333432674408, 0.33879780769348145, 0.33879780769348145, 0.3224043846130371, 0.35519126057624817, 0.3442623019218445, 0.34972676634788513, 0.35519126057624817, 0.35519126057624817, 0.36612021923065186, 0.3606557250022888, 0.3442623019218445, 0.3442623019218445, 0.33879780769348145, 0.3606557250022888, 0.36612021923065186, 0.3606557250022888, 0.33879780769348145, 0.3715847134590149, 0.3606557250022888, 0.36612021923065186, 0.38797813653945923, 0.3442623019218445, 0.3169398903846741, 0.3825136721134186, 0.36612021923065186, 0.3442623019218445, 0.3606557250022888, 0.34972676634788513, 0.3825136721134186, 0.3606557250022888, 0.35519126057624817, 0.3825136721134186, 0.3606557250022888, 0.38797813653945923, 0.36612021923065186, 0.3825136721134186, 0.3715847134590149, 0.41530054807662964, 0.39344263076782227, 0.41530054807662964, 0.3989070951938629, 0.36612021923065186, 0.3989070951938629, 0.37704917788505554, 0.3989070951938629, 0.3989070951938629, 0.37704917788505554, 0.37704917788505554, 0.3715847134590149, 0.3606557250022888, 0.41530054807662964, 0.37704917788505554, 0.3715847134590149, 0.41530054807662964, 0.4207650125026703, 0.3825136721134186, 0.38797813653945923, 0.40437158942222595, 0.3989070951938629, 0.41530054807662964, 0.38797813653945923, 0.41530054807662964, 0.38797813653945923, 0.3989070951938629, 0.38797813653945923, 0.4098360538482666, 0.39344263076782227, 0.40437158942222595, 0.41530054807662964, 0.38797813653945923, 0.39344263076782227, 0.4098360538482666, 0.4098360538482666, 0.39344263076782227, 0.40437158942222595, 0.40437158942222595, 0.3989070951938629, 0.4098360538482666, 0.437158465385437, 0.40437158942222595, 0.4207650125026703, 0.37704917788505554, 0.38797813653945923, 0.41530054807662964, 0.44262295961380005, 0.4207650125026703, 0.41530054807662964, 0.40437158942222595, 0.4098360538482666, 0.41530054807662964, 0.38797813653945923, 0.4098360538482666, 0.4207650125026703, 0.41530054807662964, 0.39344263076782227, 0.43169400095939636, 0.41530054807662964, 0.40437158942222595, 0.41530054807662964, 0.43169400095939636, 0.39344263076782227, 0.40437158942222595, 0.41530054807662964, 0.4207650125026703, 0.3989070951938629, 0.43169400095939636, 0.4098360538482666, 0.4098360538482666, 0.4207650125026703, 0.437158465385437, 0.3825136721134186, 0.40437158942222595, 0.44262295961380005, 0.4207650125026703, 0.4098360538482666, 0.40437158942222595, 0.44262295961380005, 0.40437158942222595, 0.437158465385437, 0.4262295067310333, 0.41530054807662964, 0.437158465385437, 0.41530054807662964, 0.4207650125026703, 0.4207650125026703, 0.41530054807662964, 0.4262295067310333, 0.44262295961380005, 0.40437158942222595, 0.4098360538482666, 0.40437158942222595, 0.4480874240398407, 0.437158465385437, 0.43169400095939636, 0.4262295067310333, 0.4262295067310333, 0.4207650125026703, 0.437158465385437, 0.437158465385437, 0.41530054807662964, 0.41530054807662964, 0.4262295067310333, 0.41530054807662964, 0.4480874240398407, 0.44262295961380005, 0.4262295067310333, 0.4262295067310333, 0.4207650125026703, 0.4262295067310333, 0.4480874240398407, 0.4207650125026703, 0.40437158942222595, 0.38797813653945923, 0.43169400095939636, 0.38797813653945923, 0.4262295067310333, 0.4098360538482666, 0.44262295961380005, 0.4480874240398407, 0.4207650125026703, 0.4590163826942444, 0.437158465385437, 0.4480874240398407, 0.44262295961380005, 0.43169400095939636, 0.43169400095939636, 0.437158465385437, 0.4480874240398407, 0.44262295961380005, 0.4480874240398407, 0.4098360538482666, 0.39344263076782227, 0.43169400095939636, 0.40437158942222595, 0.43169400095939636, 0.4480874240398407, 0.43169400095939636, 0.4098360538482666, 0.437158465385437, 0.437158465385437, 0.437158465385437, 0.43169400095939636, 0.44262295961380005, 0.45355191826820374, 0.43169400095939636, 0.4207650125026703, 0.4207650125026703, 0.45355191826820374, 0.4590163826942444, 0.4207650125026703, 0.4262295067310333, 0.44262295961380005, 0.4262295067310333, 0.4590163826942444, 0.4480874240398407, 0.45355191826820374, 0.4262295067310333, 0.43169400095939636, 0.4262295067310333, 0.4098360538482666, 0.43169400095939636, 0.4098360538482666, 0.4480874240398407, 0.437158465385437, 0.437158465385437, 0.4262295067310333, 0.4207650125026703, 0.44262295961380005, 0.44262295961380005, 0.43169400095939636, 0.437158465385437, 0.4590163826942444, 0.437158465385437, 0.4480874240398407, 0.4098360538482666, 0.4644808769226074, 0.437158465385437, 0.44262295961380005, 0.45355191826820374, 0.4480874240398407, 0.43169400095939636, 0.45355191826820374, 0.4480874240398407, 0.44262295961380005, 0.45355191826820374, 0.4590163826942444, 0.44262295961380005, 0.4863387942314148, 0.4590163826942444, 0.4644808769226074, 0.4480874240398407, 0.4590163826942444, 0.44262295961380005, 0.45355191826820374, 0.4480874240398407, 0.4480874240398407, 0.44262295961380005, 0.45355191826820374, 0.46994534134864807, 0.46994534134864807, 0.43169400095939636, 0.4590163826942444, 0.4590163826942444, 0.44262295961380005, 0.4480874240398407, 0.4644808769226074, 0.46994534134864807, 0.43169400095939636, 0.437158465385437, 0.45355191826820374, 0.437158465385437, 0.45355191826820374, 0.45355191826820374, 0.4480874240398407, 0.4590163826942444, 0.4644808769226074, 0.46994534134864807, 0.41530054807662964, 0.4590163826942444, 0.44262295961380005, 0.4644808769226074, 0.4262295067310333, 0.4262295067310333, 0.4644808769226074, 0.4754098355770111, 0.48087432980537415, 0.4590163826942444, 0.4644808769226074, 0.4754098355770111, 0.437158465385437, 0.4480874240398407, 0.45355191826820374, 0.4644808769226074, 0.4480874240398407, 0.46994534134864807, 0.48087432980537415, 0.4863387942314148, 0.4644808769226074, 0.4644808769226074, 0.45355191826820374, 0.4644808769226074, 0.46994534134864807, 0.4644808769226074, 0.4644808769226074, 0.4754098355770111, 0.4644808769226074, 0.4754098355770111, 0.437158465385437, 0.4644808769226074, 0.4644808769226074, 0.44262295961380005, 0.4590163826942444, 0.4590163826942444, 0.4754098355770111, 0.4590163826942444, 0.46994534134864807, 0.45355191826820374, 0.4644808769226074, 0.46994534134864807, 0.4480874240398407, 0.4644808769226074, 0.48087432980537415, 0.4644808769226074, 0.4590163826942444, 0.46994534134864807, 0.4480874240398407, 0.45355191826820374, 0.48087432980537415, 0.48087432980537415, 0.4590163826942444, 0.4590163826942444, 0.46994534134864807, 0.45355191826820374, 0.4590163826942444, 0.43169400095939636, 0.4754098355770111, 0.4863387942314148, 0.4754098355770111, 0.49180328845977783, 0.4590163826942444, 0.4480874240398407, 0.4590163826942444, 0.4480874240398407, 0.46994534134864807, 0.4590163826942444, 0.4754098355770111, 0.46994534134864807, 0.48087432980537415, 0.45355191826820374, 0.46994534134864807, 0.48087432980537415, 0.4644808769226074, 0.4644808769226074, 0.46994534134864807, 0.4590163826942444, 0.4754098355770111, 0.4863387942314148, 0.48087432980537415, 0.4590163826942444, 0.49180328845977783, 0.46994534134864807, 0.48087432980537415, 0.45355191826820374, 0.46994534134864807, 0.4590163826942444, 0.48087432980537415, 0.4644808769226074, 0.46994534134864807, 0.46994534134864807, 0.4863387942314148, 0.49180328845977783, 0.4754098355770111, 0.4863387942314148, 0.49180328845977783, 0.4590163826942444, 0.4754098355770111, 0.45355191826820374, 0.45355191826820374, 0.4972677528858185, 0.45355191826820374, 0.48087432980537415, 0.48087432980537415, 0.4754098355770111, 0.4863387942314148, 0.48087432980537415, 0.48087432980537415, 0.4590163826942444, 0.4863387942314148, 0.4863387942314148, 0.44262295961380005, 0.4863387942314148, 0.5027322173118591, 0.48087432980537415, 0.4754098355770111, 0.43169400095939636, 0.48087432980537415, 0.48087432980537415, 0.49180328845977783, 0.49180328845977783, 0.4754098355770111, 0.48087432980537415, 0.45355191826820374, 0.46994534134864807, 0.4754098355770111, 0.49180328845977783, 0.46994534134864807, 0.4863387942314148, 0.4863387942314148, 0.4972677528858185, 0.4754098355770111, 0.4590163826942444, 0.46994534134864807, 0.48087432980537415, 0.4863387942314148, 0.48087432980537415, 0.4754098355770111, 0.4754098355770111, 0.4480874240398407, 0.49180328845977783, 0.4590163826942444, 0.46994534134864807, 0.4754098355770111, 0.4754098355770111, 0.48087432980537415, 0.46994534134864807, 0.49180328845977783, 0.48087432980537415, 0.48087432980537415, 0.4863387942314148, 0.4754098355770111, 0.4863387942314148, 0.4863387942314148, 0.4863387942314148, 0.4972677528858185, 0.4590163826942444, 0.4754098355770111, 0.46994534134864807, 0.4644808769226074, 0.4863387942314148, 0.45355191826820374, 0.48087432980537415, 0.49180328845977783, 0.48087432980537415, 0.4972677528858185, 0.46994534134864807, 0.45355191826820374, 0.4972677528858185, 0.4863387942314148, 0.4590163826942444, 0.4590163826942444, 0.48087432980537415, 0.48087432980537415, 0.4480874240398407, 0.4863387942314148, 0.48087432980537415, 0.5081967115402222, 0.46994534134864807, 0.48087432980537415, 0.4863387942314148, 0.4972677528858185, 0.48087432980537415, 0.46994534134864807, 0.49180328845977783, 0.4754098355770111, 0.49180328845977783, 0.4863387942314148, 0.46994534134864807, 0.49180328845977783, 0.48087432980537415, 0.4863387942314148, 0.4863387942314148, 0.46994534134864807, 0.48087432980537415, 0.49180328845977783, 0.4863387942314148, 0.49180328845977783, 0.4754098355770111, 0.4863387942314148, 0.4972677528858185, 0.5027322173118591, 0.46994534134864807, 0.49180328845977783, 0.48087432980537415, 0.49180328845977783, 0.4644808769226074, 0.4754098355770111, 0.48087432980537415, 0.46994534134864807, 0.4972677528858185, 0.4863387942314148, 0.4754098355770111, 0.5027322173118591, 0.5081967115402222, 0.4754098355770111, 0.4754098355770111, 0.4754098355770111, 0.48087432980537415, 0.48087432980537415, 0.49180328845977783, 0.48087432980537415, 0.49180328845977783, 0.5027322173118591, 0.48087432980537415, 0.48087432980537415, 0.4754098355770111, 0.5027322173118591, 0.49180328845977783, 0.49180328845977783, 0.4972677528858185, 0.49180328845977783, 0.49180328845977783, 0.4754098355770111, 0.4754098355770111, 0.49180328845977783, 0.49180328845977783, 0.4972677528858185, 0.46994534134864807, 0.46994534134864807, 0.48087432980537415, 0.4972677528858185, 0.49180328845977783, 0.48087432980537415, 0.48087432980537415, 0.49180328845977783, 0.4863387942314148, 0.49180328845977783, 0.5027322173118591, 0.5027322173118591, 0.4754098355770111, 0.5081967115402222, 0.4754098355770111, 0.4863387942314148, 0.49180328845977783, 0.5081967115402222, 0.49180328845977783, 0.48087432980537415, 0.5081967115402222, 0.4972677528858185, 0.4754098355770111, 0.46994534134864807, 0.4644808769226074, 0.5081967115402222, 0.4972677528858185, 0.4754098355770111, 0.4863387942314148, 0.49180328845977783, 0.5027322173118591, 0.4863387942314148, 0.4972677528858185, 0.49180328845977783, 0.49180328845977783, 0.49180328845977783, 0.5027322173118591, 0.5081967115402222, 0.4863387942314148, 0.4863387942314148, 0.49180328845977783, 0.4754098355770111, 0.5027322173118591, 0.4754098355770111, 0.48087432980537415, 0.5300546288490295, 0.5027322173118591, 0.4863387942314148, 0.48087432980537415, 0.4863387942314148, 0.49180328845977783, 0.4972677528858185, 0.5191256999969482, 0.4972677528858185, 0.4972677528858185, 0.4972677528858185, 0.5081967115402222, 0.4863387942314148, 0.49180328845977783, 0.49180328845977783, 0.49180328845977783, 0.4754098355770111, 0.4972677528858185, 0.4972677528858185, 0.4972677528858185, 0.4863387942314148, 0.49180328845977783, 0.5081967115402222, 0.4863387942314148, 0.49180328845977783, 0.5027322173118591, 0.4863387942314148, 0.49180328845977783, 0.49180328845977783, 0.49180328845977783, 0.49180328845977783, 0.4863387942314148, 0.4863387942314148, 0.4863387942314148, 0.5191256999969482, 0.49180328845977783, 0.4863387942314148, 0.5027322173118591, 0.49180328845977783, 0.5081967115402222, 0.49180328845977783, 0.4972677528858185, 0.5027322173118591, 0.5081967115402222, 0.4863387942314148, 0.49180328845977783, 0.49180328845977783, 0.4863387942314148, 0.5191256999969482, 0.4863387942314148, 0.48087432980537415, 0.4863387942314148, 0.4863387942314148, 0.5027322173118591, 0.4863387942314148, 0.4754098355770111, 0.5027322173118591, 0.4972677528858185, 0.4863387942314148, 0.5136612057685852, 0.4754098355770111, 0.49180328845977783, 0.5136612057685852, 0.49180328845977783, 0.4972677528858185, 0.4863387942314148, 0.5027322173118591, 0.4863387942314148, 0.5081967115402222, 0.5081967115402222, 0.48087432980537415, 0.48087432980537415, 0.5081967115402222, 0.5081967115402222, 0.4863387942314148, 0.49180328845977783, 0.5081967115402222, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.49180328845977783, 0.4863387942314148, 0.4863387942314148, 0.48087432980537415, 0.49180328845977783, 0.49180328845977783, 0.4972677528858185, 0.48087432980537415, 0.4972677528858185, 0.5191256999969482, 0.4863387942314148, 0.5136612057685852, 0.4754098355770111, 0.4863387942314148, 0.48087432980537415, 0.49180328845977783, 0.4972677528858185, 0.4754098355770111, 0.48087432980537415, 0.4972677528858185, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.5081967115402222, 0.5191256999969482, 0.5191256999969482, 0.4754098355770111, 0.5245901346206665, 0.5136612057685852, 0.48087432980537415, 0.5136612057685852, 0.5081967115402222, 0.4863387942314148, 0.48087432980537415, 0.4972677528858185, 0.46994534134864807, 0.5245901346206665, 0.4972677528858185, 0.49180328845977783, 0.49180328845977783, 0.46994534134864807, 0.5027322173118591, 0.49180328845977783, 0.48087432980537415, 0.4754098355770111, 0.5027322173118591, 0.4644808769226074, 0.5081967115402222, 0.5081967115402222, 0.5081967115402222, 0.4644808769226074, 0.4972677528858185, 0.5027322173118591, 0.49180328845977783, 0.4863387942314148, 0.5027322173118591, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.5081967115402222, 0.5191256999969482, 0.49180328845977783, 0.4754098355770111, 0.4972677528858185, 0.4972677528858185, 0.4972677528858185, 0.46994534134864807, 0.49180328845977783, 0.49180328845977783, 0.5081967115402222, 0.5027322173118591, 0.48087432980537415, 0.4863387942314148, 0.4863387942314148, 0.48087432980537415, 0.5245901346206665, 0.49180328845977783, 0.5245901346206665, 0.5136612057685852, 0.4863387942314148, 0.5027322173118591, 0.5300546288490295, 0.4972677528858185, 0.5081967115402222, 0.5027322173118591, 0.48087432980537415, 0.5081967115402222, 0.5191256999969482, 0.4972677528858185, 0.5027322173118591, 0.4972677528858185, 0.5027322173118591, 0.4972677528858185, 0.5136612057685852, 0.5136612057685852, 0.5191256999969482, 0.5136612057685852, 0.5081967115402222, 0.5136612057685852, 0.5027322173118591, 0.4972677528858185, 0.5191256999969482, 0.4754098355770111, 0.5081967115402222, 0.5300546288490295, 0.46994534134864807, 0.5136612057685852, 0.4972677528858185, 0.49180328845977783, 0.5136612057685852, 0.49180328845977783, 0.4972677528858185, 0.5191256999969482, 0.5136612057685852, 0.4863387942314148, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.49180328845977783, 0.5081967115402222, 0.5191256999969482, 0.4972677528858185, 0.5191256999969482, 0.5136612057685852, 0.4863387942314148, 0.5464481115341187, 0.5027322173118591, 0.4972677528858185, 0.48087432980537415, 0.5355191230773926, 0.5136612057685852, 0.4863387942314148, 0.5191256999969482, 0.4972677528858185, 0.4972677528858185, 0.5355191230773926, 0.5027322173118591, 0.5136612057685852, 0.4863387942314148, 0.5027322173118591, 0.5136612057685852, 0.5300546288490295, 0.4863387942314148, 0.5136612057685852, 0.4972677528858185, 0.5300546288490295, 0.5136612057685852, 0.5027322173118591, 0.5027322173118591, 0.5081967115402222, 0.5245901346206665, 0.5245901346206665, 0.49180328845977783, 0.5027322173118591, 0.49180328845977783, 0.5409836173057556, 0.5081967115402222, 0.5136612057685852, 0.49180328845977783, 0.49180328845977783, 0.4972677528858185, 0.4863387942314148, 0.49180328845977783, 0.5027322173118591, 0.5081967115402222, 0.5027322173118591, 0.5027322173118591, 0.5355191230773926, 0.4863387942314148, 0.5081967115402222, 0.5300546288490295, 0.5245901346206665, 0.49180328845977783, 0.5027322173118591, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5027322173118591, 0.5081967115402222, 0.49180328845977783, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.5081967115402222, 0.5136612057685852, 0.5027322173118591, 0.5245901346206665, 0.5027322173118591, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.49180328845977783, 0.5081967115402222, 0.5081967115402222, 0.5464481115341187, 0.5027322173118591, 0.4972677528858185, 0.5245901346206665, 0.4972677528858185, 0.49180328845977783, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5136612057685852, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.5245901346206665, 0.5464481115341187, 0.5245901346206665, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5191256999969482, 0.5136612057685852, 0.5300546288490295, 0.5136612057685852, 0.48087432980537415, 0.5300546288490295, 0.5355191230773926, 0.5027322173118591, 0.5245901346206665, 0.5409836173057556, 0.5081967115402222, 0.4972677528858185, 0.5245901346206665, 0.5519125461578369, 0.5136612057685852, 0.5081967115402222, 0.46994534134864807, 0.5191256999969482, 0.5027322173118591, 0.4754098355770111, 0.5409836173057556, 0.5409836173057556, 0.5191256999969482, 0.5136612057685852, 0.5300546288490295, 0.5191256999969482, 0.49180328845977783, 0.5136612057685852, 0.5464481115341187, 0.5136612057685852, 0.5300546288490295, 0.5027322173118591, 0.5191256999969482, 0.5519125461578369, 0.5136612057685852, 0.5027322173118591, 0.5245901346206665, 0.5081967115402222, 0.5573770403862, 0.5136612057685852, 0.5300546288490295, 0.5136612057685852, 0.4863387942314148, 0.5081967115402222, 0.5136612057685852, 0.4972677528858185, 0.5409836173057556, 0.5136612057685852, 0.5245901346206665, 0.4972677528858185, 0.5027322173118591, 0.5136612057685852, 0.5027322173118591, 0.5355191230773926, 0.5300546288490295, 0.5245901346206665, 0.5191256999969482, 0.5027322173118591, 0.4972677528858185, 0.5136612057685852, 0.5136612057685852, 0.5136612057685852, 0.49180328845977783, 0.5191256999969482, 0.5027322173118591, 0.4972677528858185, 0.5245901346206665, 0.5027322173118591, 0.49180328845977783, 0.5136612057685852, 0.5136612057685852, 0.5191256999969482, 0.49180328845977783, 0.4972677528858185, 0.5136612057685852, 0.5027322173118591, 0.5027322173118591, 0.5136612057685852, 0.5191256999969482, 0.5136612057685852, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.4972677528858185, 0.5136612057685852, 0.5191256999969482, 0.4972677528858185, 0.5191256999969482, 0.5191256999969482, 0.4972677528858185, 0.5081967115402222, 0.49180328845977783, 0.5027322173118591, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5245901346206665, 0.5081967115402222, 0.5027322173118591, 0.5136612057685852, 0.5191256999969482, 0.4972677528858185, 0.4972677528858185, 0.5191256999969482, 0.5300546288490295, 0.48087432980537415, 0.5081967115402222, 0.4972677528858185, 0.4972677528858185, 0.49180328845977783, 0.5027322173118591, 0.5409836173057556, 0.5245901346206665, 0.48087432980537415, 0.5081967115402222, 0.5027322173118591, 0.4972677528858185, 0.5027322173118591, 0.5191256999969482, 0.5191256999969482, 0.5081967115402222, 0.5245901346206665, 0.5136612057685852, 0.5027322173118591, 0.5245901346206665, 0.5191256999969482, 0.4972677528858185, 0.5081967115402222, 0.5136612057685852, 0.4972677528858185, 0.5245901346206665, 0.5191256999969482, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5191256999969482, 0.5027322173118591, 0.4972677528858185, 0.4972677528858185, 0.5245901346206665, 0.5136612057685852, 0.4972677528858185, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.4972677528858185, 0.5191256999969482, 0.49180328845977783, 0.5136612057685852, 0.49180328845977783, 0.5027322173118591, 0.5081967115402222, 0.5081967115402222, 0.4863387942314148, 0.5136612057685852, 0.5245901346206665, 0.5027322173118591, 0.5191256999969482, 0.4863387942314148, 0.5081967115402222, 0.5081967115402222, 0.4863387942314148, 0.5300546288490295, 0.48087432980537415, 0.5136612057685852, 0.5245901346206665, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.4863387942314148, 0.5355191230773926, 0.5245901346206665, 0.49180328845977783, 0.5136612057685852, 0.4863387942314148, 0.5191256999969482, 0.48087432980537415, 0.49180328845977783, 0.5136612057685852, 0.49180328845977783, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5081967115402222, 0.5136612057685852, 0.5027322173118591, 0.5245901346206665, 0.5191256999969482, 0.4972677528858185, 0.5027322173118591, 0.5027322173118591, 0.48087432980537415, 0.5081967115402222, 0.48087432980537415, 0.5027322173118591, 0.49180328845977783, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.4863387942314148, 0.5191256999969482, 0.5081967115402222, 0.5245901346206665, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.4972677528858185, 0.5027322173118591, 0.5191256999969482, 0.5136612057685852, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.5136612057685852, 0.5081967115402222, 0.49180328845977783, 0.4972677528858185, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5300546288490295, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.49180328845977783, 0.5136612057685852, 0.5355191230773926, 0.4972677528858185, 0.5027322173118591, 0.5136612057685852, 0.5191256999969482, 0.5136612057685852, 0.4863387942314148, 0.5136612057685852, 0.5245901346206665, 0.4972677528858185, 0.4972677528858185, 0.5191256999969482, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5245901346206665, 0.5027322173118591, 0.5191256999969482, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.5136612057685852, 0.5081967115402222, 0.5355191230773926, 0.5136612057685852, 0.5081967115402222, 0.4972677528858185, 0.49180328845977783, 0.4972677528858185, 0.4863387942314148, 0.49180328845977783, 0.5027322173118591, 0.5081967115402222, 0.5300546288490295, 0.5027322173118591, 0.5300546288490295, 0.5191256999969482, 0.5081967115402222, 0.5191256999969482, 0.5191256999969482, 0.4972677528858185, 0.5191256999969482, 0.5136612057685852, 0.5081967115402222, 0.4972677528858185, 0.5191256999969482, 0.49180328845977783, 0.5027322173118591, 0.49180328845977783, 0.49180328845977783, 0.5355191230773926, 0.5191256999969482, 0.5081967115402222, 0.49180328845977783, 0.49180328845977783, 0.5081967115402222, 0.5300546288490295, 0.5191256999969482, 0.5464481115341187, 0.5081967115402222, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.5136612057685852, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.5191256999969482, 0.5027322173118591, 0.5136612057685852, 0.5027322173118591, 0.5081967115402222, 0.5136612057685852, 0.5191256999969482, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.5245901346206665, 0.5136612057685852, 0.5081967115402222, 0.49180328845977783, 0.4863387942314148, 0.5136612057685852, 0.4863387942314148, 0.5027322173118591, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5136612057685852, 0.5027322173118591, 0.5245901346206665, 0.5081967115402222, 0.5136612057685852, 0.5191256999969482, 0.48087432980537415, 0.4972677528858185, 0.5027322173118591, 0.4863387942314148, 0.5136612057685852, 0.4972677528858185, 0.49180328845977783, 0.5136612057685852, 0.5136612057685852, 0.49180328845977783, 0.5081967115402222, 0.4972677528858185, 0.5136612057685852, 0.5136612057685852, 0.5245901346206665, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.4972677528858185, 0.5191256999969482, 0.49180328845977783, 0.4863387942314148, 0.5245901346206665, 0.4972677528858185, 0.5191256999969482, 0.49180328845977783, 0.5136612057685852, 0.4863387942314148, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.5136612057685852, 0.5245901346206665, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.5081967115402222, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5136612057685852, 0.5191256999969482, 0.5081967115402222, 0.5191256999969482, 0.49180328845977783, 0.5136612057685852, 0.5245901346206665, 0.5081967115402222, 0.5191256999969482, 0.5027322173118591, 0.4972677528858185, 0.5136612057685852, 0.5027322173118591, 0.5081967115402222, 0.5245901346206665, 0.5081967115402222, 0.5136612057685852, 0.4972677528858185, 0.5027322173118591, 0.5191256999969482, 0.5136612057685852, 0.5245901346206665, 0.4972677528858185, 0.5245901346206665, 0.4863387942314148, 0.5027322173118591, 0.5191256999969482, 0.4863387942314148, 0.5245901346206665, 0.4972677528858185, 0.49180328845977783, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5081967115402222, 0.5027322173118591, 0.4972677528858185, 0.4972677528858185, 0.5136612057685852, 0.5136612057685852, 0.4972677528858185, 0.4972677528858185, 0.5191256999969482, 0.5191256999969482, 0.5081967115402222, 0.5081967115402222, 0.5081967115402222, 0.5245901346206665, 0.5136612057685852, 0.4972677528858185, 0.5245901346206665, 0.49180328845977783, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5300546288490295, 0.5027322173118591, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.49180328845977783, 0.5027322173118591, 0.5136612057685852, 0.5191256999969482, 0.49180328845977783, 0.4972677528858185, 0.5081967115402222, 0.5027322173118591, 0.5300546288490295, 0.5191256999969482, 0.5027322173118591, 0.5300546288490295, 0.5136612057685852, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.49180328845977783, 0.49180328845977783, 0.5027322173118591, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.5136612057685852, 0.4972677528858185, 0.49180328845977783, 0.5027322173118591, 0.4863387942314148, 0.5136612057685852, 0.5191256999969482, 0.5081967115402222, 0.4972677528858185, 0.5300546288490295, 0.5136612057685852, 0.49180328845977783, 0.5081967115402222, 0.5027322173118591, 0.5027322173118591, 0.5081967115402222, 0.5081967115402222, 0.5136612057685852, 0.5027322173118591, 0.5027322173118591, 0.49180328845977783, 0.4972677528858185, 0.5027322173118591, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.5245901346206665, 0.5191256999969482, 0.5081967115402222, 0.49180328845977783, 0.5136612057685852, 0.49180328845977783, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.5027322173118591, 0.5081967115402222, 0.4972677528858185, 0.49180328845977783, 0.5081967115402222, 0.5027322173118591, 0.49180328845977783, 0.5191256999969482, 0.5136612057685852, 0.4863387942314148, 0.5027322173118591, 0.5027322173118591, 0.49180328845977783, 0.5136612057685852, 0.5136612057685852, 0.5191256999969482, 0.5191256999969482, 0.5081967115402222, 0.5027322173118591, 0.5191256999969482, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5300546288490295, 0.5191256999969482, 0.5245901346206665, 0.5081967115402222, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.49180328845977783, 0.5081967115402222, 0.5081967115402222, 0.5027322173118591, 0.5027322173118591, 0.5081967115402222, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.5191256999969482, 0.5081967115402222, 0.5081967115402222, 0.49180328845977783, 0.5027322173118591, 0.5191256999969482, 0.4972677528858185, 0.5081967115402222, 0.5191256999969482, 0.5191256999969482, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.4972677528858185, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.4972677528858185, 0.5027322173118591, 0.5081967115402222, 0.5136612057685852, 0.5245901346206665, 0.5245901346206665, 0.5081967115402222, 0.49180328845977783, 0.5245901346206665, 0.5300546288490295, 0.5136612057685852, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.4972677528858185, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.4972677528858185, 0.5081967115402222, 0.5355191230773926, 0.5081967115402222, 0.5081967115402222, 0.5245901346206665, 0.5081967115402222, 0.5300546288490295, 0.5081967115402222, 0.5136612057685852, 0.5027322173118591, 0.5191256999969482, 0.5245901346206665, 0.5136612057685852, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5136612057685852, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.5136612057685852, 0.49180328845977783, 0.5081967115402222, 0.5191256999969482, 0.5027322173118591, 0.49180328845977783, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5081967115402222, 0.5136612057685852, 0.5027322173118591, 0.5027322173118591, 0.49180328845977783, 0.5245901346206665, 0.4972677528858185, 0.5245901346206665, 0.4972677528858185, 0.49180328845977783, 0.5136612057685852, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5245901346206665, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.4972677528858185, 0.5191256999969482, 0.5027322173118591, 0.5136612057685852, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.5136612057685852, 0.5300546288490295, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5136612057685852, 0.5245901346206665, 0.5027322173118591, 0.5027322173118591, 0.5191256999969482, 0.5136612057685852, 0.5081967115402222, 0.5136612057685852, 0.49180328845977783, 0.5136612057685852, 0.5081967115402222, 0.5027322173118591, 0.5136612057685852, 0.4863387942314148, 0.4972677528858185, 0.5027322173118591, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.49180328845977783, 0.5081967115402222, 0.49180328845977783, 0.5136612057685852, 0.5136612057685852, 0.5245901346206665, 0.5081967115402222, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.5136612057685852, 0.4863387942314148, 0.5081967115402222, 0.5191256999969482, 0.4972677528858185, 0.5081967115402222, 0.5081967115402222, 0.5245901346206665, 0.5027322173118591, 0.5136612057685852, 0.5027322173118591, 0.5081967115402222, 0.5081967115402222, 0.5136612057685852, 0.5245901346206665, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5081967115402222, 0.5081967115402222, 0.4972677528858185, 0.5191256999969482, 0.4972677528858185, 0.5081967115402222, 0.5191256999969482, 0.4972677528858185, 0.49180328845977783, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5081967115402222, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.5081967115402222, 0.5191256999969482, 0.5191256999969482, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5027322173118591, 0.4972677528858185, 0.4863387942314148, 0.5081967115402222, 0.5081967115402222, 0.5136612057685852, 0.4972677528858185, 0.5191256999969482, 0.5191256999969482, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.5136612057685852, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.5136612057685852, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.5027322173118591, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.4972677528858185, 0.5191256999969482, 0.5191256999969482, 0.4972677528858185, 0.5027322173118591, 0.5081967115402222, 0.4972677528858185, 0.5027322173118591, 0.5191256999969482, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.5081967115402222, 0.5245901346206665, 0.4863387942314148, 0.5136612057685852, 0.5136612057685852, 0.5191256999969482, 0.5191256999969482, 0.5136612057685852, 0.5027322173118591, 0.5245901346206665, 0.5191256999969482, 0.5191256999969482, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.5136612057685852, 0.5191256999969482, 0.5081967115402222, 0.5081967115402222, 0.5136612057685852, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.5027322173118591, 0.49180328845977783, 0.4972677528858185, 0.5136612057685852, 0.5191256999969482, 0.4972677528858185, 0.5027322173118591, 0.49180328845977783, 0.4972677528858185, 0.4972677528858185, 0.49180328845977783, 0.5136612057685852, 0.5136612057685852, 0.4972677528858185, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.46994534134864807, 0.5136612057685852, 0.5191256999969482, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.4863387942314148, 0.4972677528858185, 0.5027322173118591, 0.5191256999969482, 0.5136612057685852, 0.5136612057685852, 0.5081967115402222, 0.4972677528858185, 0.5081967115402222, 0.5136612057685852, 0.5191256999969482, 0.5136612057685852, 0.5136612057685852, 0.5300546288490295, 0.5191256999969482, 0.5191256999969482, 0.4972677528858185, 0.4972677528858185, 0.4863387942314148, 0.5191256999969482, 0.5027322173118591, 0.4972677528858185, 0.49180328845977783, 0.5191256999969482, 0.5191256999969482, 0.5191256999969482, 0.49180328845977783, 0.5245901346206665, 0.5245901346206665, 0.5136612057685852, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.5027322173118591, 0.5245901346206665, 0.5191256999969482, 0.4972677528858185, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.5136612057685852, 0.5191256999969482, 0.5136612057685852, 0.5191256999969482, 0.5027322173118591, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5081967115402222, 0.5191256999969482, 0.4972677528858185, 0.5191256999969482, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.5136612057685852, 0.5027322173118591, 0.49180328845977783, 0.5136612057685852, 0.5245901346206665, 0.5027322173118591, 0.5136612057685852, 0.5136612057685852, 0.5081967115402222, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.5136612057685852, 0.4972677528858185, 0.49180328845977783, 0.5027322173118591, 0.5027322173118591, 0.5191256999969482, 0.5081967115402222, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.5245901346206665, 0.5245901346206665, 0.5081967115402222, 0.5081967115402222, 0.5191256999969482, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5027322173118591, 0.5245901346206665, 0.5081967115402222, 0.5245901346206665, 0.5081967115402222, 0.5191256999969482, 0.4972677528858185, 0.5136612057685852, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.5027322173118591, 0.5081967115402222, 0.49180328845977783, 0.5081967115402222, 0.4972677528858185, 0.4972677528858185, 0.5136612057685852, 0.5355191230773926, 0.5136612057685852, 0.4972677528858185, 0.5136612057685852, 0.5191256999969482, 0.5355191230773926, 0.5245901346206665, 0.5027322173118591, 0.5027322173118591, 0.4972677528858185, 0.5081967115402222, 0.5191256999969482, 0.5191256999969482, 0.5027322173118591, 0.5027322173118591, 0.5136612057685852, 0.5136612057685852, 0.5300546288490295, 0.5191256999969482, 0.5027322173118591, 0.5136612057685852, 0.49180328845977783, 0.5136612057685852, 0.5191256999969482, 0.5245901346206665, 0.5300546288490295, 0.5027322173118591, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5191256999969482, 0.5191256999969482, 0.5191256999969482, 0.5191256999969482, 0.5300546288490295, 0.5191256999969482, 0.5191256999969482, 0.5355191230773926, 0.5136612057685852, 0.5136612057685852, 0.5136612057685852, 0.5136612057685852, 0.5136612057685852, 0.4972677528858185, 0.4972677528858185, 0.5136612057685852, 0.5081967115402222, 0.5191256999969482, 0.5081967115402222, 0.5027322173118591, 0.5191256999969482, 0.5191256999969482, 0.5191256999969482, 0.5245901346206665, 0.5136612057685852, 0.5245901346206665, 0.5191256999969482, 0.5136612057685852, 0.5081967115402222, 0.5081967115402222, 0.5081967115402222, 0.5300546288490295, 0.5245901346206665, 0.5081967115402222, 0.5245901346206665, 0.5027322173118591, 0.5191256999969482, 0.5027322173118591, 0.5081967115402222, 0.5245901346206665, 0.48087432980537415, 0.5191256999969482, 0.5027322173118591]}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mratio\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mratio\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlogical\u001b[0m \u001b[0mto\u001b[0m \u001b[0mphysical\u001b[0m \u001b[0mpixels\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(figure, print_method)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_without_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_non_interactive_terminal_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mMetadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlatin\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mencodable\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mAccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mspecification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mshorter\u001b[0m \u001b[0mthan\u001b[0m \u001b[0;36m79\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_png' from 'matplotlib' (/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#sigmoid\n",
        "print(cnnhistory.history)\n",
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1Hgr-xHsHGJ"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "h0oVSxMcsOBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6aa6cf-ab40-4109-e919-a81b0b69d92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved trained model at /content/drive/My Drive/audio-dataset/Audio-dataset/Rawdata/../saved_models/Emotion_Voice_Detection_Model.h5 \n"
          ]
        }
      ],
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = os.path.join(os.getcwd(), \"../saved_models\")\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Predicting emotions on the test data"
      ],
      "metadata": {
        "id": "rQgvfkq9s_-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_testcnn, \n",
        "                         batch_size=32, \n",
        "                         verbose=1)\n",
        "\n",
        "preds\n",
        "\n",
        "preds1 = preds.argmax(axis=1)\n",
        "\n",
        "preds1\n",
        "\n",
        "abc = preds.astype(int).flatten()\n",
        "\n",
        "predictions = (lb.inverse_transform((abc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KjULKwstJ0s",
        "outputId": "83a3e959-d514-48e0-b61d-a6e6818b097f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
        "#print(preddf[200:230])"
      ],
      "metadata": {
        "id": "UWCxMScrt9RG"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual=y_test.argmax(axis=1)\n",
        "abc123 = actual.astype(int).flatten()\n",
        "actualvalues = (lb.inverse_transform((abc123)))\n",
        "\n",
        "actualdf = pd.DataFrame({'actualvalues': actualvalues})\n",
        "print(actualdf[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejkmMC-SuTkf",
        "outputId": "0782203e-9681-48c6-d150-d6db0fa402e4"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   actualvalues\n",
            "0     male_calm\n",
            "1  female_angry\n",
            "2    male_happy\n",
            "3    male_angry\n",
            "4   female_calm\n",
            "5    female_sad\n",
            "6    male_angry\n",
            "7   female_calm\n",
            "8   female_calm\n",
            "9  female_happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finaldf = actualdf.join(preddf)\n",
        "\n",
        "print(finaldf[170:180])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKVJ0gUKuqp7",
        "outputId": "366ff15f-d02e-4e8f-b99b-596aba45716c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     actualvalues predictedvalues\n",
            "170    male_angry    female_angry\n",
            "171  female_happy    female_angry\n",
            "172  female_happy    female_angry\n",
            "173  male_fearful    female_angry\n",
            "174     male_calm    female_angry\n",
            "175   female_calm    female_angry\n",
            "176     male_calm    female_angry\n",
            "177    male_angry    female_angry\n",
            "178    female_sad    female_angry\n",
            "179   female_calm    female_angry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finaldf.groupby('actualvalues').count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frhEG9IBu-kY",
        "outputId": "2b81c653-d388-4880-dc52-d3097bcee098"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                predictedvalues\n",
            "actualvalues                   \n",
            "female_angry                 17\n",
            "female_calm                  19\n",
            "female_fearful               16\n",
            "female_happy                 23\n",
            "female_sad                   22\n",
            "male_angry                   17\n",
            "male_calm                    19\n",
            "male_fearful                 13\n",
            "male_happy                   16\n",
            "male_sad                     21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finaldf.groupby('predictedvalues').count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76K2DFgWvJcA",
        "outputId": "c8d072ce-5625-4747-a137-47ea140b3afc"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 actualvalues\n",
            "predictedvalues              \n",
            "female_angry              183\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "project-id2223.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG8bi/aWacaFA9KupnHPJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}