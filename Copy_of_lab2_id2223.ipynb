{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilstahl97/Scalable-Machine-Learning-and-Deep-Learning-ID2223/blob/notebooks/Copy_of_lab2_id2223.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add dependencies"
      ],
      "metadata": {
        "id": "ka5YiA0Xb98z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install torch\n",
        "!pip install wget\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXUL4qsXGjqT",
        "outputId": "b15a4ac8-4dd6-4dd3-9951-4bb081ab3207"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.14.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.14.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "K7tzQW6ScEcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yugvCJ05CMg9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import wget\n",
        "import json\n",
        "import math\n",
        "import scipy\n",
        "import torch\n",
        "import string\n",
        "import sklearn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import LoggingHandler\n",
        "from sentence_transformers import models, losses, util\n",
        "from sentence_transformers.readers import InputExample\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FygOU3dAb6TA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR1aiNbSpaAa"
      },
      "source": [
        "**Mount Google Drive to load saved models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlNVUAMfpXko",
        "outputId": "e177507c-be29-4fa2-eec3-0e82c43b32d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset exists\n"
          ]
        }
      ],
      "source": [
        "# README - Execute this cell to mount the notebook in your google drive. \n",
        "# Execute the cell and follow the link to sign and, paste the given key in the little text box. The credentials are only available for you. \n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/stsbenchmark\"): \n",
        "  os.mkdir(\"/content/drive/MyDrive/stsbenchmark\")\n",
        "  os.chdir(\"/content/drive/MyDrive/stsbenchmark\")\n",
        "  !git clone https://github.com/emilstahl97/stsbenchmark.git\n",
        "  !git pull\n",
        "else:\n",
        "  print(\"Dataset exists\")\n",
        "  os.chdir(\"/content/drive/MyDrive/stsbenchmark\")\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/stsbenchmark/stsbenchmark/sts-train.csv\"\n",
        "test_path = \"/content/drive/MyDrive/stsbenchmark/stsbenchmark/sts-test.csv\"\n",
        "dev_path = \"/content/drive/MyDrive/stsbenchmark/stsbenchmark/sts-dev.csv\"\n",
        "\n",
        "news_path = \"/content/drive/MyDrive/stsbenchmark/stsbenchmark/news.csv\"\n",
        "\n",
        "# saved models\n",
        "if not os.path.exists(\"/content/drive/MyDrive/id2223/lab2/models\"):\n",
        "  os.makedirs(\"/content/drive/MyDrive/id2223/lab2/models\")\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/id2223/lab2/results\"):\n",
        "  os.makedirs(\"/content/drive/MyDrive/id2223/lab2/results\")\n",
        "\n",
        "regression_model_path = \"/content/drive/MyDrive/id2223/lab2/models/regression_model\"\n",
        "regression_results_path = \"/content/drive/MyDrive/id2223/lab2/models/regression_results\"\n",
        "classification_model_path = \"/content/drive/MyDrive/id2223/lab2/models/classification_model\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHTmDfTnpiYW"
      },
      "source": [
        "## **REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOi6F9ps0biy",
        "outputId": "7848dd88-de3d-42f5-9d0e-3e6550ec034d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "word_embedding_model = models.Transformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rS97EOPBzg8v"
      },
      "outputs": [],
      "source": [
        "columns = ['title', 'type', 'year', 'id', 'score', 'sentence_1', 'sentence_2']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_samples = []\n",
        "test_samples = []\n",
        "dev_samples = []\n",
        "\n",
        "paths = {\"train\": train_path, \"test\": test_path, \"dev\": dev_path}\n",
        "\n",
        "for key, path in paths.items():\n",
        "  with open(path, newline='') as f:\n",
        "    temp = csv.DictReader(f, delimiter='\\t', fieldnames=columns, quoting=csv.QUOTE_NONE)\n",
        "    for row in temp:\n",
        "        score = float(row['score']) / 2.5 - 1 \n",
        "        input_example = InputExample(texts=[row['sentence_1'], row['sentence_2']], label=score)\n",
        "        if (key == \"train\"):\n",
        "          train_samples.append(input_example)\n",
        "        elif key == \"test\":\n",
        "          test_samples.append(input_example)\n",
        "        elif key == \"dev\":\n",
        "          dev_samples.append(input_example)\n",
        "        else:\n",
        "          raise Exception(f\"key {key} not known, exiting\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5k1SO-rlMs8-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOdKI-S91Yb5"
      },
      "source": [
        "Considering the given paper \"*Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "epxEtXh2GMYu"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 16\n",
        "learn_rate = 2e-5\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP5UDhSi2RFh"
      },
      "source": [
        "Mean-pooling strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fIRLPEnrGhG6"
      },
      "outputs": [],
      "source": [
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc11KhHR2nH4"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RozMxge52jC7"
      },
      "outputs": [],
      "source": [
        "# custom model using mean pooling of the word embeddings given as input\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNI9URmg4vCk"
      },
      "source": [
        "Load the training set and define the loss function as the cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iht5w4HJlhtt"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYZlyR-i6z3j"
      },
      "source": [
        "Define the evaluator for the sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JchPxNSDT2f9"
      },
      "outputs": [],
      "source": [
        "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, batch_size=train_batch_size, name='sts-dev')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ2IPb4y7CVk"
      },
      "source": [
        "10% of train dataset for warm-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "g-nlK2kmUCve"
      },
      "outputs": [],
      "source": [
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrrFM8r7Ly9"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": true,
        "id": "TxEXdHtiUDXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0975cbfe-4189-429a-f2a5-1b8b665b0ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained model\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(regression_model_path):\n",
        "    print(\"Loading pre-trained model\")\n",
        "    model = SentenceTransformer(regression_model_path)\n",
        "else:\n",
        "    print(\"Re-training model\")\n",
        "    model = model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "                    optimizer_class=torch.optim.Adam,\n",
        "                    optimizer_params={'lr': learn_rate},\n",
        "                    evaluator=evaluator,\n",
        "                    epochs=num_epochs,\n",
        "                    evaluation_steps=1000,\n",
        "                    warmup_steps=warmup_steps,\n",
        "                    output_path=regression_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foas3A_I981K"
      },
      "source": [
        "**Evaluation on STS benchmark dataset**\n",
        "\n",
        "Mathematical relationship: *cosine_similarity = 1 - cosine_distance*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Qv93RESm_tyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b171d3-3534-4543-8d69-536d15d65c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity with the sentence_transformers library =  0.5263565694181714\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs(\"/content/drive/MyDrive/id2223/lab2/models/regression_results/\")\n",
        "test_eval = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, batch_size=train_batch_size, name='sts-test')\n",
        "c_s = test_eval(model, output_path=regression_results_path)\n",
        "print('Cosine similarity with the sentence_transformers library = ', c_s)\n",
        "\n",
        "# sometimes the result is between 0.7 and 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axyd6hqZ_a5p"
      },
      "source": [
        "Embedding sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fMDBfVfPHlOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c33529-d62a-40bf-cfe2-2f7d99417791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "b'Skipping line 626: expected 7 fields, saw 9\\nSkipping line 627: expected 7 fields, saw 9\\nSkipping line 628: expected 7 fields, saw 9\\nSkipping line 629: expected 7 fields, saw 9\\nSkipping line 630: expected 7 fields, saw 9\\nSkipping line 631: expected 7 fields, saw 9\\nSkipping line 632: expected 7 fields, saw 9\\nSkipping line 633: expected 7 fields, saw 9\\nSkipping line 634: expected 7 fields, saw 9\\nSkipping line 635: expected 7 fields, saw 9\\nSkipping line 636: expected 7 fields, saw 9\\nSkipping line 637: expected 7 fields, saw 9\\nSkipping line 638: expected 7 fields, saw 9\\nSkipping line 639: expected 7 fields, saw 9\\nSkipping line 640: expected 7 fields, saw 9\\nSkipping line 641: expected 7 fields, saw 9\\nSkipping line 642: expected 7 fields, saw 9\\nSkipping line 643: expected 7 fields, saw 9\\nSkipping line 644: expected 7 fields, saw 9\\nSkipping line 645: expected 7 fields, saw 9\\nSkipping line 646: expected 7 fields, saw 9\\nSkipping line 647: expected 7 fields, saw 9\\nSkipping line 648: expected 7 fields, saw 9\\nSkipping line 649: expected 7 fields, saw 9\\nSkipping line 650: expected 7 fields, saw 9\\nSkipping line 651: expected 7 fields, saw 9\\nSkipping line 652: expected 7 fields, saw 9\\nSkipping line 653: expected 7 fields, saw 9\\nSkipping line 654: expected 7 fields, saw 9\\nSkipping line 655: expected 7 fields, saw 9\\nSkipping line 656: expected 7 fields, saw 9\\nSkipping line 657: expected 7 fields, saw 9\\nSkipping line 658: expected 7 fields, saw 9\\nSkipping line 659: expected 7 fields, saw 9\\nSkipping line 660: expected 7 fields, saw 9\\nSkipping line 661: expected 7 fields, saw 9\\nSkipping line 662: expected 7 fields, saw 9\\nSkipping line 663: expected 7 fields, saw 9\\nSkipping line 664: expected 7 fields, saw 9\\nSkipping line 665: expected 7 fields, saw 9\\nSkipping line 666: expected 7 fields, saw 9\\nSkipping line 667: expected 7 fields, saw 9\\nSkipping line 668: expected 7 fields, saw 9\\nSkipping line 669: expected 7 fields, saw 9\\nSkipping line 670: expected 7 fields, saw 9\\nSkipping line 671: expected 7 fields, saw 9\\nSkipping line 672: expected 7 fields, saw 9\\nSkipping line 673: expected 7 fields, saw 9\\nSkipping line 674: expected 7 fields, saw 9\\nSkipping line 675: expected 7 fields, saw 9\\nSkipping line 676: expected 7 fields, saw 9\\nSkipping line 677: expected 7 fields, saw 9\\nSkipping line 678: expected 7 fields, saw 9\\nSkipping line 679: expected 7 fields, saw 9\\nSkipping line 680: expected 7 fields, saw 9\\nSkipping line 681: expected 7 fields, saw 9\\nSkipping line 682: expected 7 fields, saw 9\\nSkipping line 683: expected 7 fields, saw 9\\nSkipping line 684: expected 7 fields, saw 9\\nSkipping line 685: expected 7 fields, saw 9\\nSkipping line 686: expected 7 fields, saw 9\\nSkipping line 687: expected 7 fields, saw 9\\nSkipping line 688: expected 7 fields, saw 9\\nSkipping line 689: expected 7 fields, saw 9\\nSkipping line 690: expected 7 fields, saw 9\\nSkipping line 691: expected 7 fields, saw 9\\nSkipping line 692: expected 7 fields, saw 9\\nSkipping line 693: expected 7 fields, saw 9\\nSkipping line 694: expected 7 fields, saw 9\\nSkipping line 695: expected 7 fields, saw 9\\nSkipping line 696: expected 7 fields, saw 9\\nSkipping line 697: expected 7 fields, saw 9\\nSkipping line 698: expected 7 fields, saw 9\\nSkipping line 699: expected 7 fields, saw 9\\nSkipping line 700: expected 7 fields, saw 9\\nSkipping line 701: expected 7 fields, saw 9\\nSkipping line 702: expected 7 fields, saw 9\\nSkipping line 703: expected 7 fields, saw 9\\nSkipping line 704: expected 7 fields, saw 9\\nSkipping line 705: expected 7 fields, saw 9\\nSkipping line 706: expected 7 fields, saw 9\\nSkipping line 707: expected 7 fields, saw 9\\nSkipping line 708: expected 7 fields, saw 9\\nSkipping line 709: expected 7 fields, saw 9\\nSkipping line 710: expected 7 fields, saw 9\\nSkipping line 711: expected 7 fields, saw 9\\nSkipping line 712: expected 7 fields, saw 9\\nSkipping line 713: expected 7 fields, saw 9\\nSkipping line 714: expected 7 fields, saw 9\\nSkipping line 715: expected 7 fields, saw 9\\nSkipping line 716: expected 7 fields, saw 9\\nSkipping line 717: expected 7 fields, saw 9\\nSkipping line 718: expected 7 fields, saw 9\\nSkipping line 719: expected 7 fields, saw 9\\nSkipping line 720: expected 7 fields, saw 9\\nSkipping line 721: expected 7 fields, saw 9\\nSkipping line 722: expected 7 fields, saw 9\\nSkipping line 723: expected 7 fields, saw 9\\nSkipping line 724: expected 7 fields, saw 9\\nSkipping line 725: expected 7 fields, saw 9\\nSkipping line 726: expected 7 fields, saw 9\\nSkipping line 727: expected 7 fields, saw 9\\nSkipping line 728: expected 7 fields, saw 9\\nSkipping line 729: expected 7 fields, saw 9\\nSkipping line 730: expected 7 fields, saw 9\\nSkipping line 731: expected 7 fields, saw 9\\nSkipping line 732: expected 7 fields, saw 9\\nSkipping line 733: expected 7 fields, saw 9\\nSkipping line 734: expected 7 fields, saw 9\\nSkipping line 735: expected 7 fields, saw 9\\nSkipping line 736: expected 7 fields, saw 9\\nSkipping line 737: expected 7 fields, saw 9\\nSkipping line 738: expected 7 fields, saw 9\\nSkipping line 739: expected 7 fields, saw 9\\nSkipping line 740: expected 7 fields, saw 9\\nSkipping line 741: expected 7 fields, saw 9\\nSkipping line 742: expected 7 fields, saw 9\\nSkipping line 743: expected 7 fields, saw 9\\nSkipping line 744: expected 7 fields, saw 9\\nSkipping line 745: expected 7 fields, saw 9\\nSkipping line 746: expected 7 fields, saw 9\\nSkipping line 747: expected 7 fields, saw 9\\nSkipping line 748: expected 7 fields, saw 9\\nSkipping line 749: expected 7 fields, saw 9\\nSkipping line 750: expected 7 fields, saw 9\\nSkipping line 751: expected 7 fields, saw 9\\nSkipping line 752: expected 7 fields, saw 9\\nSkipping line 753: expected 7 fields, saw 9\\nSkipping line 754: expected 7 fields, saw 9\\nSkipping line 755: expected 7 fields, saw 9\\nSkipping line 756: expected 7 fields, saw 9\\nSkipping line 757: expected 7 fields, saw 9\\nSkipping line 758: expected 7 fields, saw 9\\nSkipping line 759: expected 7 fields, saw 9\\nSkipping line 760: expected 7 fields, saw 9\\nSkipping line 761: expected 7 fields, saw 9\\nSkipping line 762: expected 7 fields, saw 9\\nSkipping line 763: expected 7 fields, saw 9\\nSkipping line 764: expected 7 fields, saw 9\\nSkipping line 765: expected 7 fields, saw 9\\nSkipping line 766: expected 7 fields, saw 9\\nSkipping line 767: expected 7 fields, saw 9\\nSkipping line 768: expected 7 fields, saw 9\\nSkipping line 769: expected 7 fields, saw 9\\nSkipping line 770: expected 7 fields, saw 9\\nSkipping line 771: expected 7 fields, saw 9\\nSkipping line 772: expected 7 fields, saw 9\\nSkipping line 773: expected 7 fields, saw 9\\nSkipping line 774: expected 7 fields, saw 9\\nSkipping line 775: expected 7 fields, saw 9\\nSkipping line 776: expected 7 fields, saw 9\\nSkipping line 777: expected 7 fields, saw 9\\nSkipping line 778: expected 7 fields, saw 9\\nSkipping line 779: expected 7 fields, saw 9\\nSkipping line 780: expected 7 fields, saw 9\\nSkipping line 781: expected 7 fields, saw 9\\nSkipping line 782: expected 7 fields, saw 9\\nSkipping line 783: expected 7 fields, saw 9\\nSkipping line 784: expected 7 fields, saw 9\\nSkipping line 785: expected 7 fields, saw 9\\nSkipping line 786: expected 7 fields, saw 9\\nSkipping line 787: expected 7 fields, saw 9\\nSkipping line 788: expected 7 fields, saw 9\\nSkipping line 789: expected 7 fields, saw 9\\nSkipping line 790: expected 7 fields, saw 9\\nSkipping line 791: expected 7 fields, saw 9\\nSkipping line 792: expected 7 fields, saw 9\\nSkipping line 793: expected 7 fields, saw 9\\nSkipping line 794: expected 7 fields, saw 9\\nSkipping line 795: expected 7 fields, saw 9\\nSkipping line 796: expected 7 fields, saw 9\\nSkipping line 797: expected 7 fields, saw 9\\nSkipping line 798: expected 7 fields, saw 9\\nSkipping line 799: expected 7 fields, saw 9\\nSkipping line 800: expected 7 fields, saw 9\\nSkipping line 801: expected 7 fields, saw 9\\nSkipping line 802: expected 7 fields, saw 9\\nSkipping line 803: expected 7 fields, saw 9\\nSkipping line 804: expected 7 fields, saw 9\\nSkipping line 805: expected 7 fields, saw 9\\nSkipping line 806: expected 7 fields, saw 9\\nSkipping line 807: expected 7 fields, saw 9\\nSkipping line 808: expected 7 fields, saw 9\\nSkipping line 809: expected 7 fields, saw 9\\nSkipping line 810: expected 7 fields, saw 9\\nSkipping line 811: expected 7 fields, saw 9\\nSkipping line 812: expected 7 fields, saw 9\\nSkipping line 813: expected 7 fields, saw 9\\nSkipping line 814: expected 7 fields, saw 9\\nSkipping line 815: expected 7 fields, saw 9\\nSkipping line 816: expected 7 fields, saw 9\\nSkipping line 817: expected 7 fields, saw 9\\nSkipping line 818: expected 7 fields, saw 9\\nSkipping line 819: expected 7 fields, saw 9\\nSkipping line 820: expected 7 fields, saw 9\\nSkipping line 821: expected 7 fields, saw 9\\nSkipping line 822: expected 7 fields, saw 9\\nSkipping line 823: expected 7 fields, saw 9\\nSkipping line 824: expected 7 fields, saw 9\\nSkipping line 825: expected 7 fields, saw 9\\nSkipping line 826: expected 7 fields, saw 9\\nSkipping line 827: expected 7 fields, saw 9\\nSkipping line 828: expected 7 fields, saw 9\\nSkipping line 829: expected 7 fields, saw 9\\nSkipping line 830: expected 7 fields, saw 9\\nSkipping line 831: expected 7 fields, saw 9\\nSkipping line 832: expected 7 fields, saw 9\\nSkipping line 833: expected 7 fields, saw 9\\nSkipping line 834: expected 7 fields, saw 9\\nSkipping line 835: expected 7 fields, saw 9\\nSkipping line 836: expected 7 fields, saw 9\\nSkipping line 837: expected 7 fields, saw 9\\nSkipping line 838: expected 7 fields, saw 9\\nSkipping line 839: expected 7 fields, saw 9\\nSkipping line 840: expected 7 fields, saw 9\\nSkipping line 841: expected 7 fields, saw 9\\nSkipping line 842: expected 7 fields, saw 9\\nSkipping line 843: expected 7 fields, saw 9\\nSkipping line 844: expected 7 fields, saw 9\\nSkipping line 845: expected 7 fields, saw 9\\nSkipping line 846: expected 7 fields, saw 9\\nSkipping line 847: expected 7 fields, saw 9\\nSkipping line 848: expected 7 fields, saw 9\\nSkipping line 849: expected 7 fields, saw 9\\nSkipping line 850: expected 7 fields, saw 9\\nSkipping line 851: expected 7 fields, saw 9\\nSkipping line 852: expected 7 fields, saw 9\\nSkipping line 853: expected 7 fields, saw 9\\nSkipping line 854: expected 7 fields, saw 9\\nSkipping line 855: expected 7 fields, saw 9\\nSkipping line 856: expected 7 fields, saw 9\\nSkipping line 857: expected 7 fields, saw 9\\nSkipping line 858: expected 7 fields, saw 9\\nSkipping line 859: expected 7 fields, saw 9\\nSkipping line 860: expected 7 fields, saw 9\\nSkipping line 861: expected 7 fields, saw 9\\nSkipping line 862: expected 7 fields, saw 9\\nSkipping line 863: expected 7 fields, saw 9\\nSkipping line 864: expected 7 fields, saw 9\\nSkipping line 865: expected 7 fields, saw 9\\nSkipping line 866: expected 7 fields, saw 9\\nSkipping line 867: expected 7 fields, saw 9\\nSkipping line 868: expected 7 fields, saw 9\\nSkipping line 869: expected 7 fields, saw 9\\nSkipping line 870: expected 7 fields, saw 9\\nSkipping line 871: expected 7 fields, saw 9\\nSkipping line 872: expected 7 fields, saw 9\\nSkipping line 873: expected 7 fields, saw 9\\nSkipping line 874: expected 7 fields, saw 9\\nSkipping line 875: expected 7 fields, saw 9\\nSkipping line 876: expected 7 fields, saw 9\\nSkipping line 877: expected 7 fields, saw 9\\nSkipping line 878: expected 7 fields, saw 9\\nSkipping line 879: expected 7 fields, saw 9\\nSkipping line 1350: expected 7 fields, saw 9\\nSkipping line 1351: expected 7 fields, saw 9\\nSkipping line 1352: expected 7 fields, saw 9\\nSkipping line 1353: expected 7 fields, saw 9\\nSkipping line 1354: expected 7 fields, saw 9\\nSkipping line 1355: expected 7 fields, saw 9\\nSkipping line 1356: expected 7 fields, saw 9\\nSkipping line 1357: expected 7 fields, saw 9\\nSkipping line 1358: expected 7 fields, saw 9\\nSkipping line 1359: expected 7 fields, saw 9\\nSkipping line 1360: expected 7 fields, saw 9\\nSkipping line 1361: expected 7 fields, saw 9\\nSkipping line 1362: expected 7 fields, saw 9\\nSkipping line 1363: expected 7 fields, saw 9\\nSkipping line 1364: expected 7 fields, saw 9\\nSkipping line 1365: expected 7 fields, saw 9\\nSkipping line 1366: expected 7 fields, saw 9\\nSkipping line 1367: expected 7 fields, saw 9\\nSkipping line 1368: expected 7 fields, saw 9\\nSkipping line 1369: expected 7 fields, saw 9\\nSkipping line 1370: expected 7 fields, saw 9\\nSkipping line 1371: expected 7 fields, saw 9\\nSkipping line 1372: expected 7 fields, saw 9\\nSkipping line 1373: expected 7 fields, saw 9\\nSkipping line 1374: expected 7 fields, saw 9\\nSkipping line 1375: expected 7 fields, saw 9\\nSkipping line 1376: expected 7 fields, saw 9\\nSkipping line 1377: expected 7 fields, saw 9\\nSkipping line 1378: expected 7 fields, saw 9\\nSkipping line 1379: expected 7 fields, saw 9\\n'\n"
          ]
        }
      ],
      "source": [
        "df_test = pd.read_csv(test_path, sep='\\t', header=None, error_bad_lines=False, quoting=csv.QUOTE_NONE)\n",
        "df_test.columns = columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Vy0C_4qm_tsR"
      },
      "outputs": [],
      "source": [
        "embed_1 = model.encode(df_test['sentence_1'], convert_to_numpy=True, batch_size=train_batch_size)\n",
        "embed_2 = model.encode(df_test['sentence_2'], convert_to_numpy=True, batch_size=train_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv3qFiDeB3uS"
      },
      "source": [
        "Compute the cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Y9oux_4b_tk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39149f5f-8564-4eb9-fd99-bc98360315c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity =  [0.27117753 0.8809062  0.58966494 ... 0.74274147 0.85241306 0.9187385 ]\n"
          ]
        }
      ],
      "source": [
        "cos_sim = 1 - sklearn.metrics.pairwise.paired_cosine_distances(embed_1, embed_2)\n",
        "print('Cosine similarity = ', cos_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4d__O86B9DB"
      },
      "source": [
        "Spearmean correlation coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oVziirO7-vHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784d98c8-0cc5-4458-fcd9-62145382a99f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearmean correlation coefficient =  0.534628805412517\n"
          ]
        }
      ],
      "source": [
        "spr_corr = scipy.stats.spearmanr(cos_sim, df_test['score'])\n",
        "print('Spearmean correlation coefficient = ', spr_corr[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW3C-qqGIcpc"
      },
      "source": [
        "**Comment:** the two results match each other"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *S-BERT classification objective*"
      ],
      "metadata": {
        "id": "b3mhfJOeeq0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('***** Downloading dataset ...')\n",
        "wget.download('https://nlp.stanford.edu/projects/snli/snli_1.0.zip', './snli_1.0.zip')\n",
        "!unzip snli_1.0.zip\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "train_class_path = 'snli_1.0/snli_1.0_train.jsonl'\n",
        "train_class = spark.read.json(train_class_path)\n",
        "\n",
        "test_class_path = 'snli_1.0/snli_1.0_test.jsonl'\n",
        "test_class = spark.read.json(test_class_path)\n",
        "\n",
        "dev_class_path = 'snli_1.0/snli_1.0_dev.jsonl'\n",
        "dev_class = spark.read.json(dev_class_path)"
      ],
      "metadata": {
        "id": "-l227HwyfPVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "indexer = StringIndexer(inputCol=\"gold_label\", outputCol=\"label\")"
      ],
      "metadata": {
        "id": "Vq4ip8WcgKsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CreatInputExampleList(df):\n",
        "    samples = []\n",
        "    for index, row in df.iterrows():\n",
        "        input_example = InputExample(texts=[row['sentence1'], row['sentence2']], label=row['label'])\n",
        "        samples.append(input_example)\n",
        "    return samples\n",
        "    \n",
        "def CreatClassSamples(df):\n",
        "    df = df.filter(col(\"gold_label\") != \"-\")\n",
        "    df = indexer.fit(df).transform(df)\n",
        "    df = df.withColumn(\"label\", col(\"label\").cast('int'))\n",
        "\n",
        "    df_class = df.select(\"sentence1\", \"sentence2\", \"label\").toPandas()\n",
        "\n",
        "    samples = CreatInputExampleList(df_class)\n",
        "    return samples"
      ],
      "metadata": {
        "id": "W7hsq3Asg8XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_samples = CreatClassSamples(train_class)\n",
        "test_class_samples = CreatClassSamples(test_class)\n",
        "dev_class_samples = CreatClassSamples(dev_class)"
      ],
      "metadata": {
        "id": "8Lowe76XjAu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_cl = DataLoader(train_class_samples, shuffle=True, batch_size=train_batch_size)\n",
        "num_lables = test_class.select('annotator_labels').distinct().count()\n",
        "train_loss_cl = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=num_lables)"
      ],
      "metadata": {
        "id": "INbLW2lVjCpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_cl = EmbeddingSimilarityEvaluator.from_input_examples(dev_class_samples, batch_size=train_batch_size, name='snli-dev')\n",
        "warmup_steps_cl = math.ceil(len(train_dataloader) * num_epochs * 0.1)"
      ],
      "metadata": {
        "id": "KsoBl_UtsuPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class_location = './training_snli'\n",
        "\n",
        "if os.path.exists(model_class_location):\n",
        "    model = SentenceTransformer(model_class_location)\n",
        "else:\n",
        "    model.fit(train_objectives=[(train_dataloader_cl, train_loss_cl)],\n",
        "             evaluator=evaluator_cl,\n",
        "             epochs=num_epochs,\n",
        "             evaluation_steps=1000,\n",
        "             warmup_steps=warmup_steps_cl,\n",
        "             output_path=model_class_location)"
      ],
      "metadata": {
        "id": "9SjRjDzwtCyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_class_location = \"./classification\"\n",
        "\n",
        "if not os.path.exists(evaluation_class_location):\n",
        "    os.makedirs(evaluation_class_location)\n",
        "\n",
        "test_eval_cl = EmbeddingSimilarityEvaluator.from_input_examples(test_class_samples, batch_size=train_batch_size, name='snli-test')\n",
        "c_s_cl = test_eval_cl(model, output_path=evaluation_class_location)\n",
        "print('Cosine similarity with the sentence_transformers library = ', c_s_cl)"
      ],
      "metadata": {
        "id": "w89Q02Wexhb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_s_sts = test_eval(model, output_path=evaluation_class_location)\n",
        "print('Cosine similarity with the sentence_transformers library = ', c_s_sts)"
      ],
      "metadata": {
        "id": "d4VLPRjGx4dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_1_snli = model.encode(df_test['sentence_1'], convert_to_numpy=True, batch_size=train_batch_size)\n",
        "embed_2_snli = model.encode(df_test['sentence_2'], convert_to_numpy=True, batch_size=train_batch_size)\n",
        "\n",
        "embed_1 = model.encode(df_test['sentence_1'], convert_to_numpy=True, batch_size=train_batch_size)\n",
        "embed_2 = model.encode(df_test['sentence_2'], convert_to_numpy=True, batch_size=train_batch_size)"
      ],
      "metadata": {
        "id": "btaNGbXXyARY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim_cl = 1 - sklearn.metrics.pairwise.paired_cosine_distances(embed_1_snli, embed_2_snli)\n",
        "print('SNLI-test: cosine similarity = ', cos_sim_cl)\n",
        "\n",
        "cos_sim_sts = 1 - sklearn.metrics.pairwise.paired_cosine_distances(embed_1, embed_2)\n",
        "print('STS benchmark: cosine similarity = ', cos_sim_sts)"
      ],
      "metadata": {
        "id": "YS_S9IQYyGWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spr_corr_cl = scipy.stats.spearmanr(cos_sim_cl, df_test['score'])\n",
        "print('SNLI-test: Spearmean correlation coefficient = ', spr_corr_cl[0])\n",
        "\n",
        "spr_corr_sts = scipy.stats.spearmanr(cos_sim_sts, df_test['score'])\n",
        "print('STS benchmark: Spearmean correlation coefficient = ', spr_corr_sts[0])"
      ],
      "metadata": {
        "id": "VNxyvZOFyKZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Semantic Search**"
      ],
      "metadata": {
        "id": "RfzMn0ETuf9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "bS9ymP7yubBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data, remove the first row ie the header and dates\n",
        "df = pd.read_csv(news_path, sep =\",\", error_bad_lines=False, names = [\"Date\", \"Sentence\"], nrows=10000)\n",
        "df = df.iloc[1: , :]\n",
        "df.drop(['Date'], axis=1, inplace = True)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "VEKGC1zquxDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the cosine similarity between the search sentences and the sentences from data, return the indexes of the k most similar sentences\n",
        "def search(search_sentences_embedding, sentence_embeddings):\n",
        "    similarity_matrix = cosine_similarity(search_sentences_embedding, sentence_embeddings[:])\n",
        "    similarities = similarity_matrix[0]\n",
        "    return similarities.argsort()[-k:][::1]"
      ],
      "metadata": {
        "id": "PKA1BeYIvdwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df.iloc[:, 0] # Get the sentences from the data\n",
        "\n",
        "sentence_embeddings= model.encode(sentences.values) # do embedding for the sentecnes from the data"
      ],
      "metadata": {
        "id": "tDEzKU4RvjsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_sentence_embedding = model.encode([\"kallis out of bangladesh\"]) #Embedd the search sentence\n",
        "\n",
        "k = 10 # number of similar sentences to include in the result\n",
        "similar_sentences = search(search_sentence_embedding, sentence_embeddings) #get indexes for the most similar sentences"
      ],
      "metadata": {
        "id": "RrmfDk5dvxsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the K most similar sentences\n",
        "for idx in similar_sentences:\n",
        "    print(sentences[idx])"
      ],
      "metadata": {
        "id": "AwmdaqRGv7L8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of lab2_id2223.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}