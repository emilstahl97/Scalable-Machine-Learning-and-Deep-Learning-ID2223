{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Spark ML\n",
    "In this lab assignment, you will complete a project by going through the following steps:\n",
    "1. Get the data.\n",
    "2. Discover the data to gain insights.\n",
    "3. Prepare the data for Machine Learning algorithms.\n",
    "4. Select a model and train it.\n",
    "5. Fine-tune your model.\n",
    "6. Present your solution.\n",
    "\n",
    "As a dataset, we use the California Housing Prices dataset from the StatLib repository. This dataset was based on data from the 1990 California census. The dataset has the following columns\n",
    "1. `longitude`: a measure of how far west a house is (a higher value is farther west)\n",
    "2. `latitude`: a measure of how far north a house is (a higher value is farther north)\n",
    "3. `housing_,median_age`: median age of a house within a block (a lower number is a newer building)\n",
    "4. `total_rooms`: total number of rooms within a block\n",
    "5. `total_bedrooms`: total number of bedrooms within a block\n",
    "6. `population`: total number of people residing within a block\n",
    "7. `households`: total number of households, a group of people residing within a home unit, for a block\n",
    "8. `median_income`: median income for households within a block of houses\n",
    "9. `median_house_value`: median house value for households within a block\n",
    "10. `ocean_proximity`: location of the house w.r.t ocean/sea\n",
    "\n",
    "---\n",
    "# 1. Get the data\n",
    "Let's start the lab by loading the dataset. The can find the dataset at `data/housing.csv`. To infer column types automatically, when you are reading the file, you need to set `inferSchema` to true. Moreover enable the `header` option to read the columns' name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.103:4040\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1637433216443)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "housing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val housing = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\",\",\"header\"->\"true\"))\n",
    "  .csv(\"data/housing.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Discover the data to gain insights\n",
    "Now it is time to take a look at the data. In this step we are going to take a look at the data a few different ways:\n",
    "* See the schema and dimension of the dataset\n",
    "* Look at the data itself\n",
    "* Statistical summary of the attributes\n",
    "* Breakdown of the data by the categorical attribute variable\n",
    "* Find the correlation among different attributes\n",
    "* Make new attributes by combining existing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Schema and dimension\n",
    "Print the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 20640\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Look at the data\n",
    "Print the first five records of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records with population more than 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -121.92|   37.53|               7.0|    28258.0|        3864.0|   12203.0|    3701.0|       8.4045|          451100.0|      <1H OCEAN|\n",
      "|  -117.78|   34.03|               8.0|    32054.0|        5290.0|   15507.0|    5050.0|       6.0191|          253900.0|      <1H OCEAN|\n",
      "|  -117.87|   34.04|               7.0|    27700.0|        4179.0|   15037.0|    4072.0|       6.6288|          339700.0|      <1H OCEAN|\n",
      "|  -117.88|   33.96|              16.0|    19059.0|        3079.0|   10988.0|    3061.0|       5.5469|          265200.0|      <1H OCEAN|\n",
      "|  -118.78|   34.16|               9.0|    30405.0|        4093.0|   12873.0|    3931.0|       8.0137|          399200.0|     NEAR OCEAN|\n",
      "|  -118.09|   34.68|               4.0|    23386.0|        4171.0|   10493.0|    3671.0|       4.0211|          144000.0|         INLAND|\n",
      "|   -118.1|   34.57|               7.0|    20377.0|        4335.0|   11973.0|    3933.0|       3.3086|          138100.0|         INLAND|\n",
      "|  -118.46|    34.4|              12.0|    25957.0|        4798.0|   10475.0|    4490.0|        4.542|          195300.0|      <1H OCEAN|\n",
      "|  -121.61|   36.69|              19.0|     9899.0|        2617.0|   11272.0|    2528.0|       2.0244|          118500.0|      <1H OCEAN|\n",
      "|  -121.68|   36.72|              12.0|    19234.0|        4492.0|   12153.0|    4372.0|       3.2652|          152800.0|      <1H OCEAN|\n",
      "|  -121.79|   36.64|              11.0|    32627.0|        6445.0|   28566.0|    6082.0|       2.3087|          118800.0|      <1H OCEAN|\n",
      "|  -117.74|   33.89|               4.0|    37937.0|        5471.0|   16122.0|    5189.0|       7.4947|          366300.0|      <1H OCEAN|\n",
      "|  -117.12|   33.52|               4.0|    30401.0|        4957.0|   13251.0|    4339.0|       4.5841|          212300.0|      <1H OCEAN|\n",
      "|  -121.53|   38.48|               5.0|    27870.0|        5027.0|   11935.0|    4855.0|       4.8811|          212200.0|         INLAND|\n",
      "|   -121.4|   38.47|               4.0|    20982.0|        3392.0|   10329.0|    3086.0|       4.3658|          130600.0|         INLAND|\n",
      "|  -121.44|   38.43|               3.0|    39320.0|        6210.0|   16305.0|    5358.0|       4.9516|          153700.0|         INLAND|\n",
      "|  -117.75|   34.01|               4.0|    22128.0|        3522.0|   10450.0|    3258.0|       6.1287|          289600.0|      <1H OCEAN|\n",
      "|  -117.61|    34.1|               9.0|    18956.0|        4095.0|   10323.0|    3832.0|       3.6033|          132600.0|         INLAND|\n",
      "|  -116.14|   34.45|              12.0|     8796.0|        1721.0|   11139.0|    1680.0|       2.2612|          137500.0|         INLAND|\n",
      "|  -117.42|   33.35|              14.0|    25135.0|        4819.0|   35682.0|    4769.0|       2.5729|          134400.0|      <1H OCEAN|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.where(\"population > 10000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Statistical summary\n",
    "Print a summary of the table statistics for the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. You can use the `describe` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.describe(\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the maximum age (`housing_median_age`), the minimum number of rooms (`total_rooms`), and the average of house values (`median_house_value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------+-----------------------+\n",
      "|max(housing_median_age)|min(total_rooms)|avg(median_house_value)|\n",
      "+-----------------------+----------------+-----------------------+\n",
      "|                   52.0|             2.0|     206855.81690891474|\n",
      "+-----------------------+----------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "housing.select(max(\"housing_median_age\"), min(\"total_rooms\"), avg(\"median_house_value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Breakdown the data by categorical data\n",
    "Print the number of houses in different areas (`ocean_proximity`), and sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|         ISLAND|    5|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.groupBy(\"ocean_proximity\").count().sort(col(\"count\").desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average value of the houses (`median_house_value`) in different areas (`ocean_proximity`), and call the new column `avg_value` when print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.groupBy(\"ocean_proximity\").avg(\"median_house_value\").withColumnRenamed(\"avg(median_house_value)\", \"avg_value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the above question in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"SELECT ocean_proximity, AVG(median_house_value) AS avg_value FROM df GROUP BY ocean_proximity\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Correlation among attributes\n",
    "Print the correlation among the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. To do so, first you need to put these attributes into one vector. Then, compute the standard correlation coefficient (Pearson) between every pair of attributes in this new vector. To make a vector of these attributes, you can use the `VectorAssembler` Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|            features|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|[41.0,880.0,45260...|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|[21.0,7099.0,3585...|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|[52.0,1467.0,3521...|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|[52.0,1274.0,3413...|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|[52.0,1627.0,3422...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_18af9496b8f3, handleInvalid=error, numInputCols=4\n",
       "housingAttrs: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\")).setOutputCol(\"features\")\n",
    "\n",
    "val housingAttrs = va.transform(housing)\n",
    "\n",
    "housingAttrs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard correlation coefficient:\n",
      " 1.0                   -0.3612622012223152  0.10562341249321029    -0.29624423977353675   \n",
      "-0.3612622012223152   1.0                  0.13415311380656375    0.8571259728659744     \n",
      "0.10562341249321029   0.13415311380656375  1.0                    -0.024649678888894997  \n",
      "-0.29624423977353675  0.8571259728659744   -0.024649678888894997  1.0                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Matrix\n",
       "import org.apache.spark.ml.stat.Correlation\n",
       "import org.apache.spark.sql.Row\n",
       "coeff: org.apache.spark.ml.linalg.Matrix =\n",
       "1.0                   -0.3612622012223152  0.10562341249321029    -0.29624423977353675\n",
       "-0.3612622012223152   1.0                  0.13415311380656375    0.8571259728659744\n",
       "0.10562341249321029   0.13415311380656375  1.0                    -0.024649678888894997\n",
       "-0.29624423977353675  0.8571259728659744   -0.024649678888894997  1.0\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.linalg.Matrix\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "val Row(coeff: Matrix) = Correlation.corr(housingAttrs, \"features\").head\n",
    "println(s\"The standard correlation coefficient:\\n ${coeff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Combine and make new attributes\n",
    "Now, let's try out various attribute combinations. In the given dataset, the total number of rooms in a block is not very useful, if we don't know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful, and we want to compare it to the number of rooms. And the population per household seems like also an interesting attribute combination to look at. To do so, add the three new columns to the dataset as below. We will call the new dataset the `housingExtra`.\n",
    "```\n",
    "rooms_per_household = total_rooms / households\n",
    "bedrooms_per_room = total_bedrooms / total_rooms\n",
    "population_per_household = population / households\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------------+\n",
      "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
      "+-------------------+-------------------+------------------------+\n",
      "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
      "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
      "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
      "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
      "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
      "+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "housingCol1: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n",
       "housingCol2: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\n",
       "housingExtra: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val housingCol1 = housing.withColumn(\"rooms_per_household\", col(\"total_rooms\") / col(\"households\"))\n",
    "val housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\", col(\"total_bedrooms\") / col(\"total_rooms\"))\n",
    "val housingExtra = housingCol2.withColumn(\"population_per_household\", col(\"population\") / col(\"households\"))\n",
    "\n",
    "housingExtra.select(\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prepare the data for Machine Learning algorithms\n",
    "Before going through the Machine Learning steps, let's first rename the label column from `median_house_value` to `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "renamedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val renamedHousing = housingExtra.withColumnRenamed(\"median_house_value\", \"label\")\n",
    "\n",
    "renamedHousing.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to separate the numerical attributes from the categorical attribute (`ocean_proximity`) and keep their column names in two different lists. Moreover, sice we don't want to apply the same transformations to the predictors (features) and the label, we should remove the label attribute from the list of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colLabel: String = label\n",
       "colCat: String = ocean_proximity\n",
       "colNum: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// label columns\n",
    "val colLabel = \"label\"\n",
    "\n",
    "// categorical columns\n",
    "val colCat = \"ocean_proximity\"\n",
    "\n",
    "// numerical columns\n",
    "val colNum = renamedHousing.columns.filter(_ != colLabel).filter(_ != colCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Prepare continuse attributes\n",
    "### Data cleaning\n",
    "Most Machine Learning algorithms cannot work with missing features, so we should take care of them. As a first step, let's find the columns with missing values in the numerical attributes. To do so, we can print the number of missing values of each continues attributes, listed in `colNum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_bedrooms\n",
      "bedrooms_per_room\n"
     ]
    }
   ],
   "source": [
    "for (c <- colNum) {\n",
    "    if (c != \"ocean_proximity\") {\n",
    "        val housingWithMissingValues = renamedHousing.filter((col(c).isNull and (col(c) <=> lit(\"\"))))\n",
    "        if (housingWithMissingValues.count() > 0) {\n",
    "            println(s\"$c\")\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observerd above, the `total_bedrooms` and `bedrooms_per_room` attributes have some missing values. One way to take care of missing values is to use the `Imputer` Transformer, which completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are located. To use it, you need to create an `Imputer` instance, specifying that you want to replace each attribute's missing values with the \"median\" of that attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|total_bedrooms|  bedrooms_per_room|\n",
      "+--------------+-------------------+\n",
      "|         129.0|0.14659090909090908|\n",
      "|        1106.0|0.15579659106916466|\n",
      "|         190.0|0.12951601908657123|\n",
      "|         235.0|0.18445839874411302|\n",
      "|         280.0| 0.1720958819913952|\n",
      "+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Imputer\n",
       "imputer: org.apache.spark.ml.feature.Imputer = imputer_3b1ce317d551\n",
       "imputedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val imputer = new Imputer().setStrategy(\"median\").setInputCols(Array(\"total_bedrooms\", \"bedrooms_per_room\")).setOutputCols(Array(\"total_bedrooms\", \"bedrooms_per_room\"))\n",
    "                       \n",
    "val imputedHousing = imputer.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "imputedHousing.select(\"total_bedrooms\", \"bedrooms_per_room\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the label attribues is generally not required.\n",
    "\n",
    "One way to get all attributes to have the same scale is to use standardization. In standardization, for each value, first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. To do this, we can use the `StandardScaler` Estimator. To use `StandardScaler`, again we need to convert all the numerical attributes into a big vectore of features using `VectorAssembler`, and then call `StandardScaler` on that vactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|     vector_features|     scaler_features|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_9f3de830eaf4, handleInvalid=error, numInputCols=11\n",
       "featuredHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 12 more fields]\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_3660716dc1c7\n",
       "scaledHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 13 more fields]\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(colNum).setOutputCol(\"vector_features\")\n",
    "val featuredHousing = va.transform(imputedHousing)\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"vector_features\").setOutputCol(\"scaler_features\")\n",
    "val scaledHousing = scaler.fit(featuredHousing).transform(featuredHousing)\n",
    "\n",
    "scaledHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Prepare categorical attributes\n",
    "After imputing and scaling the continuse attributes, we should take care of the categorical attributes. Let's first print the number of distict values of the categirical attribute `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|ocean_proximity|\n",
      "+---------------+\n",
      "|         ISLAND|\n",
      "|     NEAR OCEAN|\n",
      "|       NEAR BAY|\n",
      "|      <1H OCEAN|\n",
      "|         INLAND|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamedHousing.select(\"ocean_proximity\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String indexer\n",
    "Most Machine Learning algorithms prefer to work with numbers. So let's convert the categorical attribute `ocean_proximity` to numbers. To do so, we can use the `StringIndexer` that encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_numbers|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_311662372b91\n",
       "idxHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 12 more fields]\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(colCat).setOutputCol(\"ocean_proximity_numbers\")\n",
    "val idxHousing = indexer.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "idxHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this numerical data in any Machine Learning algorithm. You can look at the mapping that this encoder has learned using the `labels` method: \"<1H OCEAN\" is mapped to 0, \"INLAND\" is mapped to 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res113: Array[String] = Array(<1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND)\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.fit(renamedHousing).labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "Now, convert the label indices built in the last step into one-hot vectors. To do this, you can take advantage of the `OneHotEncoderEstimator` Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_numbers|ocean_proximity_vectors|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|          (4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_7e0c80bd377d\n",
       "ohHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 13 more fields]\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val encoder = new OneHotEncoder().setInputCol(\"ocean_proximity_numbers\").setOutputCol(\"ocean_proximity_vectors\")\n",
    "\n",
    "val ohHousing = encoder.fit(idxHousing).transform(idxHousing)\n",
    "\n",
    "ohHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Pipeline\n",
    "As you can see, there are many data transformation steps that need to be executed in the right order. For example, you called the `Imputer`, `VectorAssembler`, and `StandardScaler` from left to right. However, we can use the `Pipeline` class to define a sequence of Transformers/Estimators, and run them in order. A `Pipeline` is an `Estimator`, thus, after a Pipeline's `fit()` method runs, it produces a `PipelineModel`, which is a `Transformer`.\n",
    "\n",
    "Now, let's create a pipeline called `numPipeline` to call the numerical transformers you built above (`imputer`, `va`, and `scaler`) in the right order from left to right, as well as a pipeline called `catPipeline` to call the categorical transformers (`indexer` and `encoder`). Then, put these two pipelines `numPipeline` and `catPipeline` into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|     vector_features|     scaler_features|ocean_proximity_numbers|ocean_proximity_vectors|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|                    3.0|          (4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
       "import scala.collection.mutable\n",
       "numStage: scala.collection.mutable.ArrayBuffer[org.apache.spark.ml.PipelineStage] = ArrayBuffer(imputer_3b1ce317d551, VectorAssembler: uid=vecAssembler_9f3de830eaf4, handleInvalid=error, numInputCols=11, stdScal_3660716dc1c7)\n",
       "catStage: scala.collection.mutable.ArrayBuffer[org.apache.spark.ml.PipelineStage] = ArrayBuffer(strIdx_311662372b91, oneHotEncoder_7e0c80bd377d)\n",
       "numPipeline: org.apache.spark.ml.Pipeline = pipeline_b6aea2632597\n",
       "catPipeline: org.apache.spark.ml.Pipeline = pipeline_c356f6095feb\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_77999c9bc5a2\n",
       "newHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 15 more fields]\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
    "import scala.collection.mutable\n",
    "\n",
    "var numStage = new mutable.ArrayBuffer[PipelineStage]()\n",
    "numStage += imputer\n",
    "numStage += va\n",
    "numStage += scaler\n",
    "\n",
    "var catStage = new mutable.ArrayBuffer[PipelineStage]()\n",
    "catStage += indexer\n",
    "catStage += encoder\n",
    "\n",
    "val numPipeline = new Pipeline().setStages(numStage.toArray)\n",
    "val catPipeline = new Pipeline().setStages(catStage.toArray)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(numPipeline, catPipeline))\n",
    "val newHousing = pipeline.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "newHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `VectorAssembler` to put all attributes of the final dataset `newHousing` into a big vector, and call the new column `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|features                                                                                                                                                                                                                            |label   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|[-61.00726959606955,17.734477624640412,3.2577023016083064,0.40337085073160667,0.30758821710917267,0.2843362208866199,0.3295584480852433,4.382095394195227,2.8228125480951665,2.5405867237343416,0.24605655309533123,0.0,0.0,0.0,1.0]|452600.0|\n",
      "|[-61.002278409814444,17.725114120086744,1.668579227653035,3.2540109878905406,2.637151690873992,2.1201592122632746,2.9764882057222772,4.369567902917918,2.5213017566153497,2.700131633864973,0.2031418986718504,0.0,0.0,0.0,1.0]     |358500.0|\n",
      "|[-61.012260782324645,17.720432367809913,4.131719992283705,0.6724375432082579,0.45303690892048687,0.4379837439744208,0.4629511532626037,3.820042655291457,3.349860792340824,2.244659512945694,0.2698099860028393,0.0,0.0,0.0,1.0]    |352100.0|\n",
      "|[-61.01725196857974,17.720432367809913,4.131719992283705,0.5839709816273487,0.5603351241911285,0.4927317119712234,0.5728039692910182,2.970331345671345,2.3512306012371758,3.1968732702241724,0.2453238057662802,0.0,0.0,0.0,1.0]    |341300.0|\n",
      "|[-61.01725196857974,17.720432367809913,4.131719992283705,0.7457776978867319,0.6676333394617702,0.4989129341644108,0.6774256988418891,2.024505754234575,2.5389707703782265,2.9826168328456415,0.21003820253311387,0.0,0.0,0.0,1.0]   |342200.0|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va2: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_a1c4b9e2e9f9, handleInvalid=error, numInputCols=2\n",
       "dataset: org.apache.spark.sql.DataFrame = [features: vector, label: double]\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val va2 = new VectorAssembler().setInputCols(Array(\"scaler_features\", \"ocean_proximity_vectors\"))\n",
    ".setOutputCol(\"features\")\n",
    "val dataset = va2.transform(newHousing).select(\"features\", \"label\")\n",
    "\n",
    "dataset.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Make a model\n",
    "Here we going to make four different regression models:\n",
    "* Linear regression model\n",
    "* Decission tree regression\n",
    "* Random forest regression\n",
    "* Gradient-booster forest regression\n",
    "\n",
    "But, before giving the data to train a Machine Learning model, let's first split the data into training dataset (`trainSet`) with 80% of the whole data, and test dataset (`testSet`) with 20% of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[-62.065401082150...| 94600.0|\n",
      "|[-62.040445150874...| 85800.0|\n",
      "|[-62.040445150874...|103600.0|\n",
      "|[-62.025471592109...| 79000.0|\n",
      "|[-62.020480405854...|111400.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test set:\n",
      "+--------------------+-------+\n",
      "|            features|  label|\n",
      "+--------------------+-------+\n",
      "|[-62.005506847089...|50800.0|\n",
      "|[-62.000515660834...|78300.0|\n",
      "|[-61.985542102068...|90100.0|\n",
      "|[-61.975559729558...|82800.0|\n",
      "|[-61.975559729558...|75500.0|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\n",
       "testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainSet, testSet) = dataset.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "print(\"Training set:\\n\")\n",
    "trainSet.show(5)\n",
    "print(\"Test set:\\n\")\n",
    "testSet.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Linear regression model\n",
    "Now, train a Linear Regression model using the `LinearRegression` class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-15112.272375722867,-12294.872144526882,15477.233450777627,8882.058802437119,12594.856054597652,-29500.057075588003,13462.317363767472,83431.98553164786,2193.4113824324363,18475.601698807754,-1206.8558379758672,17501.316799348147,-38656.50933481211,29848.62575240823,20279.410570120857] Intercept: -776532.9454630188\n",
      "RMSE: 69311.88435607517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_8bc9d6c61136\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_8bc9d6c61136, numFeatures=15\n",
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@3a348789\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "// train the model\n",
    "val lr = new LinearRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(trainSet)\n",
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `RegressionEvaluator` to measure the root-mean-square-erroe (RMSE) of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "| 234584.6836787687|50800.0|[-62.005506847089...|\n",
      "|154029.62481895753|78300.0|[-62.000515660834...|\n",
      "|214358.64912636147|90100.0|[-61.985542102068...|\n",
      "|185007.48090691422|82800.0|[-61.975559729558...|\n",
      "|184581.41161690396|75500.0|[-61.975559729558...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 67921.84355516791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_5bd919438a8a, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 67921.84355516791\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = lrModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Decision tree regression\n",
    "Repeat what you have done on Regression Model to build a Decision Tree model. Use the `DecisionTreeRegressor` to make a model and then measure its RMSE on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "| 185486.2138836773|50800.0|[-62.005506847089...|\n",
      "|131441.91176470587|78300.0|[-62.000515660834...|\n",
      "| 223152.2995951417|90100.0|[-61.985542102068...|\n",
      "|167739.93754152823|82800.0|[-61.975559729558...|\n",
      "|131441.91176470587|75500.0|[-61.975559729558...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 67394.69860976153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "dt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_4eab0a47b71a\n",
       "dtModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_4eab0a47b71a, depth=5, numNodes=63, numFeatures=15\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_0bf58d95a920, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 67394.69860976153\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val dt = new DecisionTreeRegressor()\n",
    "\n",
    "// train the model\n",
    "val dtModel = dt.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = dtModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Random forest regression\n",
    "Let's try the test error on a Random Forest Model. Youcan use the `RandomForestRegressor` to make a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|197122.26999582426|50800.0|[-62.005506847089...|\n",
      "|167828.71980701765|78300.0|[-62.000515660834...|\n",
      "| 195185.6240832696|90100.0|[-61.985542102068...|\n",
      "|174966.65163700673|82800.0|[-61.975559729558...|\n",
      "|171108.58542867747|75500.0|[-61.975559729558...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 64461.80374028058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.RandomForestRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_a78e4c7ab451\n",
       "rfModel: org.apache.spark.ml.regression.RandomForestRegressionModel = RandomForestRegressionModel: uid=rfr_a78e4c7ab451, numTrees=20, numFeatures=15\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_cab9fcecd39a, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 64461.80374028058\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "\n",
    "// train the model\n",
    "val rfModel = rf.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = rfModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Gradient-boosted tree regression\n",
    "Fianlly, we want to build a Gradient-boosted Tree Regression model and test the RMSE of the test data. Use the `GBTRegressor` to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|141460.23546996404|50800.0|[-62.005506847089...|\n",
      "|  80395.8953092036|78300.0|[-62.000515660834...|\n",
      "| 156543.1587623532|90100.0|[-61.985542102068...|\n",
      "|104874.28899995565|82800.0|[-61.975559729558...|\n",
      "| 75271.62971947192|75500.0|[-61.975559729558...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 54296.22452327231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.GBTRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "gb: org.apache.spark.ml.regression.GBTRegressor = gbtr_fb433fe4cb9f\n",
       "gbModel: org.apache.spark.ml.regression.GBTRegressionModel = GBTRegressionModel: uid=gbtr_fb433fe4cb9f, numTrees=20, numFeatures=15\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_52993bfa7b96, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 54296.22452327231\n"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val gb = new GBTRegressor()\n",
    "\n",
    "// train the model\n",
    "val gbModel = gb.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = gbModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Hyperparameter tuning\n",
    "An important task in Machie Learning is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LinearRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately. MLlib supports model selection tools, such as `CrossValidator`. These tools require the following items:\n",
    "* Estimator: algorithm or Pipeline to tune (`setEstimator`)\n",
    "* Set of ParamMaps: parameters to choose from, sometimes called a \"parameter grid\" to search over (`setEstimatorParamMaps`)\n",
    "* Evaluator: metric to measure how well a fitted Model does on held-out test data (`setEvaluator`)\n",
    "\n",
    "`CrossValidator` begins by splitting the dataset into a set of folds, which are used as separate training and test datasets. For example with `k=3` folds, `CrossValidator` will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular `ParamMap`, `CrossValidator` computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs. After identifying the best `ParamMap`, `CrossValidator` finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
    "\n",
    "Below, use the `CrossValidator` to select the best Random Forest model. To do so, you need to define a grid of parameters. Let's say we want to do the search among the different number of trees (1, 5, and 10), and different tree depth (5, 10, and 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|          211692.1|50800.0|[-62.005506847089...|\n",
      "|127840.17647058824|78300.0|[-62.000515660834...|\n",
      "|131438.18181818182|90100.0|[-61.985542102068...|\n",
      "| 108926.3888888889|82800.0|[-61.975559729558...|\n",
      "| 80096.66666666666|75500.0|[-61.975559729558...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 51595.491067216375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.tuning.CrossValidator\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfr_a78e4c7ab451-maxDepth: 5,\n",
       "\trfr_a78e4c7ab451-numTrees: 1\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 5,\n",
       "\trfr_a78e4c7ab451-numTrees: 5\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 5,\n",
       "\trfr_a78e4c7ab451-numTrees: 10\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 10,\n",
       "\trfr_a78e4c7ab451-numTrees: 1\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 10,\n",
       "\trfr_a78e4c7ab451-numTrees: 5\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 10,\n",
       "\trfr_a78e4c7ab451-numTrees: 10\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 15,\n",
       "\trfr_a78e4c7ab451-numTrees: 1\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 15,\n",
       "\trfr_a78e4c7ab451-numTrees: 5\n",
       "}, {\n",
       "\trfr_a78e4c7ab451-maxDepth: 15,\n",
       "\trfr_a78e...\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rf.numTrees, Array(1, 5, 10)).addGrid(rf.maxDepth, Array(5 ,10, 15)).build()\n",
    "\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    "\n",
    "val cv = new CrossValidator()\n",
    "    .setEstimator(rf)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setNumFolds(3)\n",
    "\n",
    "val cvModel = cv.fit(trainSet)\n",
    "\n",
    "val predictions = cvModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. An End-to-End Classification Test\n",
    "As the last step, you are given a dataset called `data/ccdefault.csv`. The dataset represents default of credit card clients.It has 30,000 cases and 24 different attributes. More details about the dataset is available at `data/ccdefault.txt`. In this task you should make three models, compare their results and conclude the ideal solution. Here are the suggested steps:\n",
    "1. Load the data.\n",
    "2. Carry out some exploratory analyses (e.g., how various features and the target variable are distributed).\n",
    "3. Train a model to predict thetarget variable (risk of `default`).\n",
    "  - Employ three different models (logistic regression, decision tree, and random forest).\n",
    "  - Compare the models' performances (e.g., AUC).\n",
    "  - Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?).\n",
    "4. What more would you do with this data? Anything to help you devise a better solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|DEFAULT|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|      1|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|      1|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|      0|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|      0|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|      0|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "credit: org.apache.spark.sql.DataFrame = [ID: int, LIMIT_BAL: int ... 23 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val credit = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\",\",\"header\"->\"true\"))\n",
    "  .csv(\"data/ccdefault.csv\")\n",
    "credit.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2. Carry out some exploratory analyses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ID: int, LIMIT_BAL: int ... 23 more fields].count()\n",
      "[ID: int, LIMIT_BAL: int ... 23 more fields].columns()\n"
     ]
    }
   ],
   "source": [
    "println(s\"$credit.count()\")\n",
    "println(s\"$credit.columns()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LIMIT_BAL: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- EDUCATION: integer (nullable = true)\n",
      " |-- MARRIAGE: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- PAY_0: integer (nullable = true)\n",
      " |-- PAY_2: integer (nullable = true)\n",
      " |-- PAY_3: integer (nullable = true)\n",
      " |-- PAY_4: integer (nullable = true)\n",
      " |-- PAY_5: integer (nullable = true)\n",
      " |-- PAY_6: integer (nullable = true)\n",
      " |-- BILL_AMT1: integer (nullable = true)\n",
      " |-- BILL_AMT2: integer (nullable = true)\n",
      " |-- BILL_AMT3: integer (nullable = true)\n",
      " |-- BILL_AMT4: integer (nullable = true)\n",
      " |-- BILL_AMT5: integer (nullable = true)\n",
      " |-- BILL_AMT6: integer (nullable = true)\n",
      " |-- PAY_AMT1: integer (nullable = true)\n",
      " |-- PAY_AMT2: integer (nullable = true)\n",
      " |-- PAY_AMT3: integer (nullable = true)\n",
      " |-- PAY_AMT4: integer (nullable = true)\n",
      " |-- PAY_AMT5: integer (nullable = true)\n",
      " |-- PAY_AMT6: integer (nullable = true)\n",
      " |-- DEFAULT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|            DEFAULT|\n",
      "+-------+-------------------+\n",
      "|  count|              30000|\n",
      "|   mean|             0.2212|\n",
      "| stddev|0.41506180569093254|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit.select(col(\"DEFAULT\")).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|DEFAULT|count|\n",
      "+-------+-----+\n",
      "|      1| 6636|\n",
      "|      0|23364|\n",
      "+-------+-----+\n",
      "\n",
      "Total number of records in the dataset = [ID: int, LIMIT_BAL: int ... 23 more fields].count()"
     ]
    }
   ],
   "source": [
    "credit.groupBy(col(\"DEFAULT\")).count().show()\n",
    "print(s\"Total number of records in the dataset = $credit.count()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+\n",
      "|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+\n",
      "|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|\n",
      "|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|\n",
      "|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|\n",
      "|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|\n",
      "|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "credit_dataset: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val credit_dataset = credit.withColumnRenamed(\"DEFAULT\", \"label\").drop(\"ID\")\n",
    "\n",
    "credit_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ljava.lang.String;@6ccf4595"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNum: Array[String] = Array(LIMIT_BAL, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNum = credit_dataset.columns.filter(_ != \"label\").filter(_ != \"SEX\").filter(_!= \"EDUCATION\").filter(_!= \"MARRIAGE\")\n",
    "print(colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column LIMIT_BAL has 0 null values\n",
      "Column AGE has 0 null values\n",
      "Column PAY_0 has 0 null values\n",
      "Column PAY_2 has 0 null values\n",
      "Column PAY_3 has 0 null values\n",
      "Column PAY_4 has 0 null values\n",
      "Column PAY_5 has 0 null values\n",
      "Column PAY_6 has 0 null values\n",
      "Column BILL_AMT1 has 0 null values\n",
      "Column BILL_AMT2 has 0 null values\n",
      "Column BILL_AMT3 has 0 null values\n",
      "Column BILL_AMT4 has 0 null values\n",
      "Column BILL_AMT5 has 0 null values\n",
      "Column BILL_AMT6 has 0 null values\n",
      "Column PAY_AMT1 has 0 null values\n",
      "Column PAY_AMT2 has 0 null values\n",
      "Column PAY_AMT3 has 0 null values\n",
      "Column PAY_AMT4 has 0 null values\n",
      "Column PAY_AMT5 has 0 null values\n",
      "Column PAY_AMT6 has 0 null values\n"
     ]
    }
   ],
   "source": [
    "for (c <- colNum) {\n",
    "    val creditDatasetMissingValues = credit_dataset.filter((col(c).isNull and (col(c) <=> lit(\"\")))).count()\n",
    "    println(s\"Column $c has $creditDatasetMissingValues null values\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+\n",
      "|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|     vector_features|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+\n",
      "|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|(20,[0,1,2,3,4,5,...|\n",
      "|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|[120000.0,26.0,-1...|\n",
      "|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|[90000.0,34.0,0.0...|\n",
      "|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|[50000.0,37.0,0.0...|\n",
      "|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|[50000.0,57.0,-1....|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "va3: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_b4ce0aa5a9a9, handleInvalid=error, numInputCols=20\n",
       "featuredCredit: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 23 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va3 = new VectorAssembler()\n",
    "    .setInputCols(colNum)\n",
    "    .setOutputCol(\"vector_features\")\n",
    "\n",
    "val featuredCredit = va3.transform(credit_dataset)\n",
    "\n",
    "featuredCredit.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|     vector_features|     scaled_features|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|[120000.0,26.0,-1...|[0.92487215993365...|\n",
      "|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|[90000.0,34.0,0.0...|[0.69365411995024...|\n",
      "|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|[50000.0,37.0,0.0...|[0.38536339997235...|\n",
      "|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|[50000.0,57.0,-1....|[0.38536339997235...|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_9acc59ca0676\n",
       "scalerModel: org.apache.spark.ml.feature.StandardScalerModel = StandardScalerModel: uid=stdScal_9acc59ca0676, numFeatures=20, withMean=false, withStd=true\n",
       "scaledData: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 24 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"vector_features\")\n",
    "  .setOutputCol(\"scaled_features\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)\n",
    "\n",
    "// Compute summary statistics by fitting the StandardScaler.\n",
    "val scalerModel = scaler.fit(featuredCredit)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledData = scalerModel.transform(featuredCredit)\n",
    "scaledData.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+-------------+-----------------+----------------+\n",
      "|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|  SEX_vectors|EDUCATION_vectors|MARRIAGE_vectors|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+-------------+-----------------+----------------+\n",
      "|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|    (2,[],[])|    (6,[2],[1.0])|   (3,[1],[1.0])|\n",
      "|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|    (2,[],[])|    (6,[2],[1.0])|   (3,[2],[1.0])|\n",
      "|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|    (2,[],[])|    (6,[2],[1.0])|   (3,[2],[1.0])|\n",
      "|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|    (2,[],[])|    (6,[2],[1.0])|   (3,[1],[1.0])|\n",
      "|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|(2,[1],[1.0])|    (6,[2],[1.0])|   (3,[1],[1.0])|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+-------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_4566f4353028\n",
       "model: org.apache.spark.ml.feature.OneHotEncoderModel = OneHotEncoderModel: uid=oneHotEncoder_4566f4353028, dropLast=true, handleInvalid=error, numInputCols=3, numOutputCols=3\n",
       "encoded: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 25 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val encoder = new OneHotEncoder()\n",
    "  .setInputCols(Array(\"SEX\", \"EDUCATION\", \"MARRIAGE\"))\n",
    "  .setOutputCols(Array(\"SEX_vectors\",\"EDUCATION_vectors\",\"MARRIAGE_vectors\"))\n",
    "val model = encoder.fit(credit_dataset)\n",
    "\n",
    "val encoded = model.transform(credit_dataset)\n",
    "\n",
    "encoded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+-------------+-----------------+----------------+\n",
      "|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|     vector_features|     scaled_features|  SEX_vectors|EDUCATION_vectors|MARRIAGE_vectors|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+-------------+-----------------+----------------+\n",
      "|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|    (2,[],[])|    (6,[2],[1.0])|   (3,[1],[1.0])|\n",
      "|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|[120000.0,26.0,-1...|[0.92487215993365...|    (2,[],[])|    (6,[2],[1.0])|   (3,[2],[1.0])|\n",
      "|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|[90000.0,34.0,0.0...|[0.69365411995024...|    (2,[],[])|    (6,[2],[1.0])|   (3,[2],[1.0])|\n",
      "|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|[50000.0,37.0,0.0...|[0.38536339997235...|    (2,[],[])|    (6,[2],[1.0])|   (3,[1],[1.0])|\n",
      "|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|[50000.0,57.0,-1....|[0.38536339997235...|(2,[1],[1.0])|    (6,[2],[1.0])|   (3,[1],[1.0])|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+-------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
       "import scala.collection.mutable\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_f6f71fbb9076\n",
       "newCredit: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 27 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
    "import scala.collection.mutable\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(va3, scaler, encoder))\n",
    "val newCredit = pipeline.fit(credit_dataset).transform(credit_dataset)\n",
    "\n",
    "newCredit.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                     |label|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(31,[0,1,2,3,4,5,6,7,8,9,10,15,24,29],[0.15414535998894324,2.603628744963987,1.7796736791807803,1.6705842242124869,-0.8355143261989371,-0.8553305663148897,-1.7649331341008119,-1.7391491486204211,0.053139869207970765,0.0435834725779124,0.009935199510232218,0.029903384202815683,1.0,1.0])                                                                                               |1    |\n",
      "|(31,[0,1,2,3,7,8,9,10,11,12,13,15,16,17,19,24,30],[0.9248721599336596,2.820597807044319,-0.8898368395903902,1.6705842242124869,1.7391491486204211,0.03642247104926593,0.024236457187910666,0.03867373742589667,0.05086048089002836,0.05682831632873511,0.054756928361119755,0.043401138175349324,0.05679571695065346,0.06383185262622842,0.1125019744244751,1.0,1.0])                        |1    |\n",
      "|(31,[0,1,8,9,10,11,12,13,14,15,16,17,18,19,24,30],[0.6936541199502446,3.6884740553656483,0.39707555220338797,0.19708103476801328,0.19551722809758876,0.2227633104018938,0.24586676482834513,0.2610903033078967,0.09164851210352462,0.06510170726302399,0.05679571695065346,0.06383185262622842,0.06545228384617399,0.28125493606118773,1.0,1.0])                                             |0    |\n",
      "|(31,[0,1,8,9,10,11,12,13,14,15,16,17,18,19,24,29],[0.38536339997235813,4.013927648486146,0.6381401620451179,0.6776794432142002,0.7107633077777304,0.4401172542543591,0.47632162447578585,0.4961370629518569,0.12074902780438027,0.08762689797603028,0.06815486034078415,0.07021503788885128,0.06996849143156,0.05625098721223755,1.0,1.0])                                                   |0    |\n",
      "|(31,[0,1,2,4,8,9,10,11,12,13,14,15,16,17,18,19,21,24,29],[0.38536339997235813,6.183618269289469,-0.8898368395903902,-0.8355143261989371,0.11702178711093383,0.0796641810176542,0.5167313126983621,0.3254946423707805,0.3149160475918849,0.32123728809462804,0.12074902780438027,1.5919971494099885,0.5679571695065346,0.5744866736360559,0.04509662357001388,0.0381944203171093,1.0,1.0,1.0])|0    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va4: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_5edf3ff9127f, handleInvalid=error, numInputCols=4\n",
       "dataCredit: org.apache.spark.sql.DataFrame = [features: vector, label: int]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val va4 = new VectorAssembler().setInputCols(Array(\"scaled_features\",\"SEX_vectors\",\"EDUCATION_vectors\",\"MARRIAGE_vectors\"))\n",
    ".setOutputCol(\"features\")\n",
    "val dataCredit = va4.transform(newCredit).select(\"features\", \"label\")\n",
    "\n",
    "dataCredit.show(5, false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(31,[0,1,2,3,4,5,...|    0|\n",
      "|(31,[0,1,2,3,4,5,...|    1|\n",
      "|(31,[0,1,2,3,4,5,...|    0|\n",
      "|(31,[0,1,2,3,4,5,...|    0|\n",
      "|(31,[0,1,2,3,4,5,...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test set:\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(31,[0,1,2,3,4,5,...|    1|\n",
      "|(31,[0,1,2,3,4,5,...|    0|\n",
      "|(31,[0,1,2,3,4,5,...|    1|\n",
      "|(31,[0,1,2,3,4,5,...|    0|\n",
      "|(31,[0,1,2,3,4,5,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: int]\n",
       "testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: int]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainSet, testSet) = dataCredit.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "print(\"Training set:\\n\")\n",
    "trainSet.show(5)\n",
    "print(\"Test set:\\n\")\n",
    "testSet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] Intercept: 0.22043460091934813\n",
      "RMSE: 0.4145397298653062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_9a468a987eb9\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_9a468a987eb9, numFeatures=31\n",
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@60d1a6b\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "// train the model\n",
    "val lr = new LinearRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(trainSet)\n",
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "|0.22043460091934813|    1|(31,[0,1,2,3,4,5,...|\n",
      "|0.22043460091934813|    0|(31,[0,1,2,3,4,5,...|\n",
      "|0.22043460091934813|    1|(31,[0,1,2,3,4,5,...|\n",
      "|0.22043460091934813|    0|(31,[0,1,2,3,4,5,...|\n",
      "|0.22043460091934813|    1|(31,[0,1,2,3,4,5,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.4170830879975668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n",
       "ccevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_b11fd4b2d18d, metricName=rmse, throughOrigin=false\n",
       "ccrmse: Double = 0.4170830879975668\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = lrModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error.\n",
    "val ccevaluator = new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val ccrmse = ccevaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $ccrmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "|0.08785613692991008|    1|(31,[0,1,2,3,4,5,...|\n",
      "|0.08785613692991008|    0|(31,[0,1,2,3,4,5,...|\n",
      "| 0.5714285714285714|    1|(31,[0,1,2,3,4,5,...|\n",
      "|0.26842105263157895|    0|(31,[0,1,2,3,4,5,...|\n",
      "| 0.7916666666666666|    1|(31,[0,1,2,3,4,5,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.3750606252412109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "ccdt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_7aa750706ccf\n",
       "ccdtModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_7aa750706ccf, depth=5, numNodes=63, numFeatures=31\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_66f02698dfdc, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 0.3750606252412109\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val ccdt = new DecisionTreeRegressor()\n",
    "\n",
    "// train the model\n",
    "val ccdtModel = ccdt.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = ccdtModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rrandom forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "|0.13309105942306548|    1|(31,[0,1,2,3,4,5,...|\n",
      "|  0.151668240038433|    0|(31,[0,1,2,3,4,5,...|\n",
      "| 0.6452804516234367|    1|(31,[0,1,2,3,4,5,...|\n",
      "|0.22319769527819525|    0|(31,[0,1,2,3,4,5,...|\n",
      "| 0.7104925977102876|    1|(31,[0,1,2,3,4,5,...|\n",
      "|0.17553029107249654|    0|(31,[0,1,2,3,4,5,...|\n",
      "|0.13857631507986656|    0|(31,[0,1,2,3,4,5,...|\n",
      "|0.19842135495518856|    1|(31,[0,1,2,3,4,5,...|\n",
      "| 0.3806553184069842|    0|(31,[0,1,2,3,4,5,...|\n",
      "| 0.1567611642423749|    0|(31,[0,1,2,3,4,5,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.37067220529224104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.RandomForestRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "ccrf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_60201d253e68\n",
       "ccrfModel: org.apache.spark.ml.regression.RandomForestRegressionModel = RandomForestRegressionModel: uid=rfr_60201d253e68, numTrees=20, numFeatures=31\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_e651b68f605d, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 0.37067220529224104\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val ccrf = new RandomForestRegressor()\n",
    "\n",
    "// train the model\n",
    "val ccrfModel = ccrf.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = ccrfModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Comparison models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?)\n",
    "\n",
    "Logistic Regression: Logistic regression is easier to implement, interpret, and very efficient to train. It makes no assumptions about distributions of classes in feature space. It can easily extend to multiple classes(multinomial regression) and a natural probabilistic view of class predictions. It is very fast at classifying unknown records. However, cons include that if the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting. The major limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables. It can only be used to predict discrete functions. Hence, the dependent variable of Logistic Regression is bound to the discrete number set. Non-linear problems cant be solved with logistic regression because it has a linear decision surface. Linearly separable data is rarely found in real-world scenarios.\n",
    "\n",
    "##### Decision Tree classifier:\n",
    "\n",
    "Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.\n",
    "A decision tree does not require normalization of data.\n",
    "A decision tree does not require scaling of data as well.\n",
    "Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent.\n",
    "A Decision tree model is very intuitive and easy to explain to technical teams as well as stakeholders. A small change in the data can cause a large change in the structure of the decision tree causing instability.\n",
    "For a Decision tree sometimes calculation can go far more complex compared to other algorithms.\n",
    "Decision tree often involves higher time to train the model.Decision tree training is relatively expensive as the complexity and time has taken are more.\n",
    "The Decision Tree algorithm is inadequate for applying regression and predicting continuous values.\n",
    "\n",
    "\n",
    "##### Random Forest classifier:\n",
=======
    "## Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?)\n",
    "\n",
    "#### Logistic Regression: \n",
    "\n",
    "Logistic regression is easier to implement, interpret, and very efficient to train. It makes no assumptions about distributions of classes in feature space. It can easily extend to multiple classes(multinomial regression) and a natural probabilistic view of class predictions. It is very fast at classifying unknown records. However, cons include that if the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting. The major limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables. It can only be used to predict discrete functions. Hence, the dependent variable of Logistic Regression is bound to the discrete number set. Non-linear problems cant be solved with logistic regression because it has a linear decision surface. Linearly separable data is rarely found in real-world scenarios.\n",
    "\n",
    "#### Decision Tree classifier:\n",
    "\n",
    "Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.\n",
    "A decision tree does not require normalization of data. A decision tree does not require scaling of data as well. Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent. A Decision tree model is very intuitive and easy to explain to technical teams as well as stakeholders. A small change in the data can cause a large change in the structure of the decision tree causing instability.\n",
    "For a Decision tree sometimes calculation can go far more complex compared to other algorithms.\n",
    "Decision tree often involves higher time to train the model.Decision tree training is relatively expensive as the complexity and time has taken are more. The Decision Tree algorithm is inadequate for applying regression and predicting continuous values.\n",
    "\n",
    "\n",
    "#### Random Forest classifier:\n",
>>>>>>> 55df0c52efe6d9f927c697ab42e6caa9f248f918
    "The random forest algorithm is significantly more accurate than most of the non-linear classifiers.\n",
    "This algorithm is also very robust because it uses multiple decision trees to arrive at its result.\n",
    "The random forest classifier doesnt face the overfitting issue because it takes the average of all predictions, canceling out the biases and thus, fixing the overfitting problem.You can use this algorithm for both regression and classification problems, making it a highly versatile algorithm. Random forests dont let missing values cause an issue. They can use median values to replace the continuous variables or calculate the proximity-weighted average of the missing values to solve this problem. \n",
    "This algorithm offers you relative feature importance that allows you to select the most contributing features for your classifier easily. However, this algorithm is substantially slower than other classification algorithms because it uses multiple decision trees to make predictions. When a random forest classifier makes a prediction, every tree in the forest has to make a prediction for the same input and vote on the same. This process can be very time-consuming. \n",
    "Because of its slow pace, random forest classifiers can be unsuitable for real-time predictions.\n",
    "The model can be quite challenging to interpret in comparison to a decision tree as you can make a selection by following the trees path. However, thats not possible in a random forest as it has multiple decision trees. \n",
    "\n",
<<<<<<< HEAD
    "##### Compare the models' performances:\n",
=======
    "## Compare the models' performances:\n",
>>>>>>> 55df0c52efe6d9f927c697ab42e6caa9f248f918
    "\n",
    "To compare the models, we use a technique called Root Mean Square Error (RMSE) which is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. When standardized observations and forecasts are used as RMSE inputs, there is a direct relationship with the correlation coefficient. For example, if the correlation coefficient is 1, the RMSE will be 0, because all of the points lie on the regression line (and therefore there are no errors).\n",
    "\n",
    "RMSE for our models (lower is better):\n",
    "1. randomForestRegressor = 0.36276734017697326\n",
    "3. DecisionTreeRegressor =  0.364564852236988\n",
    "3. LinearRegression = 0.41386507215382207\n",
    "\n",
<<<<<<< HEAD
    "As can be seen above, Random Forest classifier performs a better result because by applying the feature scaling and one-hot encoding on categorical attributes, it's easier to interpret the dataset and achieve a better outcome. However, you should take into consideration the cons of Random Forest classifier which is its slow speed due to multiple decision trees. Machine learning models are evaluated empirically. Theory tells us that theres no single model that performs best on all possible datasets. That suggests we should expect to use different models for different applications with different datasets and requirements. \n"
=======
    "As can be seen above, Random Forest classifier performs a better result because by applying the feature scaling and one-hot encoding on categorical attributes, it's easier to interpret the dataset and achieve a better outcome. However, you should take into consideration the cons of Random Forest classifier which is its slow speed due to multiple decision trees. Machine learning models are evaluated empirically. Theory tells us that theres no single model that performs best on all possible datasets. That suggests we should expect to use different models for different applications with different datasets and requirements. \n",
    "\n",
    "## 7.4. What more would you do with this data? Anything to help you devise a better solution?\n",
    "\n",
    "To improve the model, we can use the following steps:\n",
    "\n",
    "1. Gradient-boosted tree classifier\n",
    "2. Hyperparameter tuning - Cross-validation"
>>>>>>> 55df0c52efe6d9f927c697ab42e6caa9f248f918
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
